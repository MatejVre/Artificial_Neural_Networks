{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e016f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from nn import *\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.init as init\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9effe9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split first!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8b589ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        random.seed(42)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 3)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Use NumPy to generate random numbers for weight initialization\n",
    "        for layer in self.linear_relu_stack:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Use NumPy to generate random values for weight initialization\n",
    "                weight_init = np.random.uniform(0, 1, size=layer.weight.shape)\n",
    "                bias_init = np.ones_like(layer.bias.detach())  # Detach bias from computation graph\n",
    "\n",
    "                # Assign the NumPy-generated weights to PyTorch layers\n",
    "                layer.weight.data = torch.tensor(weight_init, dtype=torch.float32)\n",
    "                layer.bias.data = torch.tensor(bias_init, dtype=torch.float32)\n",
    "    \n",
    "def train(X, y, model, learning_rate, num_epochs, loss_fn, X_test, y_test, validation=False):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Forward pass\n",
    "        preds = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Print loss after each epoch\n",
    "        if (epoch + 1) % 5 == 0:  # Print every 10 epochs (adjust as needed)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero gradients for the next step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if validation:      \n",
    "            # Validation step\n",
    "            model.eval()  # Switch to evaluation mode (disables dropout, batch norm, etc.)\n",
    "            with torch.no_grad():  # Disable gradient calculation for validation to save memory and computation\n",
    "                val_preds = model(X_test)\n",
    "                val_loss = loss_fn(val_preds, y_test)\n",
    "            \n",
    "            # Print validation loss after each epoch\n",
    "            # if (epoch + 1) % 5 == 0:  # Print every 10 epochs (adjust as needed)\n",
    "            #     print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss.item():.4f}\")\n",
    "            \n",
    "        model.train() \n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def test(X_test, y_test, model, loss_fn):\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():  # No gradient tracking needed for inference\n",
    "        preds = model(X_test)\n",
    "    # Convert predictions to class labels (for classification)\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    total = y_test.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    loss = loss_fn(preds, y_test)\n",
    "                                                                \n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Test Loss (Cross-Entropy): {loss:.4f}\")\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cb415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a73086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], Loss: 4.2763\n",
      "Epoch [10/1000], Loss: 4.0672\n",
      "Epoch [15/1000], Loss: 3.8610\n",
      "Epoch [20/1000], Loss: 3.6583\n",
      "Epoch [25/1000], Loss: 3.4596\n",
      "Epoch [30/1000], Loss: 3.2656\n",
      "Epoch [35/1000], Loss: 3.0769\n",
      "Epoch [40/1000], Loss: 2.8945\n",
      "Epoch [45/1000], Loss: 2.7193\n",
      "Epoch [50/1000], Loss: 2.5524\n",
      "Epoch [55/1000], Loss: 2.3948\n",
      "Epoch [60/1000], Loss: 2.2472\n",
      "Epoch [65/1000], Loss: 2.1104\n",
      "Epoch [70/1000], Loss: 1.9844\n",
      "Epoch [75/1000], Loss: 1.8689\n",
      "Epoch [80/1000], Loss: 1.7636\n",
      "Epoch [85/1000], Loss: 1.6677\n",
      "Epoch [90/1000], Loss: 1.5807\n",
      "Epoch [95/1000], Loss: 1.5019\n",
      "Epoch [100/1000], Loss: 1.4310\n",
      "Epoch [105/1000], Loss: 1.3676\n",
      "Epoch [110/1000], Loss: 1.3116\n",
      "Epoch [115/1000], Loss: 1.2625\n",
      "Epoch [120/1000], Loss: 1.2202\n",
      "Epoch [125/1000], Loss: 1.1841\n",
      "Epoch [130/1000], Loss: 1.1536\n",
      "Epoch [135/1000], Loss: 1.1279\n",
      "Epoch [140/1000], Loss: 1.1065\n",
      "Epoch [145/1000], Loss: 1.0885\n",
      "Epoch [150/1000], Loss: 1.0735\n",
      "Epoch [155/1000], Loss: 1.0607\n",
      "Epoch [160/1000], Loss: 1.0499\n",
      "Epoch [165/1000], Loss: 1.0406\n",
      "Epoch [170/1000], Loss: 1.0325\n",
      "Epoch [175/1000], Loss: 1.0253\n",
      "Epoch [180/1000], Loss: 1.0190\n",
      "Epoch [185/1000], Loss: 1.0132\n",
      "Epoch [190/1000], Loss: 1.0080\n",
      "Epoch [195/1000], Loss: 1.0032\n",
      "Epoch [200/1000], Loss: 0.9988\n",
      "Epoch [205/1000], Loss: 0.9946\n",
      "Epoch [210/1000], Loss: 0.9907\n",
      "Epoch [215/1000], Loss: 0.9870\n",
      "Epoch [220/1000], Loss: 0.9835\n",
      "Epoch [225/1000], Loss: 0.9801\n",
      "Epoch [230/1000], Loss: 0.9768\n",
      "Epoch [235/1000], Loss: 0.9737\n",
      "Epoch [240/1000], Loss: 0.9706\n",
      "Epoch [245/1000], Loss: 0.9677\n",
      "Epoch [250/1000], Loss: 0.9648\n",
      "Epoch [255/1000], Loss: 0.9620\n",
      "Epoch [260/1000], Loss: 0.9592\n",
      "Epoch [265/1000], Loss: 0.9565\n",
      "Epoch [270/1000], Loss: 0.9539\n",
      "Epoch [275/1000], Loss: 0.9513\n",
      "Epoch [280/1000], Loss: 0.9487\n",
      "Epoch [285/1000], Loss: 0.9462\n",
      "Epoch [290/1000], Loss: 0.9438\n",
      "Epoch [295/1000], Loss: 0.9413\n",
      "Epoch [300/1000], Loss: 0.9389\n",
      "Epoch [305/1000], Loss: 0.9366\n",
      "Epoch [310/1000], Loss: 0.9342\n",
      "Epoch [315/1000], Loss: 0.9319\n",
      "Epoch [320/1000], Loss: 0.9296\n",
      "Epoch [325/1000], Loss: 0.9274\n",
      "Epoch [330/1000], Loss: 0.9252\n",
      "Epoch [335/1000], Loss: 0.9230\n",
      "Epoch [340/1000], Loss: 0.9208\n",
      "Epoch [345/1000], Loss: 0.9186\n",
      "Epoch [350/1000], Loss: 0.9165\n",
      "Epoch [355/1000], Loss: 0.9144\n",
      "Epoch [360/1000], Loss: 0.9123\n",
      "Epoch [365/1000], Loss: 0.9102\n",
      "Epoch [370/1000], Loss: 0.9081\n",
      "Epoch [375/1000], Loss: 0.9061\n",
      "Epoch [380/1000], Loss: 0.9041\n",
      "Epoch [385/1000], Loss: 0.9021\n",
      "Epoch [390/1000], Loss: 0.9001\n",
      "Epoch [395/1000], Loss: 0.8981\n",
      "Epoch [400/1000], Loss: 0.8961\n",
      "Epoch [405/1000], Loss: 0.8942\n",
      "Epoch [410/1000], Loss: 0.8922\n",
      "Epoch [415/1000], Loss: 0.8903\n",
      "Epoch [420/1000], Loss: 0.8884\n",
      "Epoch [425/1000], Loss: 0.8865\n",
      "Epoch [430/1000], Loss: 0.8846\n",
      "Epoch [435/1000], Loss: 0.8827\n",
      "Epoch [440/1000], Loss: 0.8809\n",
      "Epoch [445/1000], Loss: 0.8790\n",
      "Epoch [450/1000], Loss: 0.8772\n",
      "Epoch [455/1000], Loss: 0.8753\n",
      "Epoch [460/1000], Loss: 0.8735\n",
      "Epoch [465/1000], Loss: 0.8717\n",
      "Epoch [470/1000], Loss: 0.8699\n",
      "Epoch [475/1000], Loss: 0.8681\n",
      "Epoch [480/1000], Loss: 0.8663\n",
      "Epoch [485/1000], Loss: 0.8645\n",
      "Epoch [490/1000], Loss: 0.8628\n",
      "Epoch [495/1000], Loss: 0.8610\n",
      "Epoch [500/1000], Loss: 0.8593\n",
      "Epoch [505/1000], Loss: 0.8575\n",
      "Epoch [510/1000], Loss: 0.8558\n",
      "Epoch [515/1000], Loss: 0.8541\n",
      "Epoch [520/1000], Loss: 0.8524\n",
      "Epoch [525/1000], Loss: 0.8507\n",
      "Epoch [530/1000], Loss: 0.8490\n",
      "Epoch [535/1000], Loss: 0.8473\n",
      "Epoch [540/1000], Loss: 0.8456\n",
      "Epoch [545/1000], Loss: 0.8439\n",
      "Epoch [550/1000], Loss: 0.8423\n",
      "Epoch [555/1000], Loss: 0.8406\n",
      "Epoch [560/1000], Loss: 0.8390\n",
      "Epoch [565/1000], Loss: 0.8374\n",
      "Epoch [570/1000], Loss: 0.8358\n",
      "Epoch [575/1000], Loss: 0.8341\n",
      "Epoch [580/1000], Loss: 0.8325\n",
      "Epoch [585/1000], Loss: 0.8309\n",
      "Epoch [590/1000], Loss: 0.8294\n",
      "Epoch [595/1000], Loss: 0.8278\n",
      "Epoch [600/1000], Loss: 0.8262\n",
      "Epoch [605/1000], Loss: 0.8246\n",
      "Epoch [610/1000], Loss: 0.8231\n",
      "Epoch [615/1000], Loss: 0.8215\n",
      "Epoch [620/1000], Loss: 0.8200\n",
      "Epoch [625/1000], Loss: 0.8185\n",
      "Epoch [630/1000], Loss: 0.8169\n",
      "Epoch [635/1000], Loss: 0.8154\n",
      "Epoch [640/1000], Loss: 0.8139\n",
      "Epoch [645/1000], Loss: 0.8124\n",
      "Epoch [650/1000], Loss: 0.8108\n",
      "Epoch [655/1000], Loss: 0.8093\n",
      "Epoch [660/1000], Loss: 0.8078\n",
      "Epoch [665/1000], Loss: 0.8063\n",
      "Epoch [670/1000], Loss: 0.8048\n",
      "Epoch [675/1000], Loss: 0.8033\n",
      "Epoch [680/1000], Loss: 0.8018\n",
      "Epoch [685/1000], Loss: 0.8004\n",
      "Epoch [690/1000], Loss: 0.7989\n",
      "Epoch [695/1000], Loss: 0.7974\n",
      "Epoch [700/1000], Loss: 0.7960\n",
      "Epoch [705/1000], Loss: 0.7945\n",
      "Epoch [710/1000], Loss: 0.7931\n",
      "Epoch [715/1000], Loss: 0.7916\n",
      "Epoch [720/1000], Loss: 0.7902\n",
      "Epoch [725/1000], Loss: 0.7888\n",
      "Epoch [730/1000], Loss: 0.7873\n",
      "Epoch [735/1000], Loss: 0.7859\n",
      "Epoch [740/1000], Loss: 0.7845\n",
      "Epoch [745/1000], Loss: 0.7831\n",
      "Epoch [750/1000], Loss: 0.7817\n",
      "Epoch [755/1000], Loss: 0.7803\n",
      "Epoch [760/1000], Loss: 0.7789\n",
      "Epoch [765/1000], Loss: 0.7775\n",
      "Epoch [770/1000], Loss: 0.7761\n",
      "Epoch [775/1000], Loss: 0.7747\n",
      "Epoch [780/1000], Loss: 0.7734\n",
      "Epoch [785/1000], Loss: 0.7720\n",
      "Epoch [790/1000], Loss: 0.7707\n",
      "Epoch [795/1000], Loss: 0.7693\n",
      "Epoch [800/1000], Loss: 0.7680\n",
      "Epoch [805/1000], Loss: 0.7666\n",
      "Epoch [810/1000], Loss: 0.7653\n",
      "Epoch [815/1000], Loss: 0.7640\n",
      "Epoch [820/1000], Loss: 0.7626\n",
      "Epoch [825/1000], Loss: 0.7613\n",
      "Epoch [830/1000], Loss: 0.7600\n",
      "Epoch [835/1000], Loss: 0.7587\n",
      "Epoch [840/1000], Loss: 0.7574\n",
      "Epoch [845/1000], Loss: 0.7561\n",
      "Epoch [850/1000], Loss: 0.7548\n",
      "Epoch [855/1000], Loss: 0.7535\n",
      "Epoch [860/1000], Loss: 0.7522\n",
      "Epoch [865/1000], Loss: 0.7509\n",
      "Epoch [870/1000], Loss: 0.7496\n",
      "Epoch [875/1000], Loss: 0.7483\n",
      "Epoch [880/1000], Loss: 0.7470\n",
      "Epoch [885/1000], Loss: 0.7458\n",
      "Epoch [890/1000], Loss: 0.7445\n",
      "Epoch [895/1000], Loss: 0.7432\n",
      "Epoch [900/1000], Loss: 0.7420\n",
      "Epoch [905/1000], Loss: 0.7407\n",
      "Epoch [910/1000], Loss: 0.7395\n",
      "Epoch [915/1000], Loss: 0.7382\n",
      "Epoch [920/1000], Loss: 0.7370\n",
      "Epoch [925/1000], Loss: 0.7358\n",
      "Epoch [930/1000], Loss: 0.7345\n",
      "Epoch [935/1000], Loss: 0.7333\n",
      "Epoch [940/1000], Loss: 0.7321\n",
      "Epoch [945/1000], Loss: 0.7309\n",
      "Epoch [950/1000], Loss: 0.7296\n",
      "Epoch [955/1000], Loss: 0.7284\n",
      "Epoch [960/1000], Loss: 0.7272\n",
      "Epoch [965/1000], Loss: 0.7260\n",
      "Epoch [970/1000], Loss: 0.7248\n",
      "Epoch [975/1000], Loss: 0.7236\n",
      "Epoch [980/1000], Loss: 0.7224\n",
      "Epoch [985/1000], Loss: 0.7213\n",
      "Epoch [990/1000], Loss: 0.7201\n",
      "Epoch [995/1000], Loss: 0.7189\n",
      "Epoch [1000/1000], Loss: 0.7177\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model_torch = NeuralNetwork()\n",
    "train(X_train_tensor, y_train_tensor, model_torch, 0.001, 1000, nn.CrossEntropyLoss(), X_test_tensor, y_test_tensor, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6963a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.33%\n",
      "Test Loss (Cross-Entropy): 0.6653\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "preds = test(X_test_tensor, y_test_tensor, model_torch, nn.CrossEntropyLoss()).detach().numpy()\n",
    "print(np.mean(preds == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a7f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nn.ANNClassification at 0x2411d4fa270>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = ANNClassification([20], [\"relu\"])\n",
    "my_model.fit(X_train, y_train, 0.001, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b00b8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666666666666667\n"
     ]
    }
   ],
   "source": [
    "preds2 = np.argmax(my_model.predict(X_test_tensor), axis=1)\n",
    "print(np.mean(preds2 == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f10db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(preds2 == preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0948748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 1.2706\n",
      "Epoch [10/10], Loss: 1.0961\n",
      "Training complete.\n",
      "Test Accuracy: 36.67%\n",
      "Test Loss (Cross-Entropy): 1.0376\n",
      "Epoch [5/20], Loss: 1.2706\n",
      "Epoch [10/20], Loss: 1.0961\n",
      "Epoch [15/20], Loss: 1.0366\n",
      "Epoch [20/20], Loss: 1.0028\n",
      "Training complete.\n",
      "Test Accuracy: 36.67%\n",
      "Test Loss (Cross-Entropy): 0.9363\n",
      "Epoch [5/30], Loss: 1.2706\n",
      "Epoch [10/30], Loss: 1.0961\n",
      "Epoch [15/30], Loss: 1.0366\n",
      "Epoch [20/30], Loss: 1.0028\n",
      "Epoch [25/30], Loss: 0.9762\n",
      "Epoch [30/30], Loss: 0.9529\n",
      "Training complete.\n",
      "Test Accuracy: 46.67%\n",
      "Test Loss (Cross-Entropy): 0.8885\n",
      "Epoch [5/40], Loss: 1.2706\n",
      "Epoch [10/40], Loss: 1.0961\n",
      "Epoch [15/40], Loss: 1.0366\n",
      "Epoch [20/40], Loss: 1.0028\n",
      "Epoch [25/40], Loss: 0.9762\n",
      "Epoch [30/40], Loss: 0.9529\n",
      "Epoch [35/40], Loss: 0.9317\n",
      "Epoch [40/40], Loss: 0.9119\n",
      "Training complete.\n",
      "Test Accuracy: 56.67%\n",
      "Test Loss (Cross-Entropy): 0.8509\n",
      "Epoch [5/50], Loss: 1.2706\n",
      "Epoch [10/50], Loss: 1.0961\n",
      "Epoch [15/50], Loss: 1.0366\n",
      "Epoch [20/50], Loss: 1.0028\n",
      "Epoch [25/50], Loss: 0.9762\n",
      "Epoch [30/50], Loss: 0.9529\n",
      "Epoch [35/50], Loss: 0.9317\n",
      "Epoch [40/50], Loss: 0.9119\n",
      "Epoch [45/50], Loss: 0.8932\n",
      "Epoch [50/50], Loss: 0.8756\n",
      "Training complete.\n",
      "Test Accuracy: 60.00%\n",
      "Test Loss (Cross-Entropy): 0.8179\n",
      "Epoch [5/60], Loss: 1.2706\n",
      "Epoch [10/60], Loss: 1.0961\n",
      "Epoch [15/60], Loss: 1.0366\n",
      "Epoch [20/60], Loss: 1.0028\n",
      "Epoch [25/60], Loss: 0.9762\n",
      "Epoch [30/60], Loss: 0.9529\n",
      "Epoch [35/60], Loss: 0.9317\n",
      "Epoch [40/60], Loss: 0.9119\n",
      "Epoch [45/60], Loss: 0.8932\n",
      "Epoch [50/60], Loss: 0.8756\n",
      "Epoch [55/60], Loss: 0.8588\n",
      "Epoch [60/60], Loss: 0.8426\n",
      "Training complete.\n",
      "Test Accuracy: 66.67%\n",
      "Test Loss (Cross-Entropy): 0.7871\n",
      "Epoch [5/70], Loss: 1.2706\n",
      "Epoch [10/70], Loss: 1.0961\n",
      "Epoch [15/70], Loss: 1.0366\n",
      "Epoch [20/70], Loss: 1.0028\n",
      "Epoch [25/70], Loss: 0.9762\n",
      "Epoch [30/70], Loss: 0.9529\n",
      "Epoch [35/70], Loss: 0.9317\n",
      "Epoch [40/70], Loss: 0.9119\n",
      "Epoch [45/70], Loss: 0.8932\n",
      "Epoch [50/70], Loss: 0.8756\n",
      "Epoch [55/70], Loss: 0.8588\n",
      "Epoch [60/70], Loss: 0.8426\n",
      "Epoch [65/70], Loss: 0.8270\n",
      "Epoch [70/70], Loss: 0.8122\n",
      "Training complete.\n",
      "Test Accuracy: 70.00%\n",
      "Test Loss (Cross-Entropy): 0.7582\n",
      "Epoch [5/80], Loss: 1.2706\n",
      "Epoch [10/80], Loss: 1.0961\n",
      "Epoch [15/80], Loss: 1.0366\n",
      "Epoch [20/80], Loss: 1.0028\n",
      "Epoch [25/80], Loss: 0.9762\n",
      "Epoch [30/80], Loss: 0.9529\n",
      "Epoch [35/80], Loss: 0.9317\n",
      "Epoch [40/80], Loss: 0.9119\n",
      "Epoch [45/80], Loss: 0.8932\n",
      "Epoch [50/80], Loss: 0.8756\n",
      "Epoch [55/80], Loss: 0.8588\n",
      "Epoch [60/80], Loss: 0.8426\n",
      "Epoch [65/80], Loss: 0.8270\n",
      "Epoch [70/80], Loss: 0.8122\n",
      "Epoch [75/80], Loss: 0.7979\n",
      "Epoch [80/80], Loss: 0.7838\n",
      "Training complete.\n",
      "Test Accuracy: 70.00%\n",
      "Test Loss (Cross-Entropy): 0.7307\n",
      "Epoch [5/90], Loss: 1.2706\n",
      "Epoch [10/90], Loss: 1.0961\n",
      "Epoch [15/90], Loss: 1.0366\n",
      "Epoch [20/90], Loss: 1.0028\n",
      "Epoch [25/90], Loss: 0.9762\n",
      "Epoch [30/90], Loss: 0.9529\n",
      "Epoch [35/90], Loss: 0.9317\n",
      "Epoch [40/90], Loss: 0.9119\n",
      "Epoch [45/90], Loss: 0.8932\n",
      "Epoch [50/90], Loss: 0.8756\n",
      "Epoch [55/90], Loss: 0.8588\n",
      "Epoch [60/90], Loss: 0.8426\n",
      "Epoch [65/90], Loss: 0.8270\n",
      "Epoch [70/90], Loss: 0.8122\n",
      "Epoch [75/90], Loss: 0.7979\n",
      "Epoch [80/90], Loss: 0.7838\n",
      "Epoch [85/90], Loss: 0.7702\n",
      "Epoch [90/90], Loss: 0.7570\n",
      "Training complete.\n",
      "Test Accuracy: 76.67%\n",
      "Test Loss (Cross-Entropy): 0.7048\n",
      "Epoch [5/100], Loss: 1.2706\n",
      "Epoch [10/100], Loss: 1.0961\n",
      "Epoch [15/100], Loss: 1.0366\n",
      "Epoch [20/100], Loss: 1.0028\n",
      "Epoch [25/100], Loss: 0.9762\n",
      "Epoch [30/100], Loss: 0.9529\n",
      "Epoch [35/100], Loss: 0.9317\n",
      "Epoch [40/100], Loss: 0.9119\n",
      "Epoch [45/100], Loss: 0.8932\n",
      "Epoch [50/100], Loss: 0.8756\n",
      "Epoch [55/100], Loss: 0.8588\n",
      "Epoch [60/100], Loss: 0.8426\n",
      "Epoch [65/100], Loss: 0.8270\n",
      "Epoch [70/100], Loss: 0.8122\n",
      "Epoch [75/100], Loss: 0.7979\n",
      "Epoch [80/100], Loss: 0.7838\n",
      "Epoch [85/100], Loss: 0.7702\n",
      "Epoch [90/100], Loss: 0.7570\n",
      "Epoch [95/100], Loss: 0.7441\n",
      "Epoch [100/100], Loss: 0.7315\n",
      "Training complete.\n",
      "Test Accuracy: 76.67%\n",
      "Test Loss (Cross-Entropy): 0.6796\n",
      "Epoch [5/110], Loss: 1.2706\n",
      "Epoch [10/110], Loss: 1.0961\n",
      "Epoch [15/110], Loss: 1.0366\n",
      "Epoch [20/110], Loss: 1.0028\n",
      "Epoch [25/110], Loss: 0.9762\n",
      "Epoch [30/110], Loss: 0.9529\n",
      "Epoch [35/110], Loss: 0.9317\n",
      "Epoch [40/110], Loss: 0.9119\n",
      "Epoch [45/110], Loss: 0.8932\n",
      "Epoch [50/110], Loss: 0.8756\n",
      "Epoch [55/110], Loss: 0.8588\n",
      "Epoch [60/110], Loss: 0.8426\n",
      "Epoch [65/110], Loss: 0.8270\n",
      "Epoch [70/110], Loss: 0.8122\n",
      "Epoch [75/110], Loss: 0.7979\n",
      "Epoch [80/110], Loss: 0.7838\n",
      "Epoch [85/110], Loss: 0.7702\n",
      "Epoch [90/110], Loss: 0.7570\n",
      "Epoch [95/110], Loss: 0.7441\n",
      "Epoch [100/110], Loss: 0.7315\n",
      "Epoch [105/110], Loss: 0.7193\n",
      "Epoch [110/110], Loss: 0.7074\n",
      "Training complete.\n",
      "Test Accuracy: 76.67%\n",
      "Test Loss (Cross-Entropy): 0.6556\n",
      "Epoch [5/120], Loss: 1.2706\n",
      "Epoch [10/120], Loss: 1.0961\n",
      "Epoch [15/120], Loss: 1.0366\n",
      "Epoch [20/120], Loss: 1.0028\n",
      "Epoch [25/120], Loss: 0.9762\n",
      "Epoch [30/120], Loss: 0.9529\n",
      "Epoch [35/120], Loss: 0.9317\n",
      "Epoch [40/120], Loss: 0.9119\n",
      "Epoch [45/120], Loss: 0.8932\n",
      "Epoch [50/120], Loss: 0.8756\n",
      "Epoch [55/120], Loss: 0.8588\n",
      "Epoch [60/120], Loss: 0.8426\n",
      "Epoch [65/120], Loss: 0.8270\n",
      "Epoch [70/120], Loss: 0.8122\n",
      "Epoch [75/120], Loss: 0.7979\n",
      "Epoch [80/120], Loss: 0.7838\n",
      "Epoch [85/120], Loss: 0.7702\n",
      "Epoch [90/120], Loss: 0.7570\n",
      "Epoch [95/120], Loss: 0.7441\n",
      "Epoch [100/120], Loss: 0.7315\n",
      "Epoch [105/120], Loss: 0.7193\n",
      "Epoch [110/120], Loss: 0.7074\n",
      "Epoch [115/120], Loss: 0.6960\n",
      "Epoch [120/120], Loss: 0.6849\n",
      "Training complete.\n",
      "Test Accuracy: 80.00%\n",
      "Test Loss (Cross-Entropy): 0.6331\n",
      "Epoch [5/130], Loss: 1.2706\n",
      "Epoch [10/130], Loss: 1.0961\n",
      "Epoch [15/130], Loss: 1.0366\n",
      "Epoch [20/130], Loss: 1.0028\n",
      "Epoch [25/130], Loss: 0.9762\n",
      "Epoch [30/130], Loss: 0.9529\n",
      "Epoch [35/130], Loss: 0.9317\n",
      "Epoch [40/130], Loss: 0.9119\n",
      "Epoch [45/130], Loss: 0.8932\n",
      "Epoch [50/130], Loss: 0.8756\n",
      "Epoch [55/130], Loss: 0.8588\n",
      "Epoch [60/130], Loss: 0.8426\n",
      "Epoch [65/130], Loss: 0.8270\n",
      "Epoch [70/130], Loss: 0.8122\n",
      "Epoch [75/130], Loss: 0.7979\n",
      "Epoch [80/130], Loss: 0.7838\n",
      "Epoch [85/130], Loss: 0.7702\n",
      "Epoch [90/130], Loss: 0.7570\n",
      "Epoch [95/130], Loss: 0.7441\n",
      "Epoch [100/130], Loss: 0.7315\n",
      "Epoch [105/130], Loss: 0.7193\n",
      "Epoch [110/130], Loss: 0.7074\n",
      "Epoch [115/130], Loss: 0.6960\n",
      "Epoch [120/130], Loss: 0.6849\n",
      "Epoch [125/130], Loss: 0.6742\n",
      "Epoch [130/130], Loss: 0.6638\n",
      "Training complete.\n",
      "Test Accuracy: 83.33%\n",
      "Test Loss (Cross-Entropy): 0.6118\n",
      "Epoch [5/140], Loss: 1.2706\n",
      "Epoch [10/140], Loss: 1.0961\n",
      "Epoch [15/140], Loss: 1.0366\n",
      "Epoch [20/140], Loss: 1.0028\n",
      "Epoch [25/140], Loss: 0.9762\n",
      "Epoch [30/140], Loss: 0.9529\n",
      "Epoch [35/140], Loss: 0.9317\n",
      "Epoch [40/140], Loss: 0.9119\n",
      "Epoch [45/140], Loss: 0.8932\n",
      "Epoch [50/140], Loss: 0.8756\n",
      "Epoch [55/140], Loss: 0.8588\n",
      "Epoch [60/140], Loss: 0.8426\n",
      "Epoch [65/140], Loss: 0.8270\n",
      "Epoch [70/140], Loss: 0.8122\n",
      "Epoch [75/140], Loss: 0.7979\n",
      "Epoch [80/140], Loss: 0.7838\n",
      "Epoch [85/140], Loss: 0.7702\n",
      "Epoch [90/140], Loss: 0.7570\n",
      "Epoch [95/140], Loss: 0.7441\n",
      "Epoch [100/140], Loss: 0.7315\n",
      "Epoch [105/140], Loss: 0.7193\n",
      "Epoch [110/140], Loss: 0.7074\n",
      "Epoch [115/140], Loss: 0.6960\n",
      "Epoch [120/140], Loss: 0.6849\n",
      "Epoch [125/140], Loss: 0.6742\n",
      "Epoch [130/140], Loss: 0.6638\n",
      "Epoch [135/140], Loss: 0.6537\n",
      "Epoch [140/140], Loss: 0.6439\n",
      "Training complete.\n",
      "Test Accuracy: 83.33%\n",
      "Test Loss (Cross-Entropy): 0.5918\n",
      "Epoch [5/150], Loss: 1.2706\n",
      "Epoch [10/150], Loss: 1.0961\n",
      "Epoch [15/150], Loss: 1.0366\n",
      "Epoch [20/150], Loss: 1.0028\n",
      "Epoch [25/150], Loss: 0.9762\n",
      "Epoch [30/150], Loss: 0.9529\n",
      "Epoch [35/150], Loss: 0.9317\n",
      "Epoch [40/150], Loss: 0.9119\n",
      "Epoch [45/150], Loss: 0.8932\n",
      "Epoch [50/150], Loss: 0.8756\n",
      "Epoch [55/150], Loss: 0.8588\n",
      "Epoch [60/150], Loss: 0.8426\n",
      "Epoch [65/150], Loss: 0.8270\n",
      "Epoch [70/150], Loss: 0.8122\n",
      "Epoch [75/150], Loss: 0.7979\n",
      "Epoch [80/150], Loss: 0.7838\n",
      "Epoch [85/150], Loss: 0.7702\n",
      "Epoch [90/150], Loss: 0.7570\n",
      "Epoch [95/150], Loss: 0.7441\n",
      "Epoch [100/150], Loss: 0.7315\n",
      "Epoch [105/150], Loss: 0.7193\n",
      "Epoch [110/150], Loss: 0.7074\n",
      "Epoch [115/150], Loss: 0.6960\n",
      "Epoch [120/150], Loss: 0.6849\n",
      "Epoch [125/150], Loss: 0.6742\n",
      "Epoch [130/150], Loss: 0.6638\n",
      "Epoch [135/150], Loss: 0.6537\n",
      "Epoch [140/150], Loss: 0.6439\n",
      "Epoch [145/150], Loss: 0.6345\n",
      "Epoch [150/150], Loss: 0.6253\n",
      "Training complete.\n",
      "Test Accuracy: 83.33%\n",
      "Test Loss (Cross-Entropy): 0.5728\n",
      "Epoch [5/160], Loss: 1.2706\n",
      "Epoch [10/160], Loss: 1.0961\n",
      "Epoch [15/160], Loss: 1.0366\n",
      "Epoch [20/160], Loss: 1.0028\n",
      "Epoch [25/160], Loss: 0.9762\n",
      "Epoch [30/160], Loss: 0.9529\n",
      "Epoch [35/160], Loss: 0.9317\n",
      "Epoch [40/160], Loss: 0.9119\n",
      "Epoch [45/160], Loss: 0.8932\n",
      "Epoch [50/160], Loss: 0.8756\n",
      "Epoch [55/160], Loss: 0.8588\n",
      "Epoch [60/160], Loss: 0.8426\n",
      "Epoch [65/160], Loss: 0.8270\n",
      "Epoch [70/160], Loss: 0.8122\n",
      "Epoch [75/160], Loss: 0.7979\n",
      "Epoch [80/160], Loss: 0.7838\n",
      "Epoch [85/160], Loss: 0.7702\n",
      "Epoch [90/160], Loss: 0.7570\n",
      "Epoch [95/160], Loss: 0.7441\n",
      "Epoch [100/160], Loss: 0.7315\n",
      "Epoch [105/160], Loss: 0.7193\n",
      "Epoch [110/160], Loss: 0.7074\n",
      "Epoch [115/160], Loss: 0.6960\n",
      "Epoch [120/160], Loss: 0.6849\n",
      "Epoch [125/160], Loss: 0.6742\n",
      "Epoch [130/160], Loss: 0.6638\n",
      "Epoch [135/160], Loss: 0.6537\n",
      "Epoch [140/160], Loss: 0.6439\n",
      "Epoch [145/160], Loss: 0.6345\n",
      "Epoch [150/160], Loss: 0.6253\n",
      "Epoch [155/160], Loss: 0.6164\n",
      "Epoch [160/160], Loss: 0.6078\n",
      "Training complete.\n",
      "Test Accuracy: 86.67%\n",
      "Test Loss (Cross-Entropy): 0.5549\n",
      "Epoch [5/170], Loss: 1.2706\n",
      "Epoch [10/170], Loss: 1.0961\n",
      "Epoch [15/170], Loss: 1.0366\n",
      "Epoch [20/170], Loss: 1.0028\n",
      "Epoch [25/170], Loss: 0.9762\n",
      "Epoch [30/170], Loss: 0.9529\n",
      "Epoch [35/170], Loss: 0.9317\n",
      "Epoch [40/170], Loss: 0.9119\n",
      "Epoch [45/170], Loss: 0.8932\n",
      "Epoch [50/170], Loss: 0.8756\n",
      "Epoch [55/170], Loss: 0.8588\n",
      "Epoch [60/170], Loss: 0.8426\n",
      "Epoch [65/170], Loss: 0.8270\n",
      "Epoch [70/170], Loss: 0.8122\n",
      "Epoch [75/170], Loss: 0.7979\n",
      "Epoch [80/170], Loss: 0.7838\n",
      "Epoch [85/170], Loss: 0.7702\n",
      "Epoch [90/170], Loss: 0.7570\n",
      "Epoch [95/170], Loss: 0.7441\n",
      "Epoch [100/170], Loss: 0.7315\n",
      "Epoch [105/170], Loss: 0.7193\n",
      "Epoch [110/170], Loss: 0.7074\n",
      "Epoch [115/170], Loss: 0.6960\n",
      "Epoch [120/170], Loss: 0.6849\n",
      "Epoch [125/170], Loss: 0.6742\n",
      "Epoch [130/170], Loss: 0.6638\n",
      "Epoch [135/170], Loss: 0.6537\n",
      "Epoch [140/170], Loss: 0.6439\n",
      "Epoch [145/170], Loss: 0.6345\n",
      "Epoch [150/170], Loss: 0.6253\n",
      "Epoch [155/170], Loss: 0.6164\n",
      "Epoch [160/170], Loss: 0.6078\n",
      "Epoch [165/170], Loss: 0.5995\n",
      "Epoch [170/170], Loss: 0.5914\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.5379\n",
      "Epoch [5/180], Loss: 1.2706\n",
      "Epoch [10/180], Loss: 1.0961\n",
      "Epoch [15/180], Loss: 1.0366\n",
      "Epoch [20/180], Loss: 1.0028\n",
      "Epoch [25/180], Loss: 0.9762\n",
      "Epoch [30/180], Loss: 0.9529\n",
      "Epoch [35/180], Loss: 0.9317\n",
      "Epoch [40/180], Loss: 0.9119\n",
      "Epoch [45/180], Loss: 0.8932\n",
      "Epoch [50/180], Loss: 0.8756\n",
      "Epoch [55/180], Loss: 0.8588\n",
      "Epoch [60/180], Loss: 0.8426\n",
      "Epoch [65/180], Loss: 0.8270\n",
      "Epoch [70/180], Loss: 0.8122\n",
      "Epoch [75/180], Loss: 0.7979\n",
      "Epoch [80/180], Loss: 0.7838\n",
      "Epoch [85/180], Loss: 0.7702\n",
      "Epoch [90/180], Loss: 0.7570\n",
      "Epoch [95/180], Loss: 0.7441\n",
      "Epoch [100/180], Loss: 0.7315\n",
      "Epoch [105/180], Loss: 0.7193\n",
      "Epoch [110/180], Loss: 0.7074\n",
      "Epoch [115/180], Loss: 0.6960\n",
      "Epoch [120/180], Loss: 0.6849\n",
      "Epoch [125/180], Loss: 0.6742\n",
      "Epoch [130/180], Loss: 0.6638\n",
      "Epoch [135/180], Loss: 0.6537\n",
      "Epoch [140/180], Loss: 0.6439\n",
      "Epoch [145/180], Loss: 0.6345\n",
      "Epoch [150/180], Loss: 0.6253\n",
      "Epoch [155/180], Loss: 0.6164\n",
      "Epoch [160/180], Loss: 0.6078\n",
      "Epoch [165/180], Loss: 0.5995\n",
      "Epoch [170/180], Loss: 0.5914\n",
      "Epoch [175/180], Loss: 0.5836\n",
      "Epoch [180/180], Loss: 0.5760\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.5219\n",
      "Epoch [5/190], Loss: 1.2706\n",
      "Epoch [10/190], Loss: 1.0961\n",
      "Epoch [15/190], Loss: 1.0366\n",
      "Epoch [20/190], Loss: 1.0028\n",
      "Epoch [25/190], Loss: 0.9762\n",
      "Epoch [30/190], Loss: 0.9529\n",
      "Epoch [35/190], Loss: 0.9317\n",
      "Epoch [40/190], Loss: 0.9119\n",
      "Epoch [45/190], Loss: 0.8932\n",
      "Epoch [50/190], Loss: 0.8756\n",
      "Epoch [55/190], Loss: 0.8588\n",
      "Epoch [60/190], Loss: 0.8426\n",
      "Epoch [65/190], Loss: 0.8270\n",
      "Epoch [70/190], Loss: 0.8122\n",
      "Epoch [75/190], Loss: 0.7979\n",
      "Epoch [80/190], Loss: 0.7838\n",
      "Epoch [85/190], Loss: 0.7702\n",
      "Epoch [90/190], Loss: 0.7570\n",
      "Epoch [95/190], Loss: 0.7441\n",
      "Epoch [100/190], Loss: 0.7315\n",
      "Epoch [105/190], Loss: 0.7193\n",
      "Epoch [110/190], Loss: 0.7074\n",
      "Epoch [115/190], Loss: 0.6960\n",
      "Epoch [120/190], Loss: 0.6849\n",
      "Epoch [125/190], Loss: 0.6742\n",
      "Epoch [130/190], Loss: 0.6638\n",
      "Epoch [135/190], Loss: 0.6537\n",
      "Epoch [140/190], Loss: 0.6439\n",
      "Epoch [145/190], Loss: 0.6345\n",
      "Epoch [150/190], Loss: 0.6253\n",
      "Epoch [155/190], Loss: 0.6164\n",
      "Epoch [160/190], Loss: 0.6078\n",
      "Epoch [165/190], Loss: 0.5995\n",
      "Epoch [170/190], Loss: 0.5914\n",
      "Epoch [175/190], Loss: 0.5836\n",
      "Epoch [180/190], Loss: 0.5760\n",
      "Epoch [185/190], Loss: 0.5686\n",
      "Epoch [190/190], Loss: 0.5615\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.5068\n",
      "Epoch [5/200], Loss: 1.2706\n",
      "Epoch [10/200], Loss: 1.0961\n",
      "Epoch [15/200], Loss: 1.0366\n",
      "Epoch [20/200], Loss: 1.0028\n",
      "Epoch [25/200], Loss: 0.9762\n",
      "Epoch [30/200], Loss: 0.9529\n",
      "Epoch [35/200], Loss: 0.9317\n",
      "Epoch [40/200], Loss: 0.9119\n",
      "Epoch [45/200], Loss: 0.8932\n",
      "Epoch [50/200], Loss: 0.8756\n",
      "Epoch [55/200], Loss: 0.8588\n",
      "Epoch [60/200], Loss: 0.8426\n",
      "Epoch [65/200], Loss: 0.8270\n",
      "Epoch [70/200], Loss: 0.8122\n",
      "Epoch [75/200], Loss: 0.7979\n",
      "Epoch [80/200], Loss: 0.7838\n",
      "Epoch [85/200], Loss: 0.7702\n",
      "Epoch [90/200], Loss: 0.7570\n",
      "Epoch [95/200], Loss: 0.7441\n",
      "Epoch [100/200], Loss: 0.7315\n",
      "Epoch [105/200], Loss: 0.7193\n",
      "Epoch [110/200], Loss: 0.7074\n",
      "Epoch [115/200], Loss: 0.6960\n",
      "Epoch [120/200], Loss: 0.6849\n",
      "Epoch [125/200], Loss: 0.6742\n",
      "Epoch [130/200], Loss: 0.6638\n",
      "Epoch [135/200], Loss: 0.6537\n",
      "Epoch [140/200], Loss: 0.6439\n",
      "Epoch [145/200], Loss: 0.6345\n",
      "Epoch [150/200], Loss: 0.6253\n",
      "Epoch [155/200], Loss: 0.6164\n",
      "Epoch [160/200], Loss: 0.6078\n",
      "Epoch [165/200], Loss: 0.5995\n",
      "Epoch [170/200], Loss: 0.5914\n",
      "Epoch [175/200], Loss: 0.5836\n",
      "Epoch [180/200], Loss: 0.5760\n",
      "Epoch [185/200], Loss: 0.5686\n",
      "Epoch [190/200], Loss: 0.5615\n",
      "Epoch [195/200], Loss: 0.5546\n",
      "Epoch [200/200], Loss: 0.5480\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4925\n",
      "Epoch [5/210], Loss: 1.2706\n",
      "Epoch [10/210], Loss: 1.0961\n",
      "Epoch [15/210], Loss: 1.0366\n",
      "Epoch [20/210], Loss: 1.0028\n",
      "Epoch [25/210], Loss: 0.9762\n",
      "Epoch [30/210], Loss: 0.9529\n",
      "Epoch [35/210], Loss: 0.9317\n",
      "Epoch [40/210], Loss: 0.9119\n",
      "Epoch [45/210], Loss: 0.8932\n",
      "Epoch [50/210], Loss: 0.8756\n",
      "Epoch [55/210], Loss: 0.8588\n",
      "Epoch [60/210], Loss: 0.8426\n",
      "Epoch [65/210], Loss: 0.8270\n",
      "Epoch [70/210], Loss: 0.8122\n",
      "Epoch [75/210], Loss: 0.7979\n",
      "Epoch [80/210], Loss: 0.7838\n",
      "Epoch [85/210], Loss: 0.7702\n",
      "Epoch [90/210], Loss: 0.7570\n",
      "Epoch [95/210], Loss: 0.7441\n",
      "Epoch [100/210], Loss: 0.7315\n",
      "Epoch [105/210], Loss: 0.7193\n",
      "Epoch [110/210], Loss: 0.7074\n",
      "Epoch [115/210], Loss: 0.6960\n",
      "Epoch [120/210], Loss: 0.6849\n",
      "Epoch [125/210], Loss: 0.6742\n",
      "Epoch [130/210], Loss: 0.6638\n",
      "Epoch [135/210], Loss: 0.6537\n",
      "Epoch [140/210], Loss: 0.6439\n",
      "Epoch [145/210], Loss: 0.6345\n",
      "Epoch [150/210], Loss: 0.6253\n",
      "Epoch [155/210], Loss: 0.6164\n",
      "Epoch [160/210], Loss: 0.6078\n",
      "Epoch [165/210], Loss: 0.5995\n",
      "Epoch [170/210], Loss: 0.5914\n",
      "Epoch [175/210], Loss: 0.5836\n",
      "Epoch [180/210], Loss: 0.5760\n",
      "Epoch [185/210], Loss: 0.5686\n",
      "Epoch [190/210], Loss: 0.5615\n",
      "Epoch [195/210], Loss: 0.5546\n",
      "Epoch [200/210], Loss: 0.5480\n",
      "Epoch [205/210], Loss: 0.5415\n",
      "Epoch [210/210], Loss: 0.5353\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4790\n",
      "Epoch [5/220], Loss: 1.2706\n",
      "Epoch [10/220], Loss: 1.0961\n",
      "Epoch [15/220], Loss: 1.0366\n",
      "Epoch [20/220], Loss: 1.0028\n",
      "Epoch [25/220], Loss: 0.9762\n",
      "Epoch [30/220], Loss: 0.9529\n",
      "Epoch [35/220], Loss: 0.9317\n",
      "Epoch [40/220], Loss: 0.9119\n",
      "Epoch [45/220], Loss: 0.8932\n",
      "Epoch [50/220], Loss: 0.8756\n",
      "Epoch [55/220], Loss: 0.8588\n",
      "Epoch [60/220], Loss: 0.8426\n",
      "Epoch [65/220], Loss: 0.8270\n",
      "Epoch [70/220], Loss: 0.8122\n",
      "Epoch [75/220], Loss: 0.7979\n",
      "Epoch [80/220], Loss: 0.7838\n",
      "Epoch [85/220], Loss: 0.7702\n",
      "Epoch [90/220], Loss: 0.7570\n",
      "Epoch [95/220], Loss: 0.7441\n",
      "Epoch [100/220], Loss: 0.7315\n",
      "Epoch [105/220], Loss: 0.7193\n",
      "Epoch [110/220], Loss: 0.7074\n",
      "Epoch [115/220], Loss: 0.6960\n",
      "Epoch [120/220], Loss: 0.6849\n",
      "Epoch [125/220], Loss: 0.6742\n",
      "Epoch [130/220], Loss: 0.6638\n",
      "Epoch [135/220], Loss: 0.6537\n",
      "Epoch [140/220], Loss: 0.6439\n",
      "Epoch [145/220], Loss: 0.6345\n",
      "Epoch [150/220], Loss: 0.6253\n",
      "Epoch [155/220], Loss: 0.6164\n",
      "Epoch [160/220], Loss: 0.6078\n",
      "Epoch [165/220], Loss: 0.5995\n",
      "Epoch [170/220], Loss: 0.5914\n",
      "Epoch [175/220], Loss: 0.5836\n",
      "Epoch [180/220], Loss: 0.5760\n",
      "Epoch [185/220], Loss: 0.5686\n",
      "Epoch [190/220], Loss: 0.5615\n",
      "Epoch [195/220], Loss: 0.5546\n",
      "Epoch [200/220], Loss: 0.5480\n",
      "Epoch [205/220], Loss: 0.5415\n",
      "Epoch [210/220], Loss: 0.5353\n",
      "Epoch [215/220], Loss: 0.5292\n",
      "Epoch [220/220], Loss: 0.5234\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4663\n",
      "Epoch [5/230], Loss: 1.2706\n",
      "Epoch [10/230], Loss: 1.0961\n",
      "Epoch [15/230], Loss: 1.0366\n",
      "Epoch [20/230], Loss: 1.0028\n",
      "Epoch [25/230], Loss: 0.9762\n",
      "Epoch [30/230], Loss: 0.9529\n",
      "Epoch [35/230], Loss: 0.9317\n",
      "Epoch [40/230], Loss: 0.9119\n",
      "Epoch [45/230], Loss: 0.8932\n",
      "Epoch [50/230], Loss: 0.8756\n",
      "Epoch [55/230], Loss: 0.8588\n",
      "Epoch [60/230], Loss: 0.8426\n",
      "Epoch [65/230], Loss: 0.8270\n",
      "Epoch [70/230], Loss: 0.8122\n",
      "Epoch [75/230], Loss: 0.7979\n",
      "Epoch [80/230], Loss: 0.7838\n",
      "Epoch [85/230], Loss: 0.7702\n",
      "Epoch [90/230], Loss: 0.7570\n",
      "Epoch [95/230], Loss: 0.7441\n",
      "Epoch [100/230], Loss: 0.7315\n",
      "Epoch [105/230], Loss: 0.7193\n",
      "Epoch [110/230], Loss: 0.7074\n",
      "Epoch [115/230], Loss: 0.6960\n",
      "Epoch [120/230], Loss: 0.6849\n",
      "Epoch [125/230], Loss: 0.6742\n",
      "Epoch [130/230], Loss: 0.6638\n",
      "Epoch [135/230], Loss: 0.6537\n",
      "Epoch [140/230], Loss: 0.6439\n",
      "Epoch [145/230], Loss: 0.6345\n",
      "Epoch [150/230], Loss: 0.6253\n",
      "Epoch [155/230], Loss: 0.6164\n",
      "Epoch [160/230], Loss: 0.6078\n",
      "Epoch [165/230], Loss: 0.5995\n",
      "Epoch [170/230], Loss: 0.5914\n",
      "Epoch [175/230], Loss: 0.5836\n",
      "Epoch [180/230], Loss: 0.5760\n",
      "Epoch [185/230], Loss: 0.5686\n",
      "Epoch [190/230], Loss: 0.5615\n",
      "Epoch [195/230], Loss: 0.5546\n",
      "Epoch [200/230], Loss: 0.5480\n",
      "Epoch [205/230], Loss: 0.5415\n",
      "Epoch [210/230], Loss: 0.5353\n",
      "Epoch [215/230], Loss: 0.5292\n",
      "Epoch [220/230], Loss: 0.5234\n",
      "Epoch [225/230], Loss: 0.5177\n",
      "Epoch [230/230], Loss: 0.5123\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4544\n",
      "Epoch [5/240], Loss: 1.2706\n",
      "Epoch [10/240], Loss: 1.0961\n",
      "Epoch [15/240], Loss: 1.0366\n",
      "Epoch [20/240], Loss: 1.0028\n",
      "Epoch [25/240], Loss: 0.9762\n",
      "Epoch [30/240], Loss: 0.9529\n",
      "Epoch [35/240], Loss: 0.9317\n",
      "Epoch [40/240], Loss: 0.9119\n",
      "Epoch [45/240], Loss: 0.8932\n",
      "Epoch [50/240], Loss: 0.8756\n",
      "Epoch [55/240], Loss: 0.8588\n",
      "Epoch [60/240], Loss: 0.8426\n",
      "Epoch [65/240], Loss: 0.8270\n",
      "Epoch [70/240], Loss: 0.8122\n",
      "Epoch [75/240], Loss: 0.7979\n",
      "Epoch [80/240], Loss: 0.7838\n",
      "Epoch [85/240], Loss: 0.7702\n",
      "Epoch [90/240], Loss: 0.7570\n",
      "Epoch [95/240], Loss: 0.7441\n",
      "Epoch [100/240], Loss: 0.7315\n",
      "Epoch [105/240], Loss: 0.7193\n",
      "Epoch [110/240], Loss: 0.7074\n",
      "Epoch [115/240], Loss: 0.6960\n",
      "Epoch [120/240], Loss: 0.6849\n",
      "Epoch [125/240], Loss: 0.6742\n",
      "Epoch [130/240], Loss: 0.6638\n",
      "Epoch [135/240], Loss: 0.6537\n",
      "Epoch [140/240], Loss: 0.6439\n",
      "Epoch [145/240], Loss: 0.6345\n",
      "Epoch [150/240], Loss: 0.6253\n",
      "Epoch [155/240], Loss: 0.6164\n",
      "Epoch [160/240], Loss: 0.6078\n",
      "Epoch [165/240], Loss: 0.5995\n",
      "Epoch [170/240], Loss: 0.5914\n",
      "Epoch [175/240], Loss: 0.5836\n",
      "Epoch [180/240], Loss: 0.5760\n",
      "Epoch [185/240], Loss: 0.5686\n",
      "Epoch [190/240], Loss: 0.5615\n",
      "Epoch [195/240], Loss: 0.5546\n",
      "Epoch [200/240], Loss: 0.5480\n",
      "Epoch [205/240], Loss: 0.5415\n",
      "Epoch [210/240], Loss: 0.5353\n",
      "Epoch [215/240], Loss: 0.5292\n",
      "Epoch [220/240], Loss: 0.5234\n",
      "Epoch [225/240], Loss: 0.5177\n",
      "Epoch [230/240], Loss: 0.5123\n",
      "Epoch [235/240], Loss: 0.5069\n",
      "Epoch [240/240], Loss: 0.5018\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4431\n",
      "Epoch [5/250], Loss: 1.2706\n",
      "Epoch [10/250], Loss: 1.0961\n",
      "Epoch [15/250], Loss: 1.0366\n",
      "Epoch [20/250], Loss: 1.0028\n",
      "Epoch [25/250], Loss: 0.9762\n",
      "Epoch [30/250], Loss: 0.9529\n",
      "Epoch [35/250], Loss: 0.9317\n",
      "Epoch [40/250], Loss: 0.9119\n",
      "Epoch [45/250], Loss: 0.8932\n",
      "Epoch [50/250], Loss: 0.8756\n",
      "Epoch [55/250], Loss: 0.8588\n",
      "Epoch [60/250], Loss: 0.8426\n",
      "Epoch [65/250], Loss: 0.8270\n",
      "Epoch [70/250], Loss: 0.8122\n",
      "Epoch [75/250], Loss: 0.7979\n",
      "Epoch [80/250], Loss: 0.7838\n",
      "Epoch [85/250], Loss: 0.7702\n",
      "Epoch [90/250], Loss: 0.7570\n",
      "Epoch [95/250], Loss: 0.7441\n",
      "Epoch [100/250], Loss: 0.7315\n",
      "Epoch [105/250], Loss: 0.7193\n",
      "Epoch [110/250], Loss: 0.7074\n",
      "Epoch [115/250], Loss: 0.6960\n",
      "Epoch [120/250], Loss: 0.6849\n",
      "Epoch [125/250], Loss: 0.6742\n",
      "Epoch [130/250], Loss: 0.6638\n",
      "Epoch [135/250], Loss: 0.6537\n",
      "Epoch [140/250], Loss: 0.6439\n",
      "Epoch [145/250], Loss: 0.6345\n",
      "Epoch [150/250], Loss: 0.6253\n",
      "Epoch [155/250], Loss: 0.6164\n",
      "Epoch [160/250], Loss: 0.6078\n",
      "Epoch [165/250], Loss: 0.5995\n",
      "Epoch [170/250], Loss: 0.5914\n",
      "Epoch [175/250], Loss: 0.5836\n",
      "Epoch [180/250], Loss: 0.5760\n",
      "Epoch [185/250], Loss: 0.5686\n",
      "Epoch [190/250], Loss: 0.5615\n",
      "Epoch [195/250], Loss: 0.5546\n",
      "Epoch [200/250], Loss: 0.5480\n",
      "Epoch [205/250], Loss: 0.5415\n",
      "Epoch [210/250], Loss: 0.5353\n",
      "Epoch [215/250], Loss: 0.5292\n",
      "Epoch [220/250], Loss: 0.5234\n",
      "Epoch [225/250], Loss: 0.5177\n",
      "Epoch [230/250], Loss: 0.5123\n",
      "Epoch [235/250], Loss: 0.5069\n",
      "Epoch [240/250], Loss: 0.5018\n",
      "Epoch [245/250], Loss: 0.4968\n",
      "Epoch [250/250], Loss: 0.4919\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4324\n",
      "Epoch [5/260], Loss: 1.2706\n",
      "Epoch [10/260], Loss: 1.0961\n",
      "Epoch [15/260], Loss: 1.0366\n",
      "Epoch [20/260], Loss: 1.0028\n",
      "Epoch [25/260], Loss: 0.9762\n",
      "Epoch [30/260], Loss: 0.9529\n",
      "Epoch [35/260], Loss: 0.9317\n",
      "Epoch [40/260], Loss: 0.9119\n",
      "Epoch [45/260], Loss: 0.8932\n",
      "Epoch [50/260], Loss: 0.8756\n",
      "Epoch [55/260], Loss: 0.8588\n",
      "Epoch [60/260], Loss: 0.8426\n",
      "Epoch [65/260], Loss: 0.8270\n",
      "Epoch [70/260], Loss: 0.8122\n",
      "Epoch [75/260], Loss: 0.7979\n",
      "Epoch [80/260], Loss: 0.7838\n",
      "Epoch [85/260], Loss: 0.7702\n",
      "Epoch [90/260], Loss: 0.7570\n",
      "Epoch [95/260], Loss: 0.7441\n",
      "Epoch [100/260], Loss: 0.7315\n",
      "Epoch [105/260], Loss: 0.7193\n",
      "Epoch [110/260], Loss: 0.7074\n",
      "Epoch [115/260], Loss: 0.6960\n",
      "Epoch [120/260], Loss: 0.6849\n",
      "Epoch [125/260], Loss: 0.6742\n",
      "Epoch [130/260], Loss: 0.6638\n",
      "Epoch [135/260], Loss: 0.6537\n",
      "Epoch [140/260], Loss: 0.6439\n",
      "Epoch [145/260], Loss: 0.6345\n",
      "Epoch [150/260], Loss: 0.6253\n",
      "Epoch [155/260], Loss: 0.6164\n",
      "Epoch [160/260], Loss: 0.6078\n",
      "Epoch [165/260], Loss: 0.5995\n",
      "Epoch [170/260], Loss: 0.5914\n",
      "Epoch [175/260], Loss: 0.5836\n",
      "Epoch [180/260], Loss: 0.5760\n",
      "Epoch [185/260], Loss: 0.5686\n",
      "Epoch [190/260], Loss: 0.5615\n",
      "Epoch [195/260], Loss: 0.5546\n",
      "Epoch [200/260], Loss: 0.5480\n",
      "Epoch [205/260], Loss: 0.5415\n",
      "Epoch [210/260], Loss: 0.5353\n",
      "Epoch [215/260], Loss: 0.5292\n",
      "Epoch [220/260], Loss: 0.5234\n",
      "Epoch [225/260], Loss: 0.5177\n",
      "Epoch [230/260], Loss: 0.5123\n",
      "Epoch [235/260], Loss: 0.5069\n",
      "Epoch [240/260], Loss: 0.5018\n",
      "Epoch [245/260], Loss: 0.4968\n",
      "Epoch [250/260], Loss: 0.4919\n",
      "Epoch [255/260], Loss: 0.4872\n",
      "Epoch [260/260], Loss: 0.4826\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4223\n",
      "Epoch [5/270], Loss: 1.2706\n",
      "Epoch [10/270], Loss: 1.0961\n",
      "Epoch [15/270], Loss: 1.0366\n",
      "Epoch [20/270], Loss: 1.0028\n",
      "Epoch [25/270], Loss: 0.9762\n",
      "Epoch [30/270], Loss: 0.9529\n",
      "Epoch [35/270], Loss: 0.9317\n",
      "Epoch [40/270], Loss: 0.9119\n",
      "Epoch [45/270], Loss: 0.8932\n",
      "Epoch [50/270], Loss: 0.8756\n",
      "Epoch [55/270], Loss: 0.8588\n",
      "Epoch [60/270], Loss: 0.8426\n",
      "Epoch [65/270], Loss: 0.8270\n",
      "Epoch [70/270], Loss: 0.8122\n",
      "Epoch [75/270], Loss: 0.7979\n",
      "Epoch [80/270], Loss: 0.7838\n",
      "Epoch [85/270], Loss: 0.7702\n",
      "Epoch [90/270], Loss: 0.7570\n",
      "Epoch [95/270], Loss: 0.7441\n",
      "Epoch [100/270], Loss: 0.7315\n",
      "Epoch [105/270], Loss: 0.7193\n",
      "Epoch [110/270], Loss: 0.7074\n",
      "Epoch [115/270], Loss: 0.6960\n",
      "Epoch [120/270], Loss: 0.6849\n",
      "Epoch [125/270], Loss: 0.6742\n",
      "Epoch [130/270], Loss: 0.6638\n",
      "Epoch [135/270], Loss: 0.6537\n",
      "Epoch [140/270], Loss: 0.6439\n",
      "Epoch [145/270], Loss: 0.6345\n",
      "Epoch [150/270], Loss: 0.6253\n",
      "Epoch [155/270], Loss: 0.6164\n",
      "Epoch [160/270], Loss: 0.6078\n",
      "Epoch [165/270], Loss: 0.5995\n",
      "Epoch [170/270], Loss: 0.5914\n",
      "Epoch [175/270], Loss: 0.5836\n",
      "Epoch [180/270], Loss: 0.5760\n",
      "Epoch [185/270], Loss: 0.5686\n",
      "Epoch [190/270], Loss: 0.5615\n",
      "Epoch [195/270], Loss: 0.5546\n",
      "Epoch [200/270], Loss: 0.5480\n",
      "Epoch [205/270], Loss: 0.5415\n",
      "Epoch [210/270], Loss: 0.5353\n",
      "Epoch [215/270], Loss: 0.5292\n",
      "Epoch [220/270], Loss: 0.5234\n",
      "Epoch [225/270], Loss: 0.5177\n",
      "Epoch [230/270], Loss: 0.5123\n",
      "Epoch [235/270], Loss: 0.5069\n",
      "Epoch [240/270], Loss: 0.5018\n",
      "Epoch [245/270], Loss: 0.4968\n",
      "Epoch [250/270], Loss: 0.4919\n",
      "Epoch [255/270], Loss: 0.4872\n",
      "Epoch [260/270], Loss: 0.4826\n",
      "Epoch [265/270], Loss: 0.4782\n",
      "Epoch [270/270], Loss: 0.4738\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4127\n",
      "Epoch [5/280], Loss: 1.2706\n",
      "Epoch [10/280], Loss: 1.0961\n",
      "Epoch [15/280], Loss: 1.0366\n",
      "Epoch [20/280], Loss: 1.0028\n",
      "Epoch [25/280], Loss: 0.9762\n",
      "Epoch [30/280], Loss: 0.9529\n",
      "Epoch [35/280], Loss: 0.9317\n",
      "Epoch [40/280], Loss: 0.9119\n",
      "Epoch [45/280], Loss: 0.8932\n",
      "Epoch [50/280], Loss: 0.8756\n",
      "Epoch [55/280], Loss: 0.8588\n",
      "Epoch [60/280], Loss: 0.8426\n",
      "Epoch [65/280], Loss: 0.8270\n",
      "Epoch [70/280], Loss: 0.8122\n",
      "Epoch [75/280], Loss: 0.7979\n",
      "Epoch [80/280], Loss: 0.7838\n",
      "Epoch [85/280], Loss: 0.7702\n",
      "Epoch [90/280], Loss: 0.7570\n",
      "Epoch [95/280], Loss: 0.7441\n",
      "Epoch [100/280], Loss: 0.7315\n",
      "Epoch [105/280], Loss: 0.7193\n",
      "Epoch [110/280], Loss: 0.7074\n",
      "Epoch [115/280], Loss: 0.6960\n",
      "Epoch [120/280], Loss: 0.6849\n",
      "Epoch [125/280], Loss: 0.6742\n",
      "Epoch [130/280], Loss: 0.6638\n",
      "Epoch [135/280], Loss: 0.6537\n",
      "Epoch [140/280], Loss: 0.6439\n",
      "Epoch [145/280], Loss: 0.6345\n",
      "Epoch [150/280], Loss: 0.6253\n",
      "Epoch [155/280], Loss: 0.6164\n",
      "Epoch [160/280], Loss: 0.6078\n",
      "Epoch [165/280], Loss: 0.5995\n",
      "Epoch [170/280], Loss: 0.5914\n",
      "Epoch [175/280], Loss: 0.5836\n",
      "Epoch [180/280], Loss: 0.5760\n",
      "Epoch [185/280], Loss: 0.5686\n",
      "Epoch [190/280], Loss: 0.5615\n",
      "Epoch [195/280], Loss: 0.5546\n",
      "Epoch [200/280], Loss: 0.5480\n",
      "Epoch [205/280], Loss: 0.5415\n",
      "Epoch [210/280], Loss: 0.5353\n",
      "Epoch [215/280], Loss: 0.5292\n",
      "Epoch [220/280], Loss: 0.5234\n",
      "Epoch [225/280], Loss: 0.5177\n",
      "Epoch [230/280], Loss: 0.5123\n",
      "Epoch [235/280], Loss: 0.5069\n",
      "Epoch [240/280], Loss: 0.5018\n",
      "Epoch [245/280], Loss: 0.4968\n",
      "Epoch [250/280], Loss: 0.4919\n",
      "Epoch [255/280], Loss: 0.4872\n",
      "Epoch [260/280], Loss: 0.4826\n",
      "Epoch [265/280], Loss: 0.4782\n",
      "Epoch [270/280], Loss: 0.4738\n",
      "Epoch [275/280], Loss: 0.4696\n",
      "Epoch [280/280], Loss: 0.4655\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.4036\n",
      "Epoch [5/290], Loss: 1.2706\n",
      "Epoch [10/290], Loss: 1.0961\n",
      "Epoch [15/290], Loss: 1.0366\n",
      "Epoch [20/290], Loss: 1.0028\n",
      "Epoch [25/290], Loss: 0.9762\n",
      "Epoch [30/290], Loss: 0.9529\n",
      "Epoch [35/290], Loss: 0.9317\n",
      "Epoch [40/290], Loss: 0.9119\n",
      "Epoch [45/290], Loss: 0.8932\n",
      "Epoch [50/290], Loss: 0.8756\n",
      "Epoch [55/290], Loss: 0.8588\n",
      "Epoch [60/290], Loss: 0.8426\n",
      "Epoch [65/290], Loss: 0.8270\n",
      "Epoch [70/290], Loss: 0.8122\n",
      "Epoch [75/290], Loss: 0.7979\n",
      "Epoch [80/290], Loss: 0.7838\n",
      "Epoch [85/290], Loss: 0.7702\n",
      "Epoch [90/290], Loss: 0.7570\n",
      "Epoch [95/290], Loss: 0.7441\n",
      "Epoch [100/290], Loss: 0.7315\n",
      "Epoch [105/290], Loss: 0.7193\n",
      "Epoch [110/290], Loss: 0.7074\n",
      "Epoch [115/290], Loss: 0.6960\n",
      "Epoch [120/290], Loss: 0.6849\n",
      "Epoch [125/290], Loss: 0.6742\n",
      "Epoch [130/290], Loss: 0.6638\n",
      "Epoch [135/290], Loss: 0.6537\n",
      "Epoch [140/290], Loss: 0.6439\n",
      "Epoch [145/290], Loss: 0.6345\n",
      "Epoch [150/290], Loss: 0.6253\n",
      "Epoch [155/290], Loss: 0.6164\n",
      "Epoch [160/290], Loss: 0.6078\n",
      "Epoch [165/290], Loss: 0.5995\n",
      "Epoch [170/290], Loss: 0.5914\n",
      "Epoch [175/290], Loss: 0.5836\n",
      "Epoch [180/290], Loss: 0.5760\n",
      "Epoch [185/290], Loss: 0.5686\n",
      "Epoch [190/290], Loss: 0.5615\n",
      "Epoch [195/290], Loss: 0.5546\n",
      "Epoch [200/290], Loss: 0.5480\n",
      "Epoch [205/290], Loss: 0.5415\n",
      "Epoch [210/290], Loss: 0.5353\n",
      "Epoch [215/290], Loss: 0.5292\n",
      "Epoch [220/290], Loss: 0.5234\n",
      "Epoch [225/290], Loss: 0.5177\n",
      "Epoch [230/290], Loss: 0.5123\n",
      "Epoch [235/290], Loss: 0.5069\n",
      "Epoch [240/290], Loss: 0.5018\n",
      "Epoch [245/290], Loss: 0.4968\n",
      "Epoch [250/290], Loss: 0.4919\n",
      "Epoch [255/290], Loss: 0.4872\n",
      "Epoch [260/290], Loss: 0.4826\n",
      "Epoch [265/290], Loss: 0.4782\n",
      "Epoch [270/290], Loss: 0.4738\n",
      "Epoch [275/290], Loss: 0.4696\n",
      "Epoch [280/290], Loss: 0.4655\n",
      "Epoch [285/290], Loss: 0.4616\n",
      "Epoch [290/290], Loss: 0.4576\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.3949\n",
      "Epoch [5/300], Loss: 1.2706\n",
      "Epoch [10/300], Loss: 1.0961\n",
      "Epoch [15/300], Loss: 1.0366\n",
      "Epoch [20/300], Loss: 1.0028\n",
      "Epoch [25/300], Loss: 0.9762\n",
      "Epoch [30/300], Loss: 0.9529\n",
      "Epoch [35/300], Loss: 0.9317\n",
      "Epoch [40/300], Loss: 0.9119\n",
      "Epoch [45/300], Loss: 0.8932\n",
      "Epoch [50/300], Loss: 0.8756\n",
      "Epoch [55/300], Loss: 0.8588\n",
      "Epoch [60/300], Loss: 0.8426\n",
      "Epoch [65/300], Loss: 0.8270\n",
      "Epoch [70/300], Loss: 0.8122\n",
      "Epoch [75/300], Loss: 0.7979\n",
      "Epoch [80/300], Loss: 0.7838\n",
      "Epoch [85/300], Loss: 0.7702\n",
      "Epoch [90/300], Loss: 0.7570\n",
      "Epoch [95/300], Loss: 0.7441\n",
      "Epoch [100/300], Loss: 0.7315\n",
      "Epoch [105/300], Loss: 0.7193\n",
      "Epoch [110/300], Loss: 0.7074\n",
      "Epoch [115/300], Loss: 0.6960\n",
      "Epoch [120/300], Loss: 0.6849\n",
      "Epoch [125/300], Loss: 0.6742\n",
      "Epoch [130/300], Loss: 0.6638\n",
      "Epoch [135/300], Loss: 0.6537\n",
      "Epoch [140/300], Loss: 0.6439\n",
      "Epoch [145/300], Loss: 0.6345\n",
      "Epoch [150/300], Loss: 0.6253\n",
      "Epoch [155/300], Loss: 0.6164\n",
      "Epoch [160/300], Loss: 0.6078\n",
      "Epoch [165/300], Loss: 0.5995\n",
      "Epoch [170/300], Loss: 0.5914\n",
      "Epoch [175/300], Loss: 0.5836\n",
      "Epoch [180/300], Loss: 0.5760\n",
      "Epoch [185/300], Loss: 0.5686\n",
      "Epoch [190/300], Loss: 0.5615\n",
      "Epoch [195/300], Loss: 0.5546\n",
      "Epoch [200/300], Loss: 0.5480\n",
      "Epoch [205/300], Loss: 0.5415\n",
      "Epoch [210/300], Loss: 0.5353\n",
      "Epoch [215/300], Loss: 0.5292\n",
      "Epoch [220/300], Loss: 0.5234\n",
      "Epoch [225/300], Loss: 0.5177\n",
      "Epoch [230/300], Loss: 0.5123\n",
      "Epoch [235/300], Loss: 0.5069\n",
      "Epoch [240/300], Loss: 0.5018\n",
      "Epoch [245/300], Loss: 0.4968\n",
      "Epoch [250/300], Loss: 0.4919\n",
      "Epoch [255/300], Loss: 0.4872\n",
      "Epoch [260/300], Loss: 0.4826\n",
      "Epoch [265/300], Loss: 0.4782\n",
      "Epoch [270/300], Loss: 0.4738\n",
      "Epoch [275/300], Loss: 0.4696\n",
      "Epoch [280/300], Loss: 0.4655\n",
      "Epoch [285/300], Loss: 0.4616\n",
      "Epoch [290/300], Loss: 0.4576\n",
      "Epoch [295/300], Loss: 0.4537\n",
      "Epoch [300/300], Loss: 0.4499\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.3866\n",
      "Epoch [5/310], Loss: 1.2706\n",
      "Epoch [10/310], Loss: 1.0961\n",
      "Epoch [15/310], Loss: 1.0366\n",
      "Epoch [20/310], Loss: 1.0028\n",
      "Epoch [25/310], Loss: 0.9762\n",
      "Epoch [30/310], Loss: 0.9529\n",
      "Epoch [35/310], Loss: 0.9317\n",
      "Epoch [40/310], Loss: 0.9119\n",
      "Epoch [45/310], Loss: 0.8932\n",
      "Epoch [50/310], Loss: 0.8756\n",
      "Epoch [55/310], Loss: 0.8588\n",
      "Epoch [60/310], Loss: 0.8426\n",
      "Epoch [65/310], Loss: 0.8270\n",
      "Epoch [70/310], Loss: 0.8122\n",
      "Epoch [75/310], Loss: 0.7979\n",
      "Epoch [80/310], Loss: 0.7838\n",
      "Epoch [85/310], Loss: 0.7702\n",
      "Epoch [90/310], Loss: 0.7570\n",
      "Epoch [95/310], Loss: 0.7441\n",
      "Epoch [100/310], Loss: 0.7315\n",
      "Epoch [105/310], Loss: 0.7193\n",
      "Epoch [110/310], Loss: 0.7074\n",
      "Epoch [115/310], Loss: 0.6960\n",
      "Epoch [120/310], Loss: 0.6849\n",
      "Epoch [125/310], Loss: 0.6742\n",
      "Epoch [130/310], Loss: 0.6638\n",
      "Epoch [135/310], Loss: 0.6537\n",
      "Epoch [140/310], Loss: 0.6439\n",
      "Epoch [145/310], Loss: 0.6345\n",
      "Epoch [150/310], Loss: 0.6253\n",
      "Epoch [155/310], Loss: 0.6164\n",
      "Epoch [160/310], Loss: 0.6078\n",
      "Epoch [165/310], Loss: 0.5995\n",
      "Epoch [170/310], Loss: 0.5914\n",
      "Epoch [175/310], Loss: 0.5836\n",
      "Epoch [180/310], Loss: 0.5760\n",
      "Epoch [185/310], Loss: 0.5686\n",
      "Epoch [190/310], Loss: 0.5615\n",
      "Epoch [195/310], Loss: 0.5546\n",
      "Epoch [200/310], Loss: 0.5480\n",
      "Epoch [205/310], Loss: 0.5415\n",
      "Epoch [210/310], Loss: 0.5353\n",
      "Epoch [215/310], Loss: 0.5292\n",
      "Epoch [220/310], Loss: 0.5234\n",
      "Epoch [225/310], Loss: 0.5177\n",
      "Epoch [230/310], Loss: 0.5123\n",
      "Epoch [235/310], Loss: 0.5069\n",
      "Epoch [240/310], Loss: 0.5018\n",
      "Epoch [245/310], Loss: 0.4968\n",
      "Epoch [250/310], Loss: 0.4919\n",
      "Epoch [255/310], Loss: 0.4872\n",
      "Epoch [260/310], Loss: 0.4826\n",
      "Epoch [265/310], Loss: 0.4782\n",
      "Epoch [270/310], Loss: 0.4738\n",
      "Epoch [275/310], Loss: 0.4696\n",
      "Epoch [280/310], Loss: 0.4655\n",
      "Epoch [285/310], Loss: 0.4616\n",
      "Epoch [290/310], Loss: 0.4576\n",
      "Epoch [295/310], Loss: 0.4537\n",
      "Epoch [300/310], Loss: 0.4499\n",
      "Epoch [305/310], Loss: 0.4462\n",
      "Epoch [310/310], Loss: 0.4426\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.3788\n",
      "Epoch [5/320], Loss: 1.2706\n",
      "Epoch [10/320], Loss: 1.0961\n",
      "Epoch [15/320], Loss: 1.0366\n",
      "Epoch [20/320], Loss: 1.0028\n",
      "Epoch [25/320], Loss: 0.9762\n",
      "Epoch [30/320], Loss: 0.9529\n",
      "Epoch [35/320], Loss: 0.9317\n",
      "Epoch [40/320], Loss: 0.9119\n",
      "Epoch [45/320], Loss: 0.8932\n",
      "Epoch [50/320], Loss: 0.8756\n",
      "Epoch [55/320], Loss: 0.8588\n",
      "Epoch [60/320], Loss: 0.8426\n",
      "Epoch [65/320], Loss: 0.8270\n",
      "Epoch [70/320], Loss: 0.8122\n",
      "Epoch [75/320], Loss: 0.7979\n",
      "Epoch [80/320], Loss: 0.7838\n",
      "Epoch [85/320], Loss: 0.7702\n",
      "Epoch [90/320], Loss: 0.7570\n",
      "Epoch [95/320], Loss: 0.7441\n",
      "Epoch [100/320], Loss: 0.7315\n",
      "Epoch [105/320], Loss: 0.7193\n",
      "Epoch [110/320], Loss: 0.7074\n",
      "Epoch [115/320], Loss: 0.6960\n",
      "Epoch [120/320], Loss: 0.6849\n",
      "Epoch [125/320], Loss: 0.6742\n",
      "Epoch [130/320], Loss: 0.6638\n",
      "Epoch [135/320], Loss: 0.6537\n",
      "Epoch [140/320], Loss: 0.6439\n",
      "Epoch [145/320], Loss: 0.6345\n",
      "Epoch [150/320], Loss: 0.6253\n",
      "Epoch [155/320], Loss: 0.6164\n",
      "Epoch [160/320], Loss: 0.6078\n",
      "Epoch [165/320], Loss: 0.5995\n",
      "Epoch [170/320], Loss: 0.5914\n",
      "Epoch [175/320], Loss: 0.5836\n",
      "Epoch [180/320], Loss: 0.5760\n",
      "Epoch [185/320], Loss: 0.5686\n",
      "Epoch [190/320], Loss: 0.5615\n",
      "Epoch [195/320], Loss: 0.5546\n",
      "Epoch [200/320], Loss: 0.5480\n",
      "Epoch [205/320], Loss: 0.5415\n",
      "Epoch [210/320], Loss: 0.5353\n",
      "Epoch [215/320], Loss: 0.5292\n",
      "Epoch [220/320], Loss: 0.5234\n",
      "Epoch [225/320], Loss: 0.5177\n",
      "Epoch [230/320], Loss: 0.5123\n",
      "Epoch [235/320], Loss: 0.5069\n",
      "Epoch [240/320], Loss: 0.5018\n",
      "Epoch [245/320], Loss: 0.4968\n",
      "Epoch [250/320], Loss: 0.4919\n",
      "Epoch [255/320], Loss: 0.4872\n",
      "Epoch [260/320], Loss: 0.4826\n",
      "Epoch [265/320], Loss: 0.4782\n",
      "Epoch [270/320], Loss: 0.4738\n",
      "Epoch [275/320], Loss: 0.4696\n",
      "Epoch [280/320], Loss: 0.4655\n",
      "Epoch [285/320], Loss: 0.4616\n",
      "Epoch [290/320], Loss: 0.4576\n",
      "Epoch [295/320], Loss: 0.4537\n",
      "Epoch [300/320], Loss: 0.4499\n",
      "Epoch [305/320], Loss: 0.4462\n",
      "Epoch [310/320], Loss: 0.4426\n",
      "Epoch [315/320], Loss: 0.4391\n",
      "Epoch [320/320], Loss: 0.4357\n",
      "Training complete.\n",
      "Test Accuracy: 90.00%\n",
      "Test Loss (Cross-Entropy): 0.3714\n",
      "Epoch [5/330], Loss: 1.2706\n",
      "Epoch [10/330], Loss: 1.0961\n",
      "Epoch [15/330], Loss: 1.0366\n",
      "Epoch [20/330], Loss: 1.0028\n",
      "Epoch [25/330], Loss: 0.9762\n",
      "Epoch [30/330], Loss: 0.9529\n",
      "Epoch [35/330], Loss: 0.9317\n",
      "Epoch [40/330], Loss: 0.9119\n",
      "Epoch [45/330], Loss: 0.8932\n",
      "Epoch [50/330], Loss: 0.8756\n",
      "Epoch [55/330], Loss: 0.8588\n",
      "Epoch [60/330], Loss: 0.8426\n",
      "Epoch [65/330], Loss: 0.8270\n",
      "Epoch [70/330], Loss: 0.8122\n",
      "Epoch [75/330], Loss: 0.7979\n",
      "Epoch [80/330], Loss: 0.7838\n",
      "Epoch [85/330], Loss: 0.7702\n",
      "Epoch [90/330], Loss: 0.7570\n",
      "Epoch [95/330], Loss: 0.7441\n",
      "Epoch [100/330], Loss: 0.7315\n",
      "Epoch [105/330], Loss: 0.7193\n",
      "Epoch [110/330], Loss: 0.7074\n",
      "Epoch [115/330], Loss: 0.6960\n",
      "Epoch [120/330], Loss: 0.6849\n",
      "Epoch [125/330], Loss: 0.6742\n",
      "Epoch [130/330], Loss: 0.6638\n",
      "Epoch [135/330], Loss: 0.6537\n",
      "Epoch [140/330], Loss: 0.6439\n",
      "Epoch [145/330], Loss: 0.6345\n",
      "Epoch [150/330], Loss: 0.6253\n",
      "Epoch [155/330], Loss: 0.6164\n",
      "Epoch [160/330], Loss: 0.6078\n",
      "Epoch [165/330], Loss: 0.5995\n",
      "Epoch [170/330], Loss: 0.5914\n",
      "Epoch [175/330], Loss: 0.5836\n",
      "Epoch [180/330], Loss: 0.5760\n",
      "Epoch [185/330], Loss: 0.5686\n",
      "Epoch [190/330], Loss: 0.5615\n",
      "Epoch [195/330], Loss: 0.5546\n",
      "Epoch [200/330], Loss: 0.5480\n",
      "Epoch [205/330], Loss: 0.5415\n",
      "Epoch [210/330], Loss: 0.5353\n",
      "Epoch [215/330], Loss: 0.5292\n",
      "Epoch [220/330], Loss: 0.5234\n",
      "Epoch [225/330], Loss: 0.5177\n",
      "Epoch [230/330], Loss: 0.5123\n",
      "Epoch [235/330], Loss: 0.5069\n",
      "Epoch [240/330], Loss: 0.5018\n",
      "Epoch [245/330], Loss: 0.4968\n",
      "Epoch [250/330], Loss: 0.4919\n",
      "Epoch [255/330], Loss: 0.4872\n",
      "Epoch [260/330], Loss: 0.4826\n",
      "Epoch [265/330], Loss: 0.4782\n",
      "Epoch [270/330], Loss: 0.4738\n",
      "Epoch [275/330], Loss: 0.4696\n",
      "Epoch [280/330], Loss: 0.4655\n",
      "Epoch [285/330], Loss: 0.4616\n",
      "Epoch [290/330], Loss: 0.4576\n",
      "Epoch [295/330], Loss: 0.4537\n",
      "Epoch [300/330], Loss: 0.4499\n",
      "Epoch [305/330], Loss: 0.4462\n",
      "Epoch [310/330], Loss: 0.4426\n",
      "Epoch [315/330], Loss: 0.4391\n",
      "Epoch [320/330], Loss: 0.4357\n",
      "Epoch [325/330], Loss: 0.4324\n",
      "Epoch [330/330], Loss: 0.4291\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3645\n",
      "Epoch [5/340], Loss: 1.2706\n",
      "Epoch [10/340], Loss: 1.0961\n",
      "Epoch [15/340], Loss: 1.0366\n",
      "Epoch [20/340], Loss: 1.0028\n",
      "Epoch [25/340], Loss: 0.9762\n",
      "Epoch [30/340], Loss: 0.9529\n",
      "Epoch [35/340], Loss: 0.9317\n",
      "Epoch [40/340], Loss: 0.9119\n",
      "Epoch [45/340], Loss: 0.8932\n",
      "Epoch [50/340], Loss: 0.8756\n",
      "Epoch [55/340], Loss: 0.8588\n",
      "Epoch [60/340], Loss: 0.8426\n",
      "Epoch [65/340], Loss: 0.8270\n",
      "Epoch [70/340], Loss: 0.8122\n",
      "Epoch [75/340], Loss: 0.7979\n",
      "Epoch [80/340], Loss: 0.7838\n",
      "Epoch [85/340], Loss: 0.7702\n",
      "Epoch [90/340], Loss: 0.7570\n",
      "Epoch [95/340], Loss: 0.7441\n",
      "Epoch [100/340], Loss: 0.7315\n",
      "Epoch [105/340], Loss: 0.7193\n",
      "Epoch [110/340], Loss: 0.7074\n",
      "Epoch [115/340], Loss: 0.6960\n",
      "Epoch [120/340], Loss: 0.6849\n",
      "Epoch [125/340], Loss: 0.6742\n",
      "Epoch [130/340], Loss: 0.6638\n",
      "Epoch [135/340], Loss: 0.6537\n",
      "Epoch [140/340], Loss: 0.6439\n",
      "Epoch [145/340], Loss: 0.6345\n",
      "Epoch [150/340], Loss: 0.6253\n",
      "Epoch [155/340], Loss: 0.6164\n",
      "Epoch [160/340], Loss: 0.6078\n",
      "Epoch [165/340], Loss: 0.5995\n",
      "Epoch [170/340], Loss: 0.5914\n",
      "Epoch [175/340], Loss: 0.5836\n",
      "Epoch [180/340], Loss: 0.5760\n",
      "Epoch [185/340], Loss: 0.5686\n",
      "Epoch [190/340], Loss: 0.5615\n",
      "Epoch [195/340], Loss: 0.5546\n",
      "Epoch [200/340], Loss: 0.5480\n",
      "Epoch [205/340], Loss: 0.5415\n",
      "Epoch [210/340], Loss: 0.5353\n",
      "Epoch [215/340], Loss: 0.5292\n",
      "Epoch [220/340], Loss: 0.5234\n",
      "Epoch [225/340], Loss: 0.5177\n",
      "Epoch [230/340], Loss: 0.5123\n",
      "Epoch [235/340], Loss: 0.5069\n",
      "Epoch [240/340], Loss: 0.5018\n",
      "Epoch [245/340], Loss: 0.4968\n",
      "Epoch [250/340], Loss: 0.4919\n",
      "Epoch [255/340], Loss: 0.4872\n",
      "Epoch [260/340], Loss: 0.4826\n",
      "Epoch [265/340], Loss: 0.4782\n",
      "Epoch [270/340], Loss: 0.4738\n",
      "Epoch [275/340], Loss: 0.4696\n",
      "Epoch [280/340], Loss: 0.4655\n",
      "Epoch [285/340], Loss: 0.4616\n",
      "Epoch [290/340], Loss: 0.4576\n",
      "Epoch [295/340], Loss: 0.4537\n",
      "Epoch [300/340], Loss: 0.4499\n",
      "Epoch [305/340], Loss: 0.4462\n",
      "Epoch [310/340], Loss: 0.4426\n",
      "Epoch [315/340], Loss: 0.4391\n",
      "Epoch [320/340], Loss: 0.4357\n",
      "Epoch [325/340], Loss: 0.4324\n",
      "Epoch [330/340], Loss: 0.4291\n",
      "Epoch [335/340], Loss: 0.4260\n",
      "Epoch [340/340], Loss: 0.4229\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3579\n",
      "Epoch [5/350], Loss: 1.2706\n",
      "Epoch [10/350], Loss: 1.0961\n",
      "Epoch [15/350], Loss: 1.0366\n",
      "Epoch [20/350], Loss: 1.0028\n",
      "Epoch [25/350], Loss: 0.9762\n",
      "Epoch [30/350], Loss: 0.9529\n",
      "Epoch [35/350], Loss: 0.9317\n",
      "Epoch [40/350], Loss: 0.9119\n",
      "Epoch [45/350], Loss: 0.8932\n",
      "Epoch [50/350], Loss: 0.8756\n",
      "Epoch [55/350], Loss: 0.8588\n",
      "Epoch [60/350], Loss: 0.8426\n",
      "Epoch [65/350], Loss: 0.8270\n",
      "Epoch [70/350], Loss: 0.8122\n",
      "Epoch [75/350], Loss: 0.7979\n",
      "Epoch [80/350], Loss: 0.7838\n",
      "Epoch [85/350], Loss: 0.7702\n",
      "Epoch [90/350], Loss: 0.7570\n",
      "Epoch [95/350], Loss: 0.7441\n",
      "Epoch [100/350], Loss: 0.7315\n",
      "Epoch [105/350], Loss: 0.7193\n",
      "Epoch [110/350], Loss: 0.7074\n",
      "Epoch [115/350], Loss: 0.6960\n",
      "Epoch [120/350], Loss: 0.6849\n",
      "Epoch [125/350], Loss: 0.6742\n",
      "Epoch [130/350], Loss: 0.6638\n",
      "Epoch [135/350], Loss: 0.6537\n",
      "Epoch [140/350], Loss: 0.6439\n",
      "Epoch [145/350], Loss: 0.6345\n",
      "Epoch [150/350], Loss: 0.6253\n",
      "Epoch [155/350], Loss: 0.6164\n",
      "Epoch [160/350], Loss: 0.6078\n",
      "Epoch [165/350], Loss: 0.5995\n",
      "Epoch [170/350], Loss: 0.5914\n",
      "Epoch [175/350], Loss: 0.5836\n",
      "Epoch [180/350], Loss: 0.5760\n",
      "Epoch [185/350], Loss: 0.5686\n",
      "Epoch [190/350], Loss: 0.5615\n",
      "Epoch [195/350], Loss: 0.5546\n",
      "Epoch [200/350], Loss: 0.5480\n",
      "Epoch [205/350], Loss: 0.5415\n",
      "Epoch [210/350], Loss: 0.5353\n",
      "Epoch [215/350], Loss: 0.5292\n",
      "Epoch [220/350], Loss: 0.5234\n",
      "Epoch [225/350], Loss: 0.5177\n",
      "Epoch [230/350], Loss: 0.5123\n",
      "Epoch [235/350], Loss: 0.5069\n",
      "Epoch [240/350], Loss: 0.5018\n",
      "Epoch [245/350], Loss: 0.4968\n",
      "Epoch [250/350], Loss: 0.4919\n",
      "Epoch [255/350], Loss: 0.4872\n",
      "Epoch [260/350], Loss: 0.4826\n",
      "Epoch [265/350], Loss: 0.4782\n",
      "Epoch [270/350], Loss: 0.4738\n",
      "Epoch [275/350], Loss: 0.4696\n",
      "Epoch [280/350], Loss: 0.4655\n",
      "Epoch [285/350], Loss: 0.4616\n",
      "Epoch [290/350], Loss: 0.4576\n",
      "Epoch [295/350], Loss: 0.4537\n",
      "Epoch [300/350], Loss: 0.4499\n",
      "Epoch [305/350], Loss: 0.4462\n",
      "Epoch [310/350], Loss: 0.4426\n",
      "Epoch [315/350], Loss: 0.4391\n",
      "Epoch [320/350], Loss: 0.4357\n",
      "Epoch [325/350], Loss: 0.4324\n",
      "Epoch [330/350], Loss: 0.4291\n",
      "Epoch [335/350], Loss: 0.4260\n",
      "Epoch [340/350], Loss: 0.4229\n",
      "Epoch [345/350], Loss: 0.4199\n",
      "Epoch [350/350], Loss: 0.4169\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3516\n",
      "Epoch [5/360], Loss: 1.2706\n",
      "Epoch [10/360], Loss: 1.0961\n",
      "Epoch [15/360], Loss: 1.0366\n",
      "Epoch [20/360], Loss: 1.0028\n",
      "Epoch [25/360], Loss: 0.9762\n",
      "Epoch [30/360], Loss: 0.9529\n",
      "Epoch [35/360], Loss: 0.9317\n",
      "Epoch [40/360], Loss: 0.9119\n",
      "Epoch [45/360], Loss: 0.8932\n",
      "Epoch [50/360], Loss: 0.8756\n",
      "Epoch [55/360], Loss: 0.8588\n",
      "Epoch [60/360], Loss: 0.8426\n",
      "Epoch [65/360], Loss: 0.8270\n",
      "Epoch [70/360], Loss: 0.8122\n",
      "Epoch [75/360], Loss: 0.7979\n",
      "Epoch [80/360], Loss: 0.7838\n",
      "Epoch [85/360], Loss: 0.7702\n",
      "Epoch [90/360], Loss: 0.7570\n",
      "Epoch [95/360], Loss: 0.7441\n",
      "Epoch [100/360], Loss: 0.7315\n",
      "Epoch [105/360], Loss: 0.7193\n",
      "Epoch [110/360], Loss: 0.7074\n",
      "Epoch [115/360], Loss: 0.6960\n",
      "Epoch [120/360], Loss: 0.6849\n",
      "Epoch [125/360], Loss: 0.6742\n",
      "Epoch [130/360], Loss: 0.6638\n",
      "Epoch [135/360], Loss: 0.6537\n",
      "Epoch [140/360], Loss: 0.6439\n",
      "Epoch [145/360], Loss: 0.6345\n",
      "Epoch [150/360], Loss: 0.6253\n",
      "Epoch [155/360], Loss: 0.6164\n",
      "Epoch [160/360], Loss: 0.6078\n",
      "Epoch [165/360], Loss: 0.5995\n",
      "Epoch [170/360], Loss: 0.5914\n",
      "Epoch [175/360], Loss: 0.5836\n",
      "Epoch [180/360], Loss: 0.5760\n",
      "Epoch [185/360], Loss: 0.5686\n",
      "Epoch [190/360], Loss: 0.5615\n",
      "Epoch [195/360], Loss: 0.5546\n",
      "Epoch [200/360], Loss: 0.5480\n",
      "Epoch [205/360], Loss: 0.5415\n",
      "Epoch [210/360], Loss: 0.5353\n",
      "Epoch [215/360], Loss: 0.5292\n",
      "Epoch [220/360], Loss: 0.5234\n",
      "Epoch [225/360], Loss: 0.5177\n",
      "Epoch [230/360], Loss: 0.5123\n",
      "Epoch [235/360], Loss: 0.5069\n",
      "Epoch [240/360], Loss: 0.5018\n",
      "Epoch [245/360], Loss: 0.4968\n",
      "Epoch [250/360], Loss: 0.4919\n",
      "Epoch [255/360], Loss: 0.4872\n",
      "Epoch [260/360], Loss: 0.4826\n",
      "Epoch [265/360], Loss: 0.4782\n",
      "Epoch [270/360], Loss: 0.4738\n",
      "Epoch [275/360], Loss: 0.4696\n",
      "Epoch [280/360], Loss: 0.4655\n",
      "Epoch [285/360], Loss: 0.4616\n",
      "Epoch [290/360], Loss: 0.4576\n",
      "Epoch [295/360], Loss: 0.4537\n",
      "Epoch [300/360], Loss: 0.4499\n",
      "Epoch [305/360], Loss: 0.4462\n",
      "Epoch [310/360], Loss: 0.4426\n",
      "Epoch [315/360], Loss: 0.4391\n",
      "Epoch [320/360], Loss: 0.4357\n",
      "Epoch [325/360], Loss: 0.4324\n",
      "Epoch [330/360], Loss: 0.4291\n",
      "Epoch [335/360], Loss: 0.4260\n",
      "Epoch [340/360], Loss: 0.4229\n",
      "Epoch [345/360], Loss: 0.4199\n",
      "Epoch [350/360], Loss: 0.4169\n",
      "Epoch [355/360], Loss: 0.4140\n",
      "Epoch [360/360], Loss: 0.4112\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3456\n",
      "Epoch [5/370], Loss: 1.2706\n",
      "Epoch [10/370], Loss: 1.0961\n",
      "Epoch [15/370], Loss: 1.0366\n",
      "Epoch [20/370], Loss: 1.0028\n",
      "Epoch [25/370], Loss: 0.9762\n",
      "Epoch [30/370], Loss: 0.9529\n",
      "Epoch [35/370], Loss: 0.9317\n",
      "Epoch [40/370], Loss: 0.9119\n",
      "Epoch [45/370], Loss: 0.8932\n",
      "Epoch [50/370], Loss: 0.8756\n",
      "Epoch [55/370], Loss: 0.8588\n",
      "Epoch [60/370], Loss: 0.8426\n",
      "Epoch [65/370], Loss: 0.8270\n",
      "Epoch [70/370], Loss: 0.8122\n",
      "Epoch [75/370], Loss: 0.7979\n",
      "Epoch [80/370], Loss: 0.7838\n",
      "Epoch [85/370], Loss: 0.7702\n",
      "Epoch [90/370], Loss: 0.7570\n",
      "Epoch [95/370], Loss: 0.7441\n",
      "Epoch [100/370], Loss: 0.7315\n",
      "Epoch [105/370], Loss: 0.7193\n",
      "Epoch [110/370], Loss: 0.7074\n",
      "Epoch [115/370], Loss: 0.6960\n",
      "Epoch [120/370], Loss: 0.6849\n",
      "Epoch [125/370], Loss: 0.6742\n",
      "Epoch [130/370], Loss: 0.6638\n",
      "Epoch [135/370], Loss: 0.6537\n",
      "Epoch [140/370], Loss: 0.6439\n",
      "Epoch [145/370], Loss: 0.6345\n",
      "Epoch [150/370], Loss: 0.6253\n",
      "Epoch [155/370], Loss: 0.6164\n",
      "Epoch [160/370], Loss: 0.6078\n",
      "Epoch [165/370], Loss: 0.5995\n",
      "Epoch [170/370], Loss: 0.5914\n",
      "Epoch [175/370], Loss: 0.5836\n",
      "Epoch [180/370], Loss: 0.5760\n",
      "Epoch [185/370], Loss: 0.5686\n",
      "Epoch [190/370], Loss: 0.5615\n",
      "Epoch [195/370], Loss: 0.5546\n",
      "Epoch [200/370], Loss: 0.5480\n",
      "Epoch [205/370], Loss: 0.5415\n",
      "Epoch [210/370], Loss: 0.5353\n",
      "Epoch [215/370], Loss: 0.5292\n",
      "Epoch [220/370], Loss: 0.5234\n",
      "Epoch [225/370], Loss: 0.5177\n",
      "Epoch [230/370], Loss: 0.5123\n",
      "Epoch [235/370], Loss: 0.5069\n",
      "Epoch [240/370], Loss: 0.5018\n",
      "Epoch [245/370], Loss: 0.4968\n",
      "Epoch [250/370], Loss: 0.4919\n",
      "Epoch [255/370], Loss: 0.4872\n",
      "Epoch [260/370], Loss: 0.4826\n",
      "Epoch [265/370], Loss: 0.4782\n",
      "Epoch [270/370], Loss: 0.4738\n",
      "Epoch [275/370], Loss: 0.4696\n",
      "Epoch [280/370], Loss: 0.4655\n",
      "Epoch [285/370], Loss: 0.4616\n",
      "Epoch [290/370], Loss: 0.4576\n",
      "Epoch [295/370], Loss: 0.4537\n",
      "Epoch [300/370], Loss: 0.4499\n",
      "Epoch [305/370], Loss: 0.4462\n",
      "Epoch [310/370], Loss: 0.4426\n",
      "Epoch [315/370], Loss: 0.4391\n",
      "Epoch [320/370], Loss: 0.4357\n",
      "Epoch [325/370], Loss: 0.4324\n",
      "Epoch [330/370], Loss: 0.4291\n",
      "Epoch [335/370], Loss: 0.4260\n",
      "Epoch [340/370], Loss: 0.4229\n",
      "Epoch [345/370], Loss: 0.4199\n",
      "Epoch [350/370], Loss: 0.4169\n",
      "Epoch [355/370], Loss: 0.4140\n",
      "Epoch [360/370], Loss: 0.4112\n",
      "Epoch [365/370], Loss: 0.4085\n",
      "Epoch [370/370], Loss: 0.4058\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3398\n",
      "Epoch [5/380], Loss: 1.2706\n",
      "Epoch [10/380], Loss: 1.0961\n",
      "Epoch [15/380], Loss: 1.0366\n",
      "Epoch [20/380], Loss: 1.0028\n",
      "Epoch [25/380], Loss: 0.9762\n",
      "Epoch [30/380], Loss: 0.9529\n",
      "Epoch [35/380], Loss: 0.9317\n",
      "Epoch [40/380], Loss: 0.9119\n",
      "Epoch [45/380], Loss: 0.8932\n",
      "Epoch [50/380], Loss: 0.8756\n",
      "Epoch [55/380], Loss: 0.8588\n",
      "Epoch [60/380], Loss: 0.8426\n",
      "Epoch [65/380], Loss: 0.8270\n",
      "Epoch [70/380], Loss: 0.8122\n",
      "Epoch [75/380], Loss: 0.7979\n",
      "Epoch [80/380], Loss: 0.7838\n",
      "Epoch [85/380], Loss: 0.7702\n",
      "Epoch [90/380], Loss: 0.7570\n",
      "Epoch [95/380], Loss: 0.7441\n",
      "Epoch [100/380], Loss: 0.7315\n",
      "Epoch [105/380], Loss: 0.7193\n",
      "Epoch [110/380], Loss: 0.7074\n",
      "Epoch [115/380], Loss: 0.6960\n",
      "Epoch [120/380], Loss: 0.6849\n",
      "Epoch [125/380], Loss: 0.6742\n",
      "Epoch [130/380], Loss: 0.6638\n",
      "Epoch [135/380], Loss: 0.6537\n",
      "Epoch [140/380], Loss: 0.6439\n",
      "Epoch [145/380], Loss: 0.6345\n",
      "Epoch [150/380], Loss: 0.6253\n",
      "Epoch [155/380], Loss: 0.6164\n",
      "Epoch [160/380], Loss: 0.6078\n",
      "Epoch [165/380], Loss: 0.5995\n",
      "Epoch [170/380], Loss: 0.5914\n",
      "Epoch [175/380], Loss: 0.5836\n",
      "Epoch [180/380], Loss: 0.5760\n",
      "Epoch [185/380], Loss: 0.5686\n",
      "Epoch [190/380], Loss: 0.5615\n",
      "Epoch [195/380], Loss: 0.5546\n",
      "Epoch [200/380], Loss: 0.5480\n",
      "Epoch [205/380], Loss: 0.5415\n",
      "Epoch [210/380], Loss: 0.5353\n",
      "Epoch [215/380], Loss: 0.5292\n",
      "Epoch [220/380], Loss: 0.5234\n",
      "Epoch [225/380], Loss: 0.5177\n",
      "Epoch [230/380], Loss: 0.5123\n",
      "Epoch [235/380], Loss: 0.5069\n",
      "Epoch [240/380], Loss: 0.5018\n",
      "Epoch [245/380], Loss: 0.4968\n",
      "Epoch [250/380], Loss: 0.4919\n",
      "Epoch [255/380], Loss: 0.4872\n",
      "Epoch [260/380], Loss: 0.4826\n",
      "Epoch [265/380], Loss: 0.4782\n",
      "Epoch [270/380], Loss: 0.4738\n",
      "Epoch [275/380], Loss: 0.4696\n",
      "Epoch [280/380], Loss: 0.4655\n",
      "Epoch [285/380], Loss: 0.4616\n",
      "Epoch [290/380], Loss: 0.4576\n",
      "Epoch [295/380], Loss: 0.4537\n",
      "Epoch [300/380], Loss: 0.4499\n",
      "Epoch [305/380], Loss: 0.4462\n",
      "Epoch [310/380], Loss: 0.4426\n",
      "Epoch [315/380], Loss: 0.4391\n",
      "Epoch [320/380], Loss: 0.4357\n",
      "Epoch [325/380], Loss: 0.4324\n",
      "Epoch [330/380], Loss: 0.4291\n",
      "Epoch [335/380], Loss: 0.4260\n",
      "Epoch [340/380], Loss: 0.4229\n",
      "Epoch [345/380], Loss: 0.4199\n",
      "Epoch [350/380], Loss: 0.4169\n",
      "Epoch [355/380], Loss: 0.4140\n",
      "Epoch [360/380], Loss: 0.4112\n",
      "Epoch [365/380], Loss: 0.4085\n",
      "Epoch [370/380], Loss: 0.4058\n",
      "Epoch [375/380], Loss: 0.4032\n",
      "Epoch [380/380], Loss: 0.4006\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3343\n",
      "Epoch [5/390], Loss: 1.2706\n",
      "Epoch [10/390], Loss: 1.0961\n",
      "Epoch [15/390], Loss: 1.0366\n",
      "Epoch [20/390], Loss: 1.0028\n",
      "Epoch [25/390], Loss: 0.9762\n",
      "Epoch [30/390], Loss: 0.9529\n",
      "Epoch [35/390], Loss: 0.9317\n",
      "Epoch [40/390], Loss: 0.9119\n",
      "Epoch [45/390], Loss: 0.8932\n",
      "Epoch [50/390], Loss: 0.8756\n",
      "Epoch [55/390], Loss: 0.8588\n",
      "Epoch [60/390], Loss: 0.8426\n",
      "Epoch [65/390], Loss: 0.8270\n",
      "Epoch [70/390], Loss: 0.8122\n",
      "Epoch [75/390], Loss: 0.7979\n",
      "Epoch [80/390], Loss: 0.7838\n",
      "Epoch [85/390], Loss: 0.7702\n",
      "Epoch [90/390], Loss: 0.7570\n",
      "Epoch [95/390], Loss: 0.7441\n",
      "Epoch [100/390], Loss: 0.7315\n",
      "Epoch [105/390], Loss: 0.7193\n",
      "Epoch [110/390], Loss: 0.7074\n",
      "Epoch [115/390], Loss: 0.6960\n",
      "Epoch [120/390], Loss: 0.6849\n",
      "Epoch [125/390], Loss: 0.6742\n",
      "Epoch [130/390], Loss: 0.6638\n",
      "Epoch [135/390], Loss: 0.6537\n",
      "Epoch [140/390], Loss: 0.6439\n",
      "Epoch [145/390], Loss: 0.6345\n",
      "Epoch [150/390], Loss: 0.6253\n",
      "Epoch [155/390], Loss: 0.6164\n",
      "Epoch [160/390], Loss: 0.6078\n",
      "Epoch [165/390], Loss: 0.5995\n",
      "Epoch [170/390], Loss: 0.5914\n",
      "Epoch [175/390], Loss: 0.5836\n",
      "Epoch [180/390], Loss: 0.5760\n",
      "Epoch [185/390], Loss: 0.5686\n",
      "Epoch [190/390], Loss: 0.5615\n",
      "Epoch [195/390], Loss: 0.5546\n",
      "Epoch [200/390], Loss: 0.5480\n",
      "Epoch [205/390], Loss: 0.5415\n",
      "Epoch [210/390], Loss: 0.5353\n",
      "Epoch [215/390], Loss: 0.5292\n",
      "Epoch [220/390], Loss: 0.5234\n",
      "Epoch [225/390], Loss: 0.5177\n",
      "Epoch [230/390], Loss: 0.5123\n",
      "Epoch [235/390], Loss: 0.5069\n",
      "Epoch [240/390], Loss: 0.5018\n",
      "Epoch [245/390], Loss: 0.4968\n",
      "Epoch [250/390], Loss: 0.4919\n",
      "Epoch [255/390], Loss: 0.4872\n",
      "Epoch [260/390], Loss: 0.4826\n",
      "Epoch [265/390], Loss: 0.4782\n",
      "Epoch [270/390], Loss: 0.4738\n",
      "Epoch [275/390], Loss: 0.4696\n",
      "Epoch [280/390], Loss: 0.4655\n",
      "Epoch [285/390], Loss: 0.4616\n",
      "Epoch [290/390], Loss: 0.4576\n",
      "Epoch [295/390], Loss: 0.4537\n",
      "Epoch [300/390], Loss: 0.4499\n",
      "Epoch [305/390], Loss: 0.4462\n",
      "Epoch [310/390], Loss: 0.4426\n",
      "Epoch [315/390], Loss: 0.4391\n",
      "Epoch [320/390], Loss: 0.4357\n",
      "Epoch [325/390], Loss: 0.4324\n",
      "Epoch [330/390], Loss: 0.4291\n",
      "Epoch [335/390], Loss: 0.4260\n",
      "Epoch [340/390], Loss: 0.4229\n",
      "Epoch [345/390], Loss: 0.4199\n",
      "Epoch [350/390], Loss: 0.4169\n",
      "Epoch [355/390], Loss: 0.4140\n",
      "Epoch [360/390], Loss: 0.4112\n",
      "Epoch [365/390], Loss: 0.4085\n",
      "Epoch [370/390], Loss: 0.4058\n",
      "Epoch [375/390], Loss: 0.4032\n",
      "Epoch [380/390], Loss: 0.4006\n",
      "Epoch [385/390], Loss: 0.3980\n",
      "Epoch [390/390], Loss: 0.3955\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3290\n",
      "Epoch [5/400], Loss: 1.2706\n",
      "Epoch [10/400], Loss: 1.0961\n",
      "Epoch [15/400], Loss: 1.0366\n",
      "Epoch [20/400], Loss: 1.0028\n",
      "Epoch [25/400], Loss: 0.9762\n",
      "Epoch [30/400], Loss: 0.9529\n",
      "Epoch [35/400], Loss: 0.9317\n",
      "Epoch [40/400], Loss: 0.9119\n",
      "Epoch [45/400], Loss: 0.8932\n",
      "Epoch [50/400], Loss: 0.8756\n",
      "Epoch [55/400], Loss: 0.8588\n",
      "Epoch [60/400], Loss: 0.8426\n",
      "Epoch [65/400], Loss: 0.8270\n",
      "Epoch [70/400], Loss: 0.8122\n",
      "Epoch [75/400], Loss: 0.7979\n",
      "Epoch [80/400], Loss: 0.7838\n",
      "Epoch [85/400], Loss: 0.7702\n",
      "Epoch [90/400], Loss: 0.7570\n",
      "Epoch [95/400], Loss: 0.7441\n",
      "Epoch [100/400], Loss: 0.7315\n",
      "Epoch [105/400], Loss: 0.7193\n",
      "Epoch [110/400], Loss: 0.7074\n",
      "Epoch [115/400], Loss: 0.6960\n",
      "Epoch [120/400], Loss: 0.6849\n",
      "Epoch [125/400], Loss: 0.6742\n",
      "Epoch [130/400], Loss: 0.6638\n",
      "Epoch [135/400], Loss: 0.6537\n",
      "Epoch [140/400], Loss: 0.6439\n",
      "Epoch [145/400], Loss: 0.6345\n",
      "Epoch [150/400], Loss: 0.6253\n",
      "Epoch [155/400], Loss: 0.6164\n",
      "Epoch [160/400], Loss: 0.6078\n",
      "Epoch [165/400], Loss: 0.5995\n",
      "Epoch [170/400], Loss: 0.5914\n",
      "Epoch [175/400], Loss: 0.5836\n",
      "Epoch [180/400], Loss: 0.5760\n",
      "Epoch [185/400], Loss: 0.5686\n",
      "Epoch [190/400], Loss: 0.5615\n",
      "Epoch [195/400], Loss: 0.5546\n",
      "Epoch [200/400], Loss: 0.5480\n",
      "Epoch [205/400], Loss: 0.5415\n",
      "Epoch [210/400], Loss: 0.5353\n",
      "Epoch [215/400], Loss: 0.5292\n",
      "Epoch [220/400], Loss: 0.5234\n",
      "Epoch [225/400], Loss: 0.5177\n",
      "Epoch [230/400], Loss: 0.5123\n",
      "Epoch [235/400], Loss: 0.5069\n",
      "Epoch [240/400], Loss: 0.5018\n",
      "Epoch [245/400], Loss: 0.4968\n",
      "Epoch [250/400], Loss: 0.4919\n",
      "Epoch [255/400], Loss: 0.4872\n",
      "Epoch [260/400], Loss: 0.4826\n",
      "Epoch [265/400], Loss: 0.4782\n",
      "Epoch [270/400], Loss: 0.4738\n",
      "Epoch [275/400], Loss: 0.4696\n",
      "Epoch [280/400], Loss: 0.4655\n",
      "Epoch [285/400], Loss: 0.4616\n",
      "Epoch [290/400], Loss: 0.4576\n",
      "Epoch [295/400], Loss: 0.4537\n",
      "Epoch [300/400], Loss: 0.4499\n",
      "Epoch [305/400], Loss: 0.4462\n",
      "Epoch [310/400], Loss: 0.4426\n",
      "Epoch [315/400], Loss: 0.4391\n",
      "Epoch [320/400], Loss: 0.4357\n",
      "Epoch [325/400], Loss: 0.4324\n",
      "Epoch [330/400], Loss: 0.4291\n",
      "Epoch [335/400], Loss: 0.4260\n",
      "Epoch [340/400], Loss: 0.4229\n",
      "Epoch [345/400], Loss: 0.4199\n",
      "Epoch [350/400], Loss: 0.4169\n",
      "Epoch [355/400], Loss: 0.4140\n",
      "Epoch [360/400], Loss: 0.4112\n",
      "Epoch [365/400], Loss: 0.4085\n",
      "Epoch [370/400], Loss: 0.4058\n",
      "Epoch [375/400], Loss: 0.4032\n",
      "Epoch [380/400], Loss: 0.4006\n",
      "Epoch [385/400], Loss: 0.3980\n",
      "Epoch [390/400], Loss: 0.3955\n",
      "Epoch [395/400], Loss: 0.3931\n",
      "Epoch [400/400], Loss: 0.3907\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3240\n",
      "Epoch [5/410], Loss: 1.2706\n",
      "Epoch [10/410], Loss: 1.0961\n",
      "Epoch [15/410], Loss: 1.0366\n",
      "Epoch [20/410], Loss: 1.0028\n",
      "Epoch [25/410], Loss: 0.9762\n",
      "Epoch [30/410], Loss: 0.9529\n",
      "Epoch [35/410], Loss: 0.9317\n",
      "Epoch [40/410], Loss: 0.9119\n",
      "Epoch [45/410], Loss: 0.8932\n",
      "Epoch [50/410], Loss: 0.8756\n",
      "Epoch [55/410], Loss: 0.8588\n",
      "Epoch [60/410], Loss: 0.8426\n",
      "Epoch [65/410], Loss: 0.8270\n",
      "Epoch [70/410], Loss: 0.8122\n",
      "Epoch [75/410], Loss: 0.7979\n",
      "Epoch [80/410], Loss: 0.7838\n",
      "Epoch [85/410], Loss: 0.7702\n",
      "Epoch [90/410], Loss: 0.7570\n",
      "Epoch [95/410], Loss: 0.7441\n",
      "Epoch [100/410], Loss: 0.7315\n",
      "Epoch [105/410], Loss: 0.7193\n",
      "Epoch [110/410], Loss: 0.7074\n",
      "Epoch [115/410], Loss: 0.6960\n",
      "Epoch [120/410], Loss: 0.6849\n",
      "Epoch [125/410], Loss: 0.6742\n",
      "Epoch [130/410], Loss: 0.6638\n",
      "Epoch [135/410], Loss: 0.6537\n",
      "Epoch [140/410], Loss: 0.6439\n",
      "Epoch [145/410], Loss: 0.6345\n",
      "Epoch [150/410], Loss: 0.6253\n",
      "Epoch [155/410], Loss: 0.6164\n",
      "Epoch [160/410], Loss: 0.6078\n",
      "Epoch [165/410], Loss: 0.5995\n",
      "Epoch [170/410], Loss: 0.5914\n",
      "Epoch [175/410], Loss: 0.5836\n",
      "Epoch [180/410], Loss: 0.5760\n",
      "Epoch [185/410], Loss: 0.5686\n",
      "Epoch [190/410], Loss: 0.5615\n",
      "Epoch [195/410], Loss: 0.5546\n",
      "Epoch [200/410], Loss: 0.5480\n",
      "Epoch [205/410], Loss: 0.5415\n",
      "Epoch [210/410], Loss: 0.5353\n",
      "Epoch [215/410], Loss: 0.5292\n",
      "Epoch [220/410], Loss: 0.5234\n",
      "Epoch [225/410], Loss: 0.5177\n",
      "Epoch [230/410], Loss: 0.5123\n",
      "Epoch [235/410], Loss: 0.5069\n",
      "Epoch [240/410], Loss: 0.5018\n",
      "Epoch [245/410], Loss: 0.4968\n",
      "Epoch [250/410], Loss: 0.4919\n",
      "Epoch [255/410], Loss: 0.4872\n",
      "Epoch [260/410], Loss: 0.4826\n",
      "Epoch [265/410], Loss: 0.4782\n",
      "Epoch [270/410], Loss: 0.4738\n",
      "Epoch [275/410], Loss: 0.4696\n",
      "Epoch [280/410], Loss: 0.4655\n",
      "Epoch [285/410], Loss: 0.4616\n",
      "Epoch [290/410], Loss: 0.4576\n",
      "Epoch [295/410], Loss: 0.4537\n",
      "Epoch [300/410], Loss: 0.4499\n",
      "Epoch [305/410], Loss: 0.4462\n",
      "Epoch [310/410], Loss: 0.4426\n",
      "Epoch [315/410], Loss: 0.4391\n",
      "Epoch [320/410], Loss: 0.4357\n",
      "Epoch [325/410], Loss: 0.4324\n",
      "Epoch [330/410], Loss: 0.4291\n",
      "Epoch [335/410], Loss: 0.4260\n",
      "Epoch [340/410], Loss: 0.4229\n",
      "Epoch [345/410], Loss: 0.4199\n",
      "Epoch [350/410], Loss: 0.4169\n",
      "Epoch [355/410], Loss: 0.4140\n",
      "Epoch [360/410], Loss: 0.4112\n",
      "Epoch [365/410], Loss: 0.4085\n",
      "Epoch [370/410], Loss: 0.4058\n",
      "Epoch [375/410], Loss: 0.4032\n",
      "Epoch [380/410], Loss: 0.4006\n",
      "Epoch [385/410], Loss: 0.3980\n",
      "Epoch [390/410], Loss: 0.3955\n",
      "Epoch [395/410], Loss: 0.3931\n",
      "Epoch [400/410], Loss: 0.3907\n",
      "Epoch [405/410], Loss: 0.3884\n",
      "Epoch [410/410], Loss: 0.3861\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3191\n",
      "Epoch [5/420], Loss: 1.2706\n",
      "Epoch [10/420], Loss: 1.0961\n",
      "Epoch [15/420], Loss: 1.0366\n",
      "Epoch [20/420], Loss: 1.0028\n",
      "Epoch [25/420], Loss: 0.9762\n",
      "Epoch [30/420], Loss: 0.9529\n",
      "Epoch [35/420], Loss: 0.9317\n",
      "Epoch [40/420], Loss: 0.9119\n",
      "Epoch [45/420], Loss: 0.8932\n",
      "Epoch [50/420], Loss: 0.8756\n",
      "Epoch [55/420], Loss: 0.8588\n",
      "Epoch [60/420], Loss: 0.8426\n",
      "Epoch [65/420], Loss: 0.8270\n",
      "Epoch [70/420], Loss: 0.8122\n",
      "Epoch [75/420], Loss: 0.7979\n",
      "Epoch [80/420], Loss: 0.7838\n",
      "Epoch [85/420], Loss: 0.7702\n",
      "Epoch [90/420], Loss: 0.7570\n",
      "Epoch [95/420], Loss: 0.7441\n",
      "Epoch [100/420], Loss: 0.7315\n",
      "Epoch [105/420], Loss: 0.7193\n",
      "Epoch [110/420], Loss: 0.7074\n",
      "Epoch [115/420], Loss: 0.6960\n",
      "Epoch [120/420], Loss: 0.6849\n",
      "Epoch [125/420], Loss: 0.6742\n",
      "Epoch [130/420], Loss: 0.6638\n",
      "Epoch [135/420], Loss: 0.6537\n",
      "Epoch [140/420], Loss: 0.6439\n",
      "Epoch [145/420], Loss: 0.6345\n",
      "Epoch [150/420], Loss: 0.6253\n",
      "Epoch [155/420], Loss: 0.6164\n",
      "Epoch [160/420], Loss: 0.6078\n",
      "Epoch [165/420], Loss: 0.5995\n",
      "Epoch [170/420], Loss: 0.5914\n",
      "Epoch [175/420], Loss: 0.5836\n",
      "Epoch [180/420], Loss: 0.5760\n",
      "Epoch [185/420], Loss: 0.5686\n",
      "Epoch [190/420], Loss: 0.5615\n",
      "Epoch [195/420], Loss: 0.5546\n",
      "Epoch [200/420], Loss: 0.5480\n",
      "Epoch [205/420], Loss: 0.5415\n",
      "Epoch [210/420], Loss: 0.5353\n",
      "Epoch [215/420], Loss: 0.5292\n",
      "Epoch [220/420], Loss: 0.5234\n",
      "Epoch [225/420], Loss: 0.5177\n",
      "Epoch [230/420], Loss: 0.5123\n",
      "Epoch [235/420], Loss: 0.5069\n",
      "Epoch [240/420], Loss: 0.5018\n",
      "Epoch [245/420], Loss: 0.4968\n",
      "Epoch [250/420], Loss: 0.4919\n",
      "Epoch [255/420], Loss: 0.4872\n",
      "Epoch [260/420], Loss: 0.4826\n",
      "Epoch [265/420], Loss: 0.4782\n",
      "Epoch [270/420], Loss: 0.4738\n",
      "Epoch [275/420], Loss: 0.4696\n",
      "Epoch [280/420], Loss: 0.4655\n",
      "Epoch [285/420], Loss: 0.4616\n",
      "Epoch [290/420], Loss: 0.4576\n",
      "Epoch [295/420], Loss: 0.4537\n",
      "Epoch [300/420], Loss: 0.4499\n",
      "Epoch [305/420], Loss: 0.4462\n",
      "Epoch [310/420], Loss: 0.4426\n",
      "Epoch [315/420], Loss: 0.4391\n",
      "Epoch [320/420], Loss: 0.4357\n",
      "Epoch [325/420], Loss: 0.4324\n",
      "Epoch [330/420], Loss: 0.4291\n",
      "Epoch [335/420], Loss: 0.4260\n",
      "Epoch [340/420], Loss: 0.4229\n",
      "Epoch [345/420], Loss: 0.4199\n",
      "Epoch [350/420], Loss: 0.4169\n",
      "Epoch [355/420], Loss: 0.4140\n",
      "Epoch [360/420], Loss: 0.4112\n",
      "Epoch [365/420], Loss: 0.4085\n",
      "Epoch [370/420], Loss: 0.4058\n",
      "Epoch [375/420], Loss: 0.4032\n",
      "Epoch [380/420], Loss: 0.4006\n",
      "Epoch [385/420], Loss: 0.3980\n",
      "Epoch [390/420], Loss: 0.3955\n",
      "Epoch [395/420], Loss: 0.3931\n",
      "Epoch [400/420], Loss: 0.3907\n",
      "Epoch [405/420], Loss: 0.3884\n",
      "Epoch [410/420], Loss: 0.3861\n",
      "Epoch [415/420], Loss: 0.3838\n",
      "Epoch [420/420], Loss: 0.3816\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3144\n",
      "Epoch [5/430], Loss: 1.2706\n",
      "Epoch [10/430], Loss: 1.0961\n",
      "Epoch [15/430], Loss: 1.0366\n",
      "Epoch [20/430], Loss: 1.0028\n",
      "Epoch [25/430], Loss: 0.9762\n",
      "Epoch [30/430], Loss: 0.9529\n",
      "Epoch [35/430], Loss: 0.9317\n",
      "Epoch [40/430], Loss: 0.9119\n",
      "Epoch [45/430], Loss: 0.8932\n",
      "Epoch [50/430], Loss: 0.8756\n",
      "Epoch [55/430], Loss: 0.8588\n",
      "Epoch [60/430], Loss: 0.8426\n",
      "Epoch [65/430], Loss: 0.8270\n",
      "Epoch [70/430], Loss: 0.8122\n",
      "Epoch [75/430], Loss: 0.7979\n",
      "Epoch [80/430], Loss: 0.7838\n",
      "Epoch [85/430], Loss: 0.7702\n",
      "Epoch [90/430], Loss: 0.7570\n",
      "Epoch [95/430], Loss: 0.7441\n",
      "Epoch [100/430], Loss: 0.7315\n",
      "Epoch [105/430], Loss: 0.7193\n",
      "Epoch [110/430], Loss: 0.7074\n",
      "Epoch [115/430], Loss: 0.6960\n",
      "Epoch [120/430], Loss: 0.6849\n",
      "Epoch [125/430], Loss: 0.6742\n",
      "Epoch [130/430], Loss: 0.6638\n",
      "Epoch [135/430], Loss: 0.6537\n",
      "Epoch [140/430], Loss: 0.6439\n",
      "Epoch [145/430], Loss: 0.6345\n",
      "Epoch [150/430], Loss: 0.6253\n",
      "Epoch [155/430], Loss: 0.6164\n",
      "Epoch [160/430], Loss: 0.6078\n",
      "Epoch [165/430], Loss: 0.5995\n",
      "Epoch [170/430], Loss: 0.5914\n",
      "Epoch [175/430], Loss: 0.5836\n",
      "Epoch [180/430], Loss: 0.5760\n",
      "Epoch [185/430], Loss: 0.5686\n",
      "Epoch [190/430], Loss: 0.5615\n",
      "Epoch [195/430], Loss: 0.5546\n",
      "Epoch [200/430], Loss: 0.5480\n",
      "Epoch [205/430], Loss: 0.5415\n",
      "Epoch [210/430], Loss: 0.5353\n",
      "Epoch [215/430], Loss: 0.5292\n",
      "Epoch [220/430], Loss: 0.5234\n",
      "Epoch [225/430], Loss: 0.5177\n",
      "Epoch [230/430], Loss: 0.5123\n",
      "Epoch [235/430], Loss: 0.5069\n",
      "Epoch [240/430], Loss: 0.5018\n",
      "Epoch [245/430], Loss: 0.4968\n",
      "Epoch [250/430], Loss: 0.4919\n",
      "Epoch [255/430], Loss: 0.4872\n",
      "Epoch [260/430], Loss: 0.4826\n",
      "Epoch [265/430], Loss: 0.4782\n",
      "Epoch [270/430], Loss: 0.4738\n",
      "Epoch [275/430], Loss: 0.4696\n",
      "Epoch [280/430], Loss: 0.4655\n",
      "Epoch [285/430], Loss: 0.4616\n",
      "Epoch [290/430], Loss: 0.4576\n",
      "Epoch [295/430], Loss: 0.4537\n",
      "Epoch [300/430], Loss: 0.4499\n",
      "Epoch [305/430], Loss: 0.4462\n",
      "Epoch [310/430], Loss: 0.4426\n",
      "Epoch [315/430], Loss: 0.4391\n",
      "Epoch [320/430], Loss: 0.4357\n",
      "Epoch [325/430], Loss: 0.4324\n",
      "Epoch [330/430], Loss: 0.4291\n",
      "Epoch [335/430], Loss: 0.4260\n",
      "Epoch [340/430], Loss: 0.4229\n",
      "Epoch [345/430], Loss: 0.4199\n",
      "Epoch [350/430], Loss: 0.4169\n",
      "Epoch [355/430], Loss: 0.4140\n",
      "Epoch [360/430], Loss: 0.4112\n",
      "Epoch [365/430], Loss: 0.4085\n",
      "Epoch [370/430], Loss: 0.4058\n",
      "Epoch [375/430], Loss: 0.4032\n",
      "Epoch [380/430], Loss: 0.4006\n",
      "Epoch [385/430], Loss: 0.3980\n",
      "Epoch [390/430], Loss: 0.3955\n",
      "Epoch [395/430], Loss: 0.3931\n",
      "Epoch [400/430], Loss: 0.3907\n",
      "Epoch [405/430], Loss: 0.3884\n",
      "Epoch [410/430], Loss: 0.3861\n",
      "Epoch [415/430], Loss: 0.3838\n",
      "Epoch [420/430], Loss: 0.3816\n",
      "Epoch [425/430], Loss: 0.3794\n",
      "Epoch [430/430], Loss: 0.3773\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3100\n",
      "Epoch [5/440], Loss: 1.2706\n",
      "Epoch [10/440], Loss: 1.0961\n",
      "Epoch [15/440], Loss: 1.0366\n",
      "Epoch [20/440], Loss: 1.0028\n",
      "Epoch [25/440], Loss: 0.9762\n",
      "Epoch [30/440], Loss: 0.9529\n",
      "Epoch [35/440], Loss: 0.9317\n",
      "Epoch [40/440], Loss: 0.9119\n",
      "Epoch [45/440], Loss: 0.8932\n",
      "Epoch [50/440], Loss: 0.8756\n",
      "Epoch [55/440], Loss: 0.8588\n",
      "Epoch [60/440], Loss: 0.8426\n",
      "Epoch [65/440], Loss: 0.8270\n",
      "Epoch [70/440], Loss: 0.8122\n",
      "Epoch [75/440], Loss: 0.7979\n",
      "Epoch [80/440], Loss: 0.7838\n",
      "Epoch [85/440], Loss: 0.7702\n",
      "Epoch [90/440], Loss: 0.7570\n",
      "Epoch [95/440], Loss: 0.7441\n",
      "Epoch [100/440], Loss: 0.7315\n",
      "Epoch [105/440], Loss: 0.7193\n",
      "Epoch [110/440], Loss: 0.7074\n",
      "Epoch [115/440], Loss: 0.6960\n",
      "Epoch [120/440], Loss: 0.6849\n",
      "Epoch [125/440], Loss: 0.6742\n",
      "Epoch [130/440], Loss: 0.6638\n",
      "Epoch [135/440], Loss: 0.6537\n",
      "Epoch [140/440], Loss: 0.6439\n",
      "Epoch [145/440], Loss: 0.6345\n",
      "Epoch [150/440], Loss: 0.6253\n",
      "Epoch [155/440], Loss: 0.6164\n",
      "Epoch [160/440], Loss: 0.6078\n",
      "Epoch [165/440], Loss: 0.5995\n",
      "Epoch [170/440], Loss: 0.5914\n",
      "Epoch [175/440], Loss: 0.5836\n",
      "Epoch [180/440], Loss: 0.5760\n",
      "Epoch [185/440], Loss: 0.5686\n",
      "Epoch [190/440], Loss: 0.5615\n",
      "Epoch [195/440], Loss: 0.5546\n",
      "Epoch [200/440], Loss: 0.5480\n",
      "Epoch [205/440], Loss: 0.5415\n",
      "Epoch [210/440], Loss: 0.5353\n",
      "Epoch [215/440], Loss: 0.5292\n",
      "Epoch [220/440], Loss: 0.5234\n",
      "Epoch [225/440], Loss: 0.5177\n",
      "Epoch [230/440], Loss: 0.5123\n",
      "Epoch [235/440], Loss: 0.5069\n",
      "Epoch [240/440], Loss: 0.5018\n",
      "Epoch [245/440], Loss: 0.4968\n",
      "Epoch [250/440], Loss: 0.4919\n",
      "Epoch [255/440], Loss: 0.4872\n",
      "Epoch [260/440], Loss: 0.4826\n",
      "Epoch [265/440], Loss: 0.4782\n",
      "Epoch [270/440], Loss: 0.4738\n",
      "Epoch [275/440], Loss: 0.4696\n",
      "Epoch [280/440], Loss: 0.4655\n",
      "Epoch [285/440], Loss: 0.4616\n",
      "Epoch [290/440], Loss: 0.4576\n",
      "Epoch [295/440], Loss: 0.4537\n",
      "Epoch [300/440], Loss: 0.4499\n",
      "Epoch [305/440], Loss: 0.4462\n",
      "Epoch [310/440], Loss: 0.4426\n",
      "Epoch [315/440], Loss: 0.4391\n",
      "Epoch [320/440], Loss: 0.4357\n",
      "Epoch [325/440], Loss: 0.4324\n",
      "Epoch [330/440], Loss: 0.4291\n",
      "Epoch [335/440], Loss: 0.4260\n",
      "Epoch [340/440], Loss: 0.4229\n",
      "Epoch [345/440], Loss: 0.4199\n",
      "Epoch [350/440], Loss: 0.4169\n",
      "Epoch [355/440], Loss: 0.4140\n",
      "Epoch [360/440], Loss: 0.4112\n",
      "Epoch [365/440], Loss: 0.4085\n",
      "Epoch [370/440], Loss: 0.4058\n",
      "Epoch [375/440], Loss: 0.4032\n",
      "Epoch [380/440], Loss: 0.4006\n",
      "Epoch [385/440], Loss: 0.3980\n",
      "Epoch [390/440], Loss: 0.3955\n",
      "Epoch [395/440], Loss: 0.3931\n",
      "Epoch [400/440], Loss: 0.3907\n",
      "Epoch [405/440], Loss: 0.3884\n",
      "Epoch [410/440], Loss: 0.3861\n",
      "Epoch [415/440], Loss: 0.3838\n",
      "Epoch [420/440], Loss: 0.3816\n",
      "Epoch [425/440], Loss: 0.3794\n",
      "Epoch [430/440], Loss: 0.3773\n",
      "Epoch [435/440], Loss: 0.3752\n",
      "Epoch [440/440], Loss: 0.3731\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3056\n",
      "Epoch [5/450], Loss: 1.2706\n",
      "Epoch [10/450], Loss: 1.0961\n",
      "Epoch [15/450], Loss: 1.0366\n",
      "Epoch [20/450], Loss: 1.0028\n",
      "Epoch [25/450], Loss: 0.9762\n",
      "Epoch [30/450], Loss: 0.9529\n",
      "Epoch [35/450], Loss: 0.9317\n",
      "Epoch [40/450], Loss: 0.9119\n",
      "Epoch [45/450], Loss: 0.8932\n",
      "Epoch [50/450], Loss: 0.8756\n",
      "Epoch [55/450], Loss: 0.8588\n",
      "Epoch [60/450], Loss: 0.8426\n",
      "Epoch [65/450], Loss: 0.8270\n",
      "Epoch [70/450], Loss: 0.8122\n",
      "Epoch [75/450], Loss: 0.7979\n",
      "Epoch [80/450], Loss: 0.7838\n",
      "Epoch [85/450], Loss: 0.7702\n",
      "Epoch [90/450], Loss: 0.7570\n",
      "Epoch [95/450], Loss: 0.7441\n",
      "Epoch [100/450], Loss: 0.7315\n",
      "Epoch [105/450], Loss: 0.7193\n",
      "Epoch [110/450], Loss: 0.7074\n",
      "Epoch [115/450], Loss: 0.6960\n",
      "Epoch [120/450], Loss: 0.6849\n",
      "Epoch [125/450], Loss: 0.6742\n",
      "Epoch [130/450], Loss: 0.6638\n",
      "Epoch [135/450], Loss: 0.6537\n",
      "Epoch [140/450], Loss: 0.6439\n",
      "Epoch [145/450], Loss: 0.6345\n",
      "Epoch [150/450], Loss: 0.6253\n",
      "Epoch [155/450], Loss: 0.6164\n",
      "Epoch [160/450], Loss: 0.6078\n",
      "Epoch [165/450], Loss: 0.5995\n",
      "Epoch [170/450], Loss: 0.5914\n",
      "Epoch [175/450], Loss: 0.5836\n",
      "Epoch [180/450], Loss: 0.5760\n",
      "Epoch [185/450], Loss: 0.5686\n",
      "Epoch [190/450], Loss: 0.5615\n",
      "Epoch [195/450], Loss: 0.5546\n",
      "Epoch [200/450], Loss: 0.5480\n",
      "Epoch [205/450], Loss: 0.5415\n",
      "Epoch [210/450], Loss: 0.5353\n",
      "Epoch [215/450], Loss: 0.5292\n",
      "Epoch [220/450], Loss: 0.5234\n",
      "Epoch [225/450], Loss: 0.5177\n",
      "Epoch [230/450], Loss: 0.5123\n",
      "Epoch [235/450], Loss: 0.5069\n",
      "Epoch [240/450], Loss: 0.5018\n",
      "Epoch [245/450], Loss: 0.4968\n",
      "Epoch [250/450], Loss: 0.4919\n",
      "Epoch [255/450], Loss: 0.4872\n",
      "Epoch [260/450], Loss: 0.4826\n",
      "Epoch [265/450], Loss: 0.4782\n",
      "Epoch [270/450], Loss: 0.4738\n",
      "Epoch [275/450], Loss: 0.4696\n",
      "Epoch [280/450], Loss: 0.4655\n",
      "Epoch [285/450], Loss: 0.4616\n",
      "Epoch [290/450], Loss: 0.4576\n",
      "Epoch [295/450], Loss: 0.4537\n",
      "Epoch [300/450], Loss: 0.4499\n",
      "Epoch [305/450], Loss: 0.4462\n",
      "Epoch [310/450], Loss: 0.4426\n",
      "Epoch [315/450], Loss: 0.4391\n",
      "Epoch [320/450], Loss: 0.4357\n",
      "Epoch [325/450], Loss: 0.4324\n",
      "Epoch [330/450], Loss: 0.4291\n",
      "Epoch [335/450], Loss: 0.4260\n",
      "Epoch [340/450], Loss: 0.4229\n",
      "Epoch [345/450], Loss: 0.4199\n",
      "Epoch [350/450], Loss: 0.4169\n",
      "Epoch [355/450], Loss: 0.4140\n",
      "Epoch [360/450], Loss: 0.4112\n",
      "Epoch [365/450], Loss: 0.4085\n",
      "Epoch [370/450], Loss: 0.4058\n",
      "Epoch [375/450], Loss: 0.4032\n",
      "Epoch [380/450], Loss: 0.4006\n",
      "Epoch [385/450], Loss: 0.3980\n",
      "Epoch [390/450], Loss: 0.3955\n",
      "Epoch [395/450], Loss: 0.3931\n",
      "Epoch [400/450], Loss: 0.3907\n",
      "Epoch [405/450], Loss: 0.3884\n",
      "Epoch [410/450], Loss: 0.3861\n",
      "Epoch [415/450], Loss: 0.3838\n",
      "Epoch [420/450], Loss: 0.3816\n",
      "Epoch [425/450], Loss: 0.3794\n",
      "Epoch [430/450], Loss: 0.3773\n",
      "Epoch [435/450], Loss: 0.3752\n",
      "Epoch [440/450], Loss: 0.3731\n",
      "Epoch [445/450], Loss: 0.3711\n",
      "Epoch [450/450], Loss: 0.3691\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.3015\n",
      "Epoch [5/460], Loss: 1.2706\n",
      "Epoch [10/460], Loss: 1.0961\n",
      "Epoch [15/460], Loss: 1.0366\n",
      "Epoch [20/460], Loss: 1.0028\n",
      "Epoch [25/460], Loss: 0.9762\n",
      "Epoch [30/460], Loss: 0.9529\n",
      "Epoch [35/460], Loss: 0.9317\n",
      "Epoch [40/460], Loss: 0.9119\n",
      "Epoch [45/460], Loss: 0.8932\n",
      "Epoch [50/460], Loss: 0.8756\n",
      "Epoch [55/460], Loss: 0.8588\n",
      "Epoch [60/460], Loss: 0.8426\n",
      "Epoch [65/460], Loss: 0.8270\n",
      "Epoch [70/460], Loss: 0.8122\n",
      "Epoch [75/460], Loss: 0.7979\n",
      "Epoch [80/460], Loss: 0.7838\n",
      "Epoch [85/460], Loss: 0.7702\n",
      "Epoch [90/460], Loss: 0.7570\n",
      "Epoch [95/460], Loss: 0.7441\n",
      "Epoch [100/460], Loss: 0.7315\n",
      "Epoch [105/460], Loss: 0.7193\n",
      "Epoch [110/460], Loss: 0.7074\n",
      "Epoch [115/460], Loss: 0.6960\n",
      "Epoch [120/460], Loss: 0.6849\n",
      "Epoch [125/460], Loss: 0.6742\n",
      "Epoch [130/460], Loss: 0.6638\n",
      "Epoch [135/460], Loss: 0.6537\n",
      "Epoch [140/460], Loss: 0.6439\n",
      "Epoch [145/460], Loss: 0.6345\n",
      "Epoch [150/460], Loss: 0.6253\n",
      "Epoch [155/460], Loss: 0.6164\n",
      "Epoch [160/460], Loss: 0.6078\n",
      "Epoch [165/460], Loss: 0.5995\n",
      "Epoch [170/460], Loss: 0.5914\n",
      "Epoch [175/460], Loss: 0.5836\n",
      "Epoch [180/460], Loss: 0.5760\n",
      "Epoch [185/460], Loss: 0.5686\n",
      "Epoch [190/460], Loss: 0.5615\n",
      "Epoch [195/460], Loss: 0.5546\n",
      "Epoch [200/460], Loss: 0.5480\n",
      "Epoch [205/460], Loss: 0.5415\n",
      "Epoch [210/460], Loss: 0.5353\n",
      "Epoch [215/460], Loss: 0.5292\n",
      "Epoch [220/460], Loss: 0.5234\n",
      "Epoch [225/460], Loss: 0.5177\n",
      "Epoch [230/460], Loss: 0.5123\n",
      "Epoch [235/460], Loss: 0.5069\n",
      "Epoch [240/460], Loss: 0.5018\n",
      "Epoch [245/460], Loss: 0.4968\n",
      "Epoch [250/460], Loss: 0.4919\n",
      "Epoch [255/460], Loss: 0.4872\n",
      "Epoch [260/460], Loss: 0.4826\n",
      "Epoch [265/460], Loss: 0.4782\n",
      "Epoch [270/460], Loss: 0.4738\n",
      "Epoch [275/460], Loss: 0.4696\n",
      "Epoch [280/460], Loss: 0.4655\n",
      "Epoch [285/460], Loss: 0.4616\n",
      "Epoch [290/460], Loss: 0.4576\n",
      "Epoch [295/460], Loss: 0.4537\n",
      "Epoch [300/460], Loss: 0.4499\n",
      "Epoch [305/460], Loss: 0.4462\n",
      "Epoch [310/460], Loss: 0.4426\n",
      "Epoch [315/460], Loss: 0.4391\n",
      "Epoch [320/460], Loss: 0.4357\n",
      "Epoch [325/460], Loss: 0.4324\n",
      "Epoch [330/460], Loss: 0.4291\n",
      "Epoch [335/460], Loss: 0.4260\n",
      "Epoch [340/460], Loss: 0.4229\n",
      "Epoch [345/460], Loss: 0.4199\n",
      "Epoch [350/460], Loss: 0.4169\n",
      "Epoch [355/460], Loss: 0.4140\n",
      "Epoch [360/460], Loss: 0.4112\n",
      "Epoch [365/460], Loss: 0.4085\n",
      "Epoch [370/460], Loss: 0.4058\n",
      "Epoch [375/460], Loss: 0.4032\n",
      "Epoch [380/460], Loss: 0.4006\n",
      "Epoch [385/460], Loss: 0.3980\n",
      "Epoch [390/460], Loss: 0.3955\n",
      "Epoch [395/460], Loss: 0.3931\n",
      "Epoch [400/460], Loss: 0.3907\n",
      "Epoch [405/460], Loss: 0.3884\n",
      "Epoch [410/460], Loss: 0.3861\n",
      "Epoch [415/460], Loss: 0.3838\n",
      "Epoch [420/460], Loss: 0.3816\n",
      "Epoch [425/460], Loss: 0.3794\n",
      "Epoch [430/460], Loss: 0.3773\n",
      "Epoch [435/460], Loss: 0.3752\n",
      "Epoch [440/460], Loss: 0.3731\n",
      "Epoch [445/460], Loss: 0.3711\n",
      "Epoch [450/460], Loss: 0.3691\n",
      "Epoch [455/460], Loss: 0.3671\n",
      "Epoch [460/460], Loss: 0.3652\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.2974\n",
      "Epoch [5/470], Loss: 1.2706\n",
      "Epoch [10/470], Loss: 1.0961\n",
      "Epoch [15/470], Loss: 1.0366\n",
      "Epoch [20/470], Loss: 1.0028\n",
      "Epoch [25/470], Loss: 0.9762\n",
      "Epoch [30/470], Loss: 0.9529\n",
      "Epoch [35/470], Loss: 0.9317\n",
      "Epoch [40/470], Loss: 0.9119\n",
      "Epoch [45/470], Loss: 0.8932\n",
      "Epoch [50/470], Loss: 0.8756\n",
      "Epoch [55/470], Loss: 0.8588\n",
      "Epoch [60/470], Loss: 0.8426\n",
      "Epoch [65/470], Loss: 0.8270\n",
      "Epoch [70/470], Loss: 0.8122\n",
      "Epoch [75/470], Loss: 0.7979\n",
      "Epoch [80/470], Loss: 0.7838\n",
      "Epoch [85/470], Loss: 0.7702\n",
      "Epoch [90/470], Loss: 0.7570\n",
      "Epoch [95/470], Loss: 0.7441\n",
      "Epoch [100/470], Loss: 0.7315\n",
      "Epoch [105/470], Loss: 0.7193\n",
      "Epoch [110/470], Loss: 0.7074\n",
      "Epoch [115/470], Loss: 0.6960\n",
      "Epoch [120/470], Loss: 0.6849\n",
      "Epoch [125/470], Loss: 0.6742\n",
      "Epoch [130/470], Loss: 0.6638\n",
      "Epoch [135/470], Loss: 0.6537\n",
      "Epoch [140/470], Loss: 0.6439\n",
      "Epoch [145/470], Loss: 0.6345\n",
      "Epoch [150/470], Loss: 0.6253\n",
      "Epoch [155/470], Loss: 0.6164\n",
      "Epoch [160/470], Loss: 0.6078\n",
      "Epoch [165/470], Loss: 0.5995\n",
      "Epoch [170/470], Loss: 0.5914\n",
      "Epoch [175/470], Loss: 0.5836\n",
      "Epoch [180/470], Loss: 0.5760\n",
      "Epoch [185/470], Loss: 0.5686\n",
      "Epoch [190/470], Loss: 0.5615\n",
      "Epoch [195/470], Loss: 0.5546\n",
      "Epoch [200/470], Loss: 0.5480\n",
      "Epoch [205/470], Loss: 0.5415\n",
      "Epoch [210/470], Loss: 0.5353\n",
      "Epoch [215/470], Loss: 0.5292\n",
      "Epoch [220/470], Loss: 0.5234\n",
      "Epoch [225/470], Loss: 0.5177\n",
      "Epoch [230/470], Loss: 0.5123\n",
      "Epoch [235/470], Loss: 0.5069\n",
      "Epoch [240/470], Loss: 0.5018\n",
      "Epoch [245/470], Loss: 0.4968\n",
      "Epoch [250/470], Loss: 0.4919\n",
      "Epoch [255/470], Loss: 0.4872\n",
      "Epoch [260/470], Loss: 0.4826\n",
      "Epoch [265/470], Loss: 0.4782\n",
      "Epoch [270/470], Loss: 0.4738\n",
      "Epoch [275/470], Loss: 0.4696\n",
      "Epoch [280/470], Loss: 0.4655\n",
      "Epoch [285/470], Loss: 0.4616\n",
      "Epoch [290/470], Loss: 0.4576\n",
      "Epoch [295/470], Loss: 0.4537\n",
      "Epoch [300/470], Loss: 0.4499\n",
      "Epoch [305/470], Loss: 0.4462\n",
      "Epoch [310/470], Loss: 0.4426\n",
      "Epoch [315/470], Loss: 0.4391\n",
      "Epoch [320/470], Loss: 0.4357\n",
      "Epoch [325/470], Loss: 0.4324\n",
      "Epoch [330/470], Loss: 0.4291\n",
      "Epoch [335/470], Loss: 0.4260\n",
      "Epoch [340/470], Loss: 0.4229\n",
      "Epoch [345/470], Loss: 0.4199\n",
      "Epoch [350/470], Loss: 0.4169\n",
      "Epoch [355/470], Loss: 0.4140\n",
      "Epoch [360/470], Loss: 0.4112\n",
      "Epoch [365/470], Loss: 0.4085\n",
      "Epoch [370/470], Loss: 0.4058\n",
      "Epoch [375/470], Loss: 0.4032\n",
      "Epoch [380/470], Loss: 0.4006\n",
      "Epoch [385/470], Loss: 0.3980\n",
      "Epoch [390/470], Loss: 0.3955\n",
      "Epoch [395/470], Loss: 0.3931\n",
      "Epoch [400/470], Loss: 0.3907\n",
      "Epoch [405/470], Loss: 0.3884\n",
      "Epoch [410/470], Loss: 0.3861\n",
      "Epoch [415/470], Loss: 0.3838\n",
      "Epoch [420/470], Loss: 0.3816\n",
      "Epoch [425/470], Loss: 0.3794\n",
      "Epoch [430/470], Loss: 0.3773\n",
      "Epoch [435/470], Loss: 0.3752\n",
      "Epoch [440/470], Loss: 0.3731\n",
      "Epoch [445/470], Loss: 0.3711\n",
      "Epoch [450/470], Loss: 0.3691\n",
      "Epoch [455/470], Loss: 0.3671\n",
      "Epoch [460/470], Loss: 0.3652\n",
      "Epoch [465/470], Loss: 0.3632\n",
      "Epoch [470/470], Loss: 0.3614\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.2935\n",
      "Epoch [5/480], Loss: 1.2706\n",
      "Epoch [10/480], Loss: 1.0961\n",
      "Epoch [15/480], Loss: 1.0366\n",
      "Epoch [20/480], Loss: 1.0028\n",
      "Epoch [25/480], Loss: 0.9762\n",
      "Epoch [30/480], Loss: 0.9529\n",
      "Epoch [35/480], Loss: 0.9317\n",
      "Epoch [40/480], Loss: 0.9119\n",
      "Epoch [45/480], Loss: 0.8932\n",
      "Epoch [50/480], Loss: 0.8756\n",
      "Epoch [55/480], Loss: 0.8588\n",
      "Epoch [60/480], Loss: 0.8426\n",
      "Epoch [65/480], Loss: 0.8270\n",
      "Epoch [70/480], Loss: 0.8122\n",
      "Epoch [75/480], Loss: 0.7979\n",
      "Epoch [80/480], Loss: 0.7838\n",
      "Epoch [85/480], Loss: 0.7702\n",
      "Epoch [90/480], Loss: 0.7570\n",
      "Epoch [95/480], Loss: 0.7441\n",
      "Epoch [100/480], Loss: 0.7315\n",
      "Epoch [105/480], Loss: 0.7193\n",
      "Epoch [110/480], Loss: 0.7074\n",
      "Epoch [115/480], Loss: 0.6960\n",
      "Epoch [120/480], Loss: 0.6849\n",
      "Epoch [125/480], Loss: 0.6742\n",
      "Epoch [130/480], Loss: 0.6638\n",
      "Epoch [135/480], Loss: 0.6537\n",
      "Epoch [140/480], Loss: 0.6439\n",
      "Epoch [145/480], Loss: 0.6345\n",
      "Epoch [150/480], Loss: 0.6253\n",
      "Epoch [155/480], Loss: 0.6164\n",
      "Epoch [160/480], Loss: 0.6078\n",
      "Epoch [165/480], Loss: 0.5995\n",
      "Epoch [170/480], Loss: 0.5914\n",
      "Epoch [175/480], Loss: 0.5836\n",
      "Epoch [180/480], Loss: 0.5760\n",
      "Epoch [185/480], Loss: 0.5686\n",
      "Epoch [190/480], Loss: 0.5615\n",
      "Epoch [195/480], Loss: 0.5546\n",
      "Epoch [200/480], Loss: 0.5480\n",
      "Epoch [205/480], Loss: 0.5415\n",
      "Epoch [210/480], Loss: 0.5353\n",
      "Epoch [215/480], Loss: 0.5292\n",
      "Epoch [220/480], Loss: 0.5234\n",
      "Epoch [225/480], Loss: 0.5177\n",
      "Epoch [230/480], Loss: 0.5123\n",
      "Epoch [235/480], Loss: 0.5069\n",
      "Epoch [240/480], Loss: 0.5018\n",
      "Epoch [245/480], Loss: 0.4968\n",
      "Epoch [250/480], Loss: 0.4919\n",
      "Epoch [255/480], Loss: 0.4872\n",
      "Epoch [260/480], Loss: 0.4826\n",
      "Epoch [265/480], Loss: 0.4782\n",
      "Epoch [270/480], Loss: 0.4738\n",
      "Epoch [275/480], Loss: 0.4696\n",
      "Epoch [280/480], Loss: 0.4655\n",
      "Epoch [285/480], Loss: 0.4616\n",
      "Epoch [290/480], Loss: 0.4576\n",
      "Epoch [295/480], Loss: 0.4537\n",
      "Epoch [300/480], Loss: 0.4499\n",
      "Epoch [305/480], Loss: 0.4462\n",
      "Epoch [310/480], Loss: 0.4426\n",
      "Epoch [315/480], Loss: 0.4391\n",
      "Epoch [320/480], Loss: 0.4357\n",
      "Epoch [325/480], Loss: 0.4324\n",
      "Epoch [330/480], Loss: 0.4291\n",
      "Epoch [335/480], Loss: 0.4260\n",
      "Epoch [340/480], Loss: 0.4229\n",
      "Epoch [345/480], Loss: 0.4199\n",
      "Epoch [350/480], Loss: 0.4169\n",
      "Epoch [355/480], Loss: 0.4140\n",
      "Epoch [360/480], Loss: 0.4112\n",
      "Epoch [365/480], Loss: 0.4085\n",
      "Epoch [370/480], Loss: 0.4058\n",
      "Epoch [375/480], Loss: 0.4032\n",
      "Epoch [380/480], Loss: 0.4006\n",
      "Epoch [385/480], Loss: 0.3980\n",
      "Epoch [390/480], Loss: 0.3955\n",
      "Epoch [395/480], Loss: 0.3931\n",
      "Epoch [400/480], Loss: 0.3907\n",
      "Epoch [405/480], Loss: 0.3884\n",
      "Epoch [410/480], Loss: 0.3861\n",
      "Epoch [415/480], Loss: 0.3838\n",
      "Epoch [420/480], Loss: 0.3816\n",
      "Epoch [425/480], Loss: 0.3794\n",
      "Epoch [430/480], Loss: 0.3773\n",
      "Epoch [435/480], Loss: 0.3752\n",
      "Epoch [440/480], Loss: 0.3731\n",
      "Epoch [445/480], Loss: 0.3711\n",
      "Epoch [450/480], Loss: 0.3691\n",
      "Epoch [455/480], Loss: 0.3671\n",
      "Epoch [460/480], Loss: 0.3652\n",
      "Epoch [465/480], Loss: 0.3632\n",
      "Epoch [470/480], Loss: 0.3614\n",
      "Epoch [475/480], Loss: 0.3595\n",
      "Epoch [480/480], Loss: 0.3577\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.2898\n",
      "Epoch [5/490], Loss: 1.2706\n",
      "Epoch [10/490], Loss: 1.0961\n",
      "Epoch [15/490], Loss: 1.0366\n",
      "Epoch [20/490], Loss: 1.0028\n",
      "Epoch [25/490], Loss: 0.9762\n",
      "Epoch [30/490], Loss: 0.9529\n",
      "Epoch [35/490], Loss: 0.9317\n",
      "Epoch [40/490], Loss: 0.9119\n",
      "Epoch [45/490], Loss: 0.8932\n",
      "Epoch [50/490], Loss: 0.8756\n",
      "Epoch [55/490], Loss: 0.8588\n",
      "Epoch [60/490], Loss: 0.8426\n",
      "Epoch [65/490], Loss: 0.8270\n",
      "Epoch [70/490], Loss: 0.8122\n",
      "Epoch [75/490], Loss: 0.7979\n",
      "Epoch [80/490], Loss: 0.7838\n",
      "Epoch [85/490], Loss: 0.7702\n",
      "Epoch [90/490], Loss: 0.7570\n",
      "Epoch [95/490], Loss: 0.7441\n",
      "Epoch [100/490], Loss: 0.7315\n",
      "Epoch [105/490], Loss: 0.7193\n",
      "Epoch [110/490], Loss: 0.7074\n",
      "Epoch [115/490], Loss: 0.6960\n",
      "Epoch [120/490], Loss: 0.6849\n",
      "Epoch [125/490], Loss: 0.6742\n",
      "Epoch [130/490], Loss: 0.6638\n",
      "Epoch [135/490], Loss: 0.6537\n",
      "Epoch [140/490], Loss: 0.6439\n",
      "Epoch [145/490], Loss: 0.6345\n",
      "Epoch [150/490], Loss: 0.6253\n",
      "Epoch [155/490], Loss: 0.6164\n",
      "Epoch [160/490], Loss: 0.6078\n",
      "Epoch [165/490], Loss: 0.5995\n",
      "Epoch [170/490], Loss: 0.5914\n",
      "Epoch [175/490], Loss: 0.5836\n",
      "Epoch [180/490], Loss: 0.5760\n",
      "Epoch [185/490], Loss: 0.5686\n",
      "Epoch [190/490], Loss: 0.5615\n",
      "Epoch [195/490], Loss: 0.5546\n",
      "Epoch [200/490], Loss: 0.5480\n",
      "Epoch [205/490], Loss: 0.5415\n",
      "Epoch [210/490], Loss: 0.5353\n",
      "Epoch [215/490], Loss: 0.5292\n",
      "Epoch [220/490], Loss: 0.5234\n",
      "Epoch [225/490], Loss: 0.5177\n",
      "Epoch [230/490], Loss: 0.5123\n",
      "Epoch [235/490], Loss: 0.5069\n",
      "Epoch [240/490], Loss: 0.5018\n",
      "Epoch [245/490], Loss: 0.4968\n",
      "Epoch [250/490], Loss: 0.4919\n",
      "Epoch [255/490], Loss: 0.4872\n",
      "Epoch [260/490], Loss: 0.4826\n",
      "Epoch [265/490], Loss: 0.4782\n",
      "Epoch [270/490], Loss: 0.4738\n",
      "Epoch [275/490], Loss: 0.4696\n",
      "Epoch [280/490], Loss: 0.4655\n",
      "Epoch [285/490], Loss: 0.4616\n",
      "Epoch [290/490], Loss: 0.4576\n",
      "Epoch [295/490], Loss: 0.4537\n",
      "Epoch [300/490], Loss: 0.4499\n",
      "Epoch [305/490], Loss: 0.4462\n",
      "Epoch [310/490], Loss: 0.4426\n",
      "Epoch [315/490], Loss: 0.4391\n",
      "Epoch [320/490], Loss: 0.4357\n",
      "Epoch [325/490], Loss: 0.4324\n",
      "Epoch [330/490], Loss: 0.4291\n",
      "Epoch [335/490], Loss: 0.4260\n",
      "Epoch [340/490], Loss: 0.4229\n",
      "Epoch [345/490], Loss: 0.4199\n",
      "Epoch [350/490], Loss: 0.4169\n",
      "Epoch [355/490], Loss: 0.4140\n",
      "Epoch [360/490], Loss: 0.4112\n",
      "Epoch [365/490], Loss: 0.4085\n",
      "Epoch [370/490], Loss: 0.4058\n",
      "Epoch [375/490], Loss: 0.4032\n",
      "Epoch [380/490], Loss: 0.4006\n",
      "Epoch [385/490], Loss: 0.3980\n",
      "Epoch [390/490], Loss: 0.3955\n",
      "Epoch [395/490], Loss: 0.3931\n",
      "Epoch [400/490], Loss: 0.3907\n",
      "Epoch [405/490], Loss: 0.3884\n",
      "Epoch [410/490], Loss: 0.3861\n",
      "Epoch [415/490], Loss: 0.3838\n",
      "Epoch [420/490], Loss: 0.3816\n",
      "Epoch [425/490], Loss: 0.3794\n",
      "Epoch [430/490], Loss: 0.3773\n",
      "Epoch [435/490], Loss: 0.3752\n",
      "Epoch [440/490], Loss: 0.3731\n",
      "Epoch [445/490], Loss: 0.3711\n",
      "Epoch [450/490], Loss: 0.3691\n",
      "Epoch [455/490], Loss: 0.3671\n",
      "Epoch [460/490], Loss: 0.3652\n",
      "Epoch [465/490], Loss: 0.3632\n",
      "Epoch [470/490], Loss: 0.3614\n",
      "Epoch [475/490], Loss: 0.3595\n",
      "Epoch [480/490], Loss: 0.3577\n",
      "Epoch [485/490], Loss: 0.3559\n",
      "Epoch [490/490], Loss: 0.3541\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.2862\n",
      "Epoch [5/500], Loss: 1.2706\n",
      "Epoch [10/500], Loss: 1.0961\n",
      "Epoch [15/500], Loss: 1.0366\n",
      "Epoch [20/500], Loss: 1.0028\n",
      "Epoch [25/500], Loss: 0.9762\n",
      "Epoch [30/500], Loss: 0.9529\n",
      "Epoch [35/500], Loss: 0.9317\n",
      "Epoch [40/500], Loss: 0.9119\n",
      "Epoch [45/500], Loss: 0.8932\n",
      "Epoch [50/500], Loss: 0.8756\n",
      "Epoch [55/500], Loss: 0.8588\n",
      "Epoch [60/500], Loss: 0.8426\n",
      "Epoch [65/500], Loss: 0.8270\n",
      "Epoch [70/500], Loss: 0.8122\n",
      "Epoch [75/500], Loss: 0.7979\n",
      "Epoch [80/500], Loss: 0.7838\n",
      "Epoch [85/500], Loss: 0.7702\n",
      "Epoch [90/500], Loss: 0.7570\n",
      "Epoch [95/500], Loss: 0.7441\n",
      "Epoch [100/500], Loss: 0.7315\n",
      "Epoch [105/500], Loss: 0.7193\n",
      "Epoch [110/500], Loss: 0.7074\n",
      "Epoch [115/500], Loss: 0.6960\n",
      "Epoch [120/500], Loss: 0.6849\n",
      "Epoch [125/500], Loss: 0.6742\n",
      "Epoch [130/500], Loss: 0.6638\n",
      "Epoch [135/500], Loss: 0.6537\n",
      "Epoch [140/500], Loss: 0.6439\n",
      "Epoch [145/500], Loss: 0.6345\n",
      "Epoch [150/500], Loss: 0.6253\n",
      "Epoch [155/500], Loss: 0.6164\n",
      "Epoch [160/500], Loss: 0.6078\n",
      "Epoch [165/500], Loss: 0.5995\n",
      "Epoch [170/500], Loss: 0.5914\n",
      "Epoch [175/500], Loss: 0.5836\n",
      "Epoch [180/500], Loss: 0.5760\n",
      "Epoch [185/500], Loss: 0.5686\n",
      "Epoch [190/500], Loss: 0.5615\n",
      "Epoch [195/500], Loss: 0.5546\n",
      "Epoch [200/500], Loss: 0.5480\n",
      "Epoch [205/500], Loss: 0.5415\n",
      "Epoch [210/500], Loss: 0.5353\n",
      "Epoch [215/500], Loss: 0.5292\n",
      "Epoch [220/500], Loss: 0.5234\n",
      "Epoch [225/500], Loss: 0.5177\n",
      "Epoch [230/500], Loss: 0.5123\n",
      "Epoch [235/500], Loss: 0.5069\n",
      "Epoch [240/500], Loss: 0.5018\n",
      "Epoch [245/500], Loss: 0.4968\n",
      "Epoch [250/500], Loss: 0.4919\n",
      "Epoch [255/500], Loss: 0.4872\n",
      "Epoch [260/500], Loss: 0.4826\n",
      "Epoch [265/500], Loss: 0.4782\n",
      "Epoch [270/500], Loss: 0.4738\n",
      "Epoch [275/500], Loss: 0.4696\n",
      "Epoch [280/500], Loss: 0.4655\n",
      "Epoch [285/500], Loss: 0.4616\n",
      "Epoch [290/500], Loss: 0.4576\n",
      "Epoch [295/500], Loss: 0.4537\n",
      "Epoch [300/500], Loss: 0.4499\n",
      "Epoch [305/500], Loss: 0.4462\n",
      "Epoch [310/500], Loss: 0.4426\n",
      "Epoch [315/500], Loss: 0.4391\n",
      "Epoch [320/500], Loss: 0.4357\n",
      "Epoch [325/500], Loss: 0.4324\n",
      "Epoch [330/500], Loss: 0.4291\n",
      "Epoch [335/500], Loss: 0.4260\n",
      "Epoch [340/500], Loss: 0.4229\n",
      "Epoch [345/500], Loss: 0.4199\n",
      "Epoch [350/500], Loss: 0.4169\n",
      "Epoch [355/500], Loss: 0.4140\n",
      "Epoch [360/500], Loss: 0.4112\n",
      "Epoch [365/500], Loss: 0.4085\n",
      "Epoch [370/500], Loss: 0.4058\n",
      "Epoch [375/500], Loss: 0.4032\n",
      "Epoch [380/500], Loss: 0.4006\n",
      "Epoch [385/500], Loss: 0.3980\n",
      "Epoch [390/500], Loss: 0.3955\n",
      "Epoch [395/500], Loss: 0.3931\n",
      "Epoch [400/500], Loss: 0.3907\n",
      "Epoch [405/500], Loss: 0.3884\n",
      "Epoch [410/500], Loss: 0.3861\n",
      "Epoch [415/500], Loss: 0.3838\n",
      "Epoch [420/500], Loss: 0.3816\n",
      "Epoch [425/500], Loss: 0.3794\n",
      "Epoch [430/500], Loss: 0.3773\n",
      "Epoch [435/500], Loss: 0.3752\n",
      "Epoch [440/500], Loss: 0.3731\n",
      "Epoch [445/500], Loss: 0.3711\n",
      "Epoch [450/500], Loss: 0.3691\n",
      "Epoch [455/500], Loss: 0.3671\n",
      "Epoch [460/500], Loss: 0.3652\n",
      "Epoch [465/500], Loss: 0.3632\n",
      "Epoch [470/500], Loss: 0.3614\n",
      "Epoch [475/500], Loss: 0.3595\n",
      "Epoch [480/500], Loss: 0.3577\n",
      "Epoch [485/500], Loss: 0.3559\n",
      "Epoch [490/500], Loss: 0.3541\n",
      "Epoch [495/500], Loss: 0.3524\n",
      "Epoch [500/500], Loss: 0.3507\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.2826\n",
      "Epoch [5/510], Loss: 1.2706\n",
      "Epoch [10/510], Loss: 1.0961\n",
      "Epoch [15/510], Loss: 1.0366\n",
      "Epoch [20/510], Loss: 1.0028\n",
      "Epoch [25/510], Loss: 0.9762\n",
      "Epoch [30/510], Loss: 0.9529\n",
      "Epoch [35/510], Loss: 0.9317\n",
      "Epoch [40/510], Loss: 0.9119\n",
      "Epoch [45/510], Loss: 0.8932\n",
      "Epoch [50/510], Loss: 0.8756\n",
      "Epoch [55/510], Loss: 0.8588\n",
      "Epoch [60/510], Loss: 0.8426\n",
      "Epoch [65/510], Loss: 0.8270\n",
      "Epoch [70/510], Loss: 0.8122\n",
      "Epoch [75/510], Loss: 0.7979\n",
      "Epoch [80/510], Loss: 0.7838\n",
      "Epoch [85/510], Loss: 0.7702\n",
      "Epoch [90/510], Loss: 0.7570\n",
      "Epoch [95/510], Loss: 0.7441\n",
      "Epoch [100/510], Loss: 0.7315\n",
      "Epoch [105/510], Loss: 0.7193\n",
      "Epoch [110/510], Loss: 0.7074\n",
      "Epoch [115/510], Loss: 0.6960\n",
      "Epoch [120/510], Loss: 0.6849\n",
      "Epoch [125/510], Loss: 0.6742\n",
      "Epoch [130/510], Loss: 0.6638\n",
      "Epoch [135/510], Loss: 0.6537\n",
      "Epoch [140/510], Loss: 0.6439\n",
      "Epoch [145/510], Loss: 0.6345\n",
      "Epoch [150/510], Loss: 0.6253\n",
      "Epoch [155/510], Loss: 0.6164\n",
      "Epoch [160/510], Loss: 0.6078\n",
      "Epoch [165/510], Loss: 0.5995\n",
      "Epoch [170/510], Loss: 0.5914\n",
      "Epoch [175/510], Loss: 0.5836\n",
      "Epoch [180/510], Loss: 0.5760\n",
      "Epoch [185/510], Loss: 0.5686\n",
      "Epoch [190/510], Loss: 0.5615\n",
      "Epoch [195/510], Loss: 0.5546\n",
      "Epoch [200/510], Loss: 0.5480\n",
      "Epoch [205/510], Loss: 0.5415\n",
      "Epoch [210/510], Loss: 0.5353\n",
      "Epoch [215/510], Loss: 0.5292\n",
      "Epoch [220/510], Loss: 0.5234\n",
      "Epoch [225/510], Loss: 0.5177\n",
      "Epoch [230/510], Loss: 0.5123\n",
      "Epoch [235/510], Loss: 0.5069\n",
      "Epoch [240/510], Loss: 0.5018\n",
      "Epoch [245/510], Loss: 0.4968\n",
      "Epoch [250/510], Loss: 0.4919\n",
      "Epoch [255/510], Loss: 0.4872\n",
      "Epoch [260/510], Loss: 0.4826\n",
      "Epoch [265/510], Loss: 0.4782\n",
      "Epoch [270/510], Loss: 0.4738\n",
      "Epoch [275/510], Loss: 0.4696\n",
      "Epoch [280/510], Loss: 0.4655\n",
      "Epoch [285/510], Loss: 0.4616\n",
      "Epoch [290/510], Loss: 0.4576\n",
      "Epoch [295/510], Loss: 0.4537\n",
      "Epoch [300/510], Loss: 0.4499\n",
      "Epoch [305/510], Loss: 0.4462\n",
      "Epoch [310/510], Loss: 0.4426\n",
      "Epoch [315/510], Loss: 0.4391\n",
      "Epoch [320/510], Loss: 0.4357\n",
      "Epoch [325/510], Loss: 0.4324\n",
      "Epoch [330/510], Loss: 0.4291\n",
      "Epoch [335/510], Loss: 0.4260\n",
      "Epoch [340/510], Loss: 0.4229\n",
      "Epoch [345/510], Loss: 0.4199\n",
      "Epoch [350/510], Loss: 0.4169\n",
      "Epoch [355/510], Loss: 0.4140\n",
      "Epoch [360/510], Loss: 0.4112\n",
      "Epoch [365/510], Loss: 0.4085\n",
      "Epoch [370/510], Loss: 0.4058\n",
      "Epoch [375/510], Loss: 0.4032\n",
      "Epoch [380/510], Loss: 0.4006\n",
      "Epoch [385/510], Loss: 0.3980\n",
      "Epoch [390/510], Loss: 0.3955\n",
      "Epoch [395/510], Loss: 0.3931\n",
      "Epoch [400/510], Loss: 0.3907\n",
      "Epoch [405/510], Loss: 0.3884\n",
      "Epoch [410/510], Loss: 0.3861\n",
      "Epoch [415/510], Loss: 0.3838\n",
      "Epoch [420/510], Loss: 0.3816\n",
      "Epoch [425/510], Loss: 0.3794\n",
      "Epoch [430/510], Loss: 0.3773\n",
      "Epoch [435/510], Loss: 0.3752\n",
      "Epoch [440/510], Loss: 0.3731\n",
      "Epoch [445/510], Loss: 0.3711\n",
      "Epoch [450/510], Loss: 0.3691\n",
      "Epoch [455/510], Loss: 0.3671\n",
      "Epoch [460/510], Loss: 0.3652\n",
      "Epoch [465/510], Loss: 0.3632\n",
      "Epoch [470/510], Loss: 0.3614\n",
      "Epoch [475/510], Loss: 0.3595\n",
      "Epoch [480/510], Loss: 0.3577\n",
      "Epoch [485/510], Loss: 0.3559\n",
      "Epoch [490/510], Loss: 0.3541\n",
      "Epoch [495/510], Loss: 0.3524\n",
      "Epoch [500/510], Loss: 0.3507\n",
      "Epoch [505/510], Loss: 0.3490\n",
      "Epoch [510/510], Loss: 0.3473\n",
      "Training complete.\n",
      "Test Accuracy: 93.33%\n",
      "Test Loss (Cross-Entropy): 0.2792\n",
      "Epoch [5/520], Loss: 1.2706\n",
      "Epoch [10/520], Loss: 1.0961\n",
      "Epoch [15/520], Loss: 1.0366\n",
      "Epoch [20/520], Loss: 1.0028\n",
      "Epoch [25/520], Loss: 0.9762\n",
      "Epoch [30/520], Loss: 0.9529\n",
      "Epoch [35/520], Loss: 0.9317\n",
      "Epoch [40/520], Loss: 0.9119\n",
      "Epoch [45/520], Loss: 0.8932\n",
      "Epoch [50/520], Loss: 0.8756\n",
      "Epoch [55/520], Loss: 0.8588\n",
      "Epoch [60/520], Loss: 0.8426\n",
      "Epoch [65/520], Loss: 0.8270\n",
      "Epoch [70/520], Loss: 0.8122\n",
      "Epoch [75/520], Loss: 0.7979\n",
      "Epoch [80/520], Loss: 0.7838\n",
      "Epoch [85/520], Loss: 0.7702\n",
      "Epoch [90/520], Loss: 0.7570\n",
      "Epoch [95/520], Loss: 0.7441\n",
      "Epoch [100/520], Loss: 0.7315\n",
      "Epoch [105/520], Loss: 0.7193\n",
      "Epoch [110/520], Loss: 0.7074\n",
      "Epoch [115/520], Loss: 0.6960\n",
      "Epoch [120/520], Loss: 0.6849\n",
      "Epoch [125/520], Loss: 0.6742\n",
      "Epoch [130/520], Loss: 0.6638\n",
      "Epoch [135/520], Loss: 0.6537\n",
      "Epoch [140/520], Loss: 0.6439\n",
      "Epoch [145/520], Loss: 0.6345\n",
      "Epoch [150/520], Loss: 0.6253\n",
      "Epoch [155/520], Loss: 0.6164\n",
      "Epoch [160/520], Loss: 0.6078\n",
      "Epoch [165/520], Loss: 0.5995\n",
      "Epoch [170/520], Loss: 0.5914\n",
      "Epoch [175/520], Loss: 0.5836\n",
      "Epoch [180/520], Loss: 0.5760\n",
      "Epoch [185/520], Loss: 0.5686\n",
      "Epoch [190/520], Loss: 0.5615\n",
      "Epoch [195/520], Loss: 0.5546\n",
      "Epoch [200/520], Loss: 0.5480\n",
      "Epoch [205/520], Loss: 0.5415\n",
      "Epoch [210/520], Loss: 0.5353\n",
      "Epoch [215/520], Loss: 0.5292\n",
      "Epoch [220/520], Loss: 0.5234\n",
      "Epoch [225/520], Loss: 0.5177\n",
      "Epoch [230/520], Loss: 0.5123\n",
      "Epoch [235/520], Loss: 0.5069\n",
      "Epoch [240/520], Loss: 0.5018\n",
      "Epoch [245/520], Loss: 0.4968\n",
      "Epoch [250/520], Loss: 0.4919\n",
      "Epoch [255/520], Loss: 0.4872\n",
      "Epoch [260/520], Loss: 0.4826\n",
      "Epoch [265/520], Loss: 0.4782\n",
      "Epoch [270/520], Loss: 0.4738\n",
      "Epoch [275/520], Loss: 0.4696\n",
      "Epoch [280/520], Loss: 0.4655\n",
      "Epoch [285/520], Loss: 0.4616\n",
      "Epoch [290/520], Loss: 0.4576\n",
      "Epoch [295/520], Loss: 0.4537\n",
      "Epoch [300/520], Loss: 0.4499\n",
      "Epoch [305/520], Loss: 0.4462\n",
      "Epoch [310/520], Loss: 0.4426\n",
      "Epoch [315/520], Loss: 0.4391\n",
      "Epoch [320/520], Loss: 0.4357\n",
      "Epoch [325/520], Loss: 0.4324\n",
      "Epoch [330/520], Loss: 0.4291\n",
      "Epoch [335/520], Loss: 0.4260\n",
      "Epoch [340/520], Loss: 0.4229\n",
      "Epoch [345/520], Loss: 0.4199\n",
      "Epoch [350/520], Loss: 0.4169\n",
      "Epoch [355/520], Loss: 0.4140\n",
      "Epoch [360/520], Loss: 0.4112\n",
      "Epoch [365/520], Loss: 0.4085\n",
      "Epoch [370/520], Loss: 0.4058\n",
      "Epoch [375/520], Loss: 0.4032\n",
      "Epoch [380/520], Loss: 0.4006\n",
      "Epoch [385/520], Loss: 0.3980\n",
      "Epoch [390/520], Loss: 0.3955\n",
      "Epoch [395/520], Loss: 0.3931\n",
      "Epoch [400/520], Loss: 0.3907\n",
      "Epoch [405/520], Loss: 0.3884\n",
      "Epoch [410/520], Loss: 0.3861\n",
      "Epoch [415/520], Loss: 0.3838\n",
      "Epoch [420/520], Loss: 0.3816\n",
      "Epoch [425/520], Loss: 0.3794\n",
      "Epoch [430/520], Loss: 0.3773\n",
      "Epoch [435/520], Loss: 0.3752\n",
      "Epoch [440/520], Loss: 0.3731\n",
      "Epoch [445/520], Loss: 0.3711\n",
      "Epoch [450/520], Loss: 0.3691\n",
      "Epoch [455/520], Loss: 0.3671\n",
      "Epoch [460/520], Loss: 0.3652\n",
      "Epoch [465/520], Loss: 0.3632\n",
      "Epoch [470/520], Loss: 0.3614\n",
      "Epoch [475/520], Loss: 0.3595\n",
      "Epoch [480/520], Loss: 0.3577\n",
      "Epoch [485/520], Loss: 0.3559\n",
      "Epoch [490/520], Loss: 0.3541\n",
      "Epoch [495/520], Loss: 0.3524\n",
      "Epoch [500/520], Loss: 0.3507\n",
      "Epoch [505/520], Loss: 0.3490\n",
      "Epoch [510/520], Loss: 0.3473\n",
      "Epoch [515/520], Loss: 0.3456\n",
      "Epoch [520/520], Loss: 0.3440\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2759\n",
      "Epoch [5/530], Loss: 1.2706\n",
      "Epoch [10/530], Loss: 1.0961\n",
      "Epoch [15/530], Loss: 1.0366\n",
      "Epoch [20/530], Loss: 1.0028\n",
      "Epoch [25/530], Loss: 0.9762\n",
      "Epoch [30/530], Loss: 0.9529\n",
      "Epoch [35/530], Loss: 0.9317\n",
      "Epoch [40/530], Loss: 0.9119\n",
      "Epoch [45/530], Loss: 0.8932\n",
      "Epoch [50/530], Loss: 0.8756\n",
      "Epoch [55/530], Loss: 0.8588\n",
      "Epoch [60/530], Loss: 0.8426\n",
      "Epoch [65/530], Loss: 0.8270\n",
      "Epoch [70/530], Loss: 0.8122\n",
      "Epoch [75/530], Loss: 0.7979\n",
      "Epoch [80/530], Loss: 0.7838\n",
      "Epoch [85/530], Loss: 0.7702\n",
      "Epoch [90/530], Loss: 0.7570\n",
      "Epoch [95/530], Loss: 0.7441\n",
      "Epoch [100/530], Loss: 0.7315\n",
      "Epoch [105/530], Loss: 0.7193\n",
      "Epoch [110/530], Loss: 0.7074\n",
      "Epoch [115/530], Loss: 0.6960\n",
      "Epoch [120/530], Loss: 0.6849\n",
      "Epoch [125/530], Loss: 0.6742\n",
      "Epoch [130/530], Loss: 0.6638\n",
      "Epoch [135/530], Loss: 0.6537\n",
      "Epoch [140/530], Loss: 0.6439\n",
      "Epoch [145/530], Loss: 0.6345\n",
      "Epoch [150/530], Loss: 0.6253\n",
      "Epoch [155/530], Loss: 0.6164\n",
      "Epoch [160/530], Loss: 0.6078\n",
      "Epoch [165/530], Loss: 0.5995\n",
      "Epoch [170/530], Loss: 0.5914\n",
      "Epoch [175/530], Loss: 0.5836\n",
      "Epoch [180/530], Loss: 0.5760\n",
      "Epoch [185/530], Loss: 0.5686\n",
      "Epoch [190/530], Loss: 0.5615\n",
      "Epoch [195/530], Loss: 0.5546\n",
      "Epoch [200/530], Loss: 0.5480\n",
      "Epoch [205/530], Loss: 0.5415\n",
      "Epoch [210/530], Loss: 0.5353\n",
      "Epoch [215/530], Loss: 0.5292\n",
      "Epoch [220/530], Loss: 0.5234\n",
      "Epoch [225/530], Loss: 0.5177\n",
      "Epoch [230/530], Loss: 0.5123\n",
      "Epoch [235/530], Loss: 0.5069\n",
      "Epoch [240/530], Loss: 0.5018\n",
      "Epoch [245/530], Loss: 0.4968\n",
      "Epoch [250/530], Loss: 0.4919\n",
      "Epoch [255/530], Loss: 0.4872\n",
      "Epoch [260/530], Loss: 0.4826\n",
      "Epoch [265/530], Loss: 0.4782\n",
      "Epoch [270/530], Loss: 0.4738\n",
      "Epoch [275/530], Loss: 0.4696\n",
      "Epoch [280/530], Loss: 0.4655\n",
      "Epoch [285/530], Loss: 0.4616\n",
      "Epoch [290/530], Loss: 0.4576\n",
      "Epoch [295/530], Loss: 0.4537\n",
      "Epoch [300/530], Loss: 0.4499\n",
      "Epoch [305/530], Loss: 0.4462\n",
      "Epoch [310/530], Loss: 0.4426\n",
      "Epoch [315/530], Loss: 0.4391\n",
      "Epoch [320/530], Loss: 0.4357\n",
      "Epoch [325/530], Loss: 0.4324\n",
      "Epoch [330/530], Loss: 0.4291\n",
      "Epoch [335/530], Loss: 0.4260\n",
      "Epoch [340/530], Loss: 0.4229\n",
      "Epoch [345/530], Loss: 0.4199\n",
      "Epoch [350/530], Loss: 0.4169\n",
      "Epoch [355/530], Loss: 0.4140\n",
      "Epoch [360/530], Loss: 0.4112\n",
      "Epoch [365/530], Loss: 0.4085\n",
      "Epoch [370/530], Loss: 0.4058\n",
      "Epoch [375/530], Loss: 0.4032\n",
      "Epoch [380/530], Loss: 0.4006\n",
      "Epoch [385/530], Loss: 0.3980\n",
      "Epoch [390/530], Loss: 0.3955\n",
      "Epoch [395/530], Loss: 0.3931\n",
      "Epoch [400/530], Loss: 0.3907\n",
      "Epoch [405/530], Loss: 0.3884\n",
      "Epoch [410/530], Loss: 0.3861\n",
      "Epoch [415/530], Loss: 0.3838\n",
      "Epoch [420/530], Loss: 0.3816\n",
      "Epoch [425/530], Loss: 0.3794\n",
      "Epoch [430/530], Loss: 0.3773\n",
      "Epoch [435/530], Loss: 0.3752\n",
      "Epoch [440/530], Loss: 0.3731\n",
      "Epoch [445/530], Loss: 0.3711\n",
      "Epoch [450/530], Loss: 0.3691\n",
      "Epoch [455/530], Loss: 0.3671\n",
      "Epoch [460/530], Loss: 0.3652\n",
      "Epoch [465/530], Loss: 0.3632\n",
      "Epoch [470/530], Loss: 0.3614\n",
      "Epoch [475/530], Loss: 0.3595\n",
      "Epoch [480/530], Loss: 0.3577\n",
      "Epoch [485/530], Loss: 0.3559\n",
      "Epoch [490/530], Loss: 0.3541\n",
      "Epoch [495/530], Loss: 0.3524\n",
      "Epoch [500/530], Loss: 0.3507\n",
      "Epoch [505/530], Loss: 0.3490\n",
      "Epoch [510/530], Loss: 0.3473\n",
      "Epoch [515/530], Loss: 0.3456\n",
      "Epoch [520/530], Loss: 0.3440\n",
      "Epoch [525/530], Loss: 0.3424\n",
      "Epoch [530/530], Loss: 0.3408\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2727\n",
      "Epoch [5/540], Loss: 1.2706\n",
      "Epoch [10/540], Loss: 1.0961\n",
      "Epoch [15/540], Loss: 1.0366\n",
      "Epoch [20/540], Loss: 1.0028\n",
      "Epoch [25/540], Loss: 0.9762\n",
      "Epoch [30/540], Loss: 0.9529\n",
      "Epoch [35/540], Loss: 0.9317\n",
      "Epoch [40/540], Loss: 0.9119\n",
      "Epoch [45/540], Loss: 0.8932\n",
      "Epoch [50/540], Loss: 0.8756\n",
      "Epoch [55/540], Loss: 0.8588\n",
      "Epoch [60/540], Loss: 0.8426\n",
      "Epoch [65/540], Loss: 0.8270\n",
      "Epoch [70/540], Loss: 0.8122\n",
      "Epoch [75/540], Loss: 0.7979\n",
      "Epoch [80/540], Loss: 0.7838\n",
      "Epoch [85/540], Loss: 0.7702\n",
      "Epoch [90/540], Loss: 0.7570\n",
      "Epoch [95/540], Loss: 0.7441\n",
      "Epoch [100/540], Loss: 0.7315\n",
      "Epoch [105/540], Loss: 0.7193\n",
      "Epoch [110/540], Loss: 0.7074\n",
      "Epoch [115/540], Loss: 0.6960\n",
      "Epoch [120/540], Loss: 0.6849\n",
      "Epoch [125/540], Loss: 0.6742\n",
      "Epoch [130/540], Loss: 0.6638\n",
      "Epoch [135/540], Loss: 0.6537\n",
      "Epoch [140/540], Loss: 0.6439\n",
      "Epoch [145/540], Loss: 0.6345\n",
      "Epoch [150/540], Loss: 0.6253\n",
      "Epoch [155/540], Loss: 0.6164\n",
      "Epoch [160/540], Loss: 0.6078\n",
      "Epoch [165/540], Loss: 0.5995\n",
      "Epoch [170/540], Loss: 0.5914\n",
      "Epoch [175/540], Loss: 0.5836\n",
      "Epoch [180/540], Loss: 0.5760\n",
      "Epoch [185/540], Loss: 0.5686\n",
      "Epoch [190/540], Loss: 0.5615\n",
      "Epoch [195/540], Loss: 0.5546\n",
      "Epoch [200/540], Loss: 0.5480\n",
      "Epoch [205/540], Loss: 0.5415\n",
      "Epoch [210/540], Loss: 0.5353\n",
      "Epoch [215/540], Loss: 0.5292\n",
      "Epoch [220/540], Loss: 0.5234\n",
      "Epoch [225/540], Loss: 0.5177\n",
      "Epoch [230/540], Loss: 0.5123\n",
      "Epoch [235/540], Loss: 0.5069\n",
      "Epoch [240/540], Loss: 0.5018\n",
      "Epoch [245/540], Loss: 0.4968\n",
      "Epoch [250/540], Loss: 0.4919\n",
      "Epoch [255/540], Loss: 0.4872\n",
      "Epoch [260/540], Loss: 0.4826\n",
      "Epoch [265/540], Loss: 0.4782\n",
      "Epoch [270/540], Loss: 0.4738\n",
      "Epoch [275/540], Loss: 0.4696\n",
      "Epoch [280/540], Loss: 0.4655\n",
      "Epoch [285/540], Loss: 0.4616\n",
      "Epoch [290/540], Loss: 0.4576\n",
      "Epoch [295/540], Loss: 0.4537\n",
      "Epoch [300/540], Loss: 0.4499\n",
      "Epoch [305/540], Loss: 0.4462\n",
      "Epoch [310/540], Loss: 0.4426\n",
      "Epoch [315/540], Loss: 0.4391\n",
      "Epoch [320/540], Loss: 0.4357\n",
      "Epoch [325/540], Loss: 0.4324\n",
      "Epoch [330/540], Loss: 0.4291\n",
      "Epoch [335/540], Loss: 0.4260\n",
      "Epoch [340/540], Loss: 0.4229\n",
      "Epoch [345/540], Loss: 0.4199\n",
      "Epoch [350/540], Loss: 0.4169\n",
      "Epoch [355/540], Loss: 0.4140\n",
      "Epoch [360/540], Loss: 0.4112\n",
      "Epoch [365/540], Loss: 0.4085\n",
      "Epoch [370/540], Loss: 0.4058\n",
      "Epoch [375/540], Loss: 0.4032\n",
      "Epoch [380/540], Loss: 0.4006\n",
      "Epoch [385/540], Loss: 0.3980\n",
      "Epoch [390/540], Loss: 0.3955\n",
      "Epoch [395/540], Loss: 0.3931\n",
      "Epoch [400/540], Loss: 0.3907\n",
      "Epoch [405/540], Loss: 0.3884\n",
      "Epoch [410/540], Loss: 0.3861\n",
      "Epoch [415/540], Loss: 0.3838\n",
      "Epoch [420/540], Loss: 0.3816\n",
      "Epoch [425/540], Loss: 0.3794\n",
      "Epoch [430/540], Loss: 0.3773\n",
      "Epoch [435/540], Loss: 0.3752\n",
      "Epoch [440/540], Loss: 0.3731\n",
      "Epoch [445/540], Loss: 0.3711\n",
      "Epoch [450/540], Loss: 0.3691\n",
      "Epoch [455/540], Loss: 0.3671\n",
      "Epoch [460/540], Loss: 0.3652\n",
      "Epoch [465/540], Loss: 0.3632\n",
      "Epoch [470/540], Loss: 0.3614\n",
      "Epoch [475/540], Loss: 0.3595\n",
      "Epoch [480/540], Loss: 0.3577\n",
      "Epoch [485/540], Loss: 0.3559\n",
      "Epoch [490/540], Loss: 0.3541\n",
      "Epoch [495/540], Loss: 0.3524\n",
      "Epoch [500/540], Loss: 0.3507\n",
      "Epoch [505/540], Loss: 0.3490\n",
      "Epoch [510/540], Loss: 0.3473\n",
      "Epoch [515/540], Loss: 0.3456\n",
      "Epoch [520/540], Loss: 0.3440\n",
      "Epoch [525/540], Loss: 0.3424\n",
      "Epoch [530/540], Loss: 0.3408\n",
      "Epoch [535/540], Loss: 0.3393\n",
      "Epoch [540/540], Loss: 0.3377\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2696\n",
      "Epoch [5/550], Loss: 1.2706\n",
      "Epoch [10/550], Loss: 1.0961\n",
      "Epoch [15/550], Loss: 1.0366\n",
      "Epoch [20/550], Loss: 1.0028\n",
      "Epoch [25/550], Loss: 0.9762\n",
      "Epoch [30/550], Loss: 0.9529\n",
      "Epoch [35/550], Loss: 0.9317\n",
      "Epoch [40/550], Loss: 0.9119\n",
      "Epoch [45/550], Loss: 0.8932\n",
      "Epoch [50/550], Loss: 0.8756\n",
      "Epoch [55/550], Loss: 0.8588\n",
      "Epoch [60/550], Loss: 0.8426\n",
      "Epoch [65/550], Loss: 0.8270\n",
      "Epoch [70/550], Loss: 0.8122\n",
      "Epoch [75/550], Loss: 0.7979\n",
      "Epoch [80/550], Loss: 0.7838\n",
      "Epoch [85/550], Loss: 0.7702\n",
      "Epoch [90/550], Loss: 0.7570\n",
      "Epoch [95/550], Loss: 0.7441\n",
      "Epoch [100/550], Loss: 0.7315\n",
      "Epoch [105/550], Loss: 0.7193\n",
      "Epoch [110/550], Loss: 0.7074\n",
      "Epoch [115/550], Loss: 0.6960\n",
      "Epoch [120/550], Loss: 0.6849\n",
      "Epoch [125/550], Loss: 0.6742\n",
      "Epoch [130/550], Loss: 0.6638\n",
      "Epoch [135/550], Loss: 0.6537\n",
      "Epoch [140/550], Loss: 0.6439\n",
      "Epoch [145/550], Loss: 0.6345\n",
      "Epoch [150/550], Loss: 0.6253\n",
      "Epoch [155/550], Loss: 0.6164\n",
      "Epoch [160/550], Loss: 0.6078\n",
      "Epoch [165/550], Loss: 0.5995\n",
      "Epoch [170/550], Loss: 0.5914\n",
      "Epoch [175/550], Loss: 0.5836\n",
      "Epoch [180/550], Loss: 0.5760\n",
      "Epoch [185/550], Loss: 0.5686\n",
      "Epoch [190/550], Loss: 0.5615\n",
      "Epoch [195/550], Loss: 0.5546\n",
      "Epoch [200/550], Loss: 0.5480\n",
      "Epoch [205/550], Loss: 0.5415\n",
      "Epoch [210/550], Loss: 0.5353\n",
      "Epoch [215/550], Loss: 0.5292\n",
      "Epoch [220/550], Loss: 0.5234\n",
      "Epoch [225/550], Loss: 0.5177\n",
      "Epoch [230/550], Loss: 0.5123\n",
      "Epoch [235/550], Loss: 0.5069\n",
      "Epoch [240/550], Loss: 0.5018\n",
      "Epoch [245/550], Loss: 0.4968\n",
      "Epoch [250/550], Loss: 0.4919\n",
      "Epoch [255/550], Loss: 0.4872\n",
      "Epoch [260/550], Loss: 0.4826\n",
      "Epoch [265/550], Loss: 0.4782\n",
      "Epoch [270/550], Loss: 0.4738\n",
      "Epoch [275/550], Loss: 0.4696\n",
      "Epoch [280/550], Loss: 0.4655\n",
      "Epoch [285/550], Loss: 0.4616\n",
      "Epoch [290/550], Loss: 0.4576\n",
      "Epoch [295/550], Loss: 0.4537\n",
      "Epoch [300/550], Loss: 0.4499\n",
      "Epoch [305/550], Loss: 0.4462\n",
      "Epoch [310/550], Loss: 0.4426\n",
      "Epoch [315/550], Loss: 0.4391\n",
      "Epoch [320/550], Loss: 0.4357\n",
      "Epoch [325/550], Loss: 0.4324\n",
      "Epoch [330/550], Loss: 0.4291\n",
      "Epoch [335/550], Loss: 0.4260\n",
      "Epoch [340/550], Loss: 0.4229\n",
      "Epoch [345/550], Loss: 0.4199\n",
      "Epoch [350/550], Loss: 0.4169\n",
      "Epoch [355/550], Loss: 0.4140\n",
      "Epoch [360/550], Loss: 0.4112\n",
      "Epoch [365/550], Loss: 0.4085\n",
      "Epoch [370/550], Loss: 0.4058\n",
      "Epoch [375/550], Loss: 0.4032\n",
      "Epoch [380/550], Loss: 0.4006\n",
      "Epoch [385/550], Loss: 0.3980\n",
      "Epoch [390/550], Loss: 0.3955\n",
      "Epoch [395/550], Loss: 0.3931\n",
      "Epoch [400/550], Loss: 0.3907\n",
      "Epoch [405/550], Loss: 0.3884\n",
      "Epoch [410/550], Loss: 0.3861\n",
      "Epoch [415/550], Loss: 0.3838\n",
      "Epoch [420/550], Loss: 0.3816\n",
      "Epoch [425/550], Loss: 0.3794\n",
      "Epoch [430/550], Loss: 0.3773\n",
      "Epoch [435/550], Loss: 0.3752\n",
      "Epoch [440/550], Loss: 0.3731\n",
      "Epoch [445/550], Loss: 0.3711\n",
      "Epoch [450/550], Loss: 0.3691\n",
      "Epoch [455/550], Loss: 0.3671\n",
      "Epoch [460/550], Loss: 0.3652\n",
      "Epoch [465/550], Loss: 0.3632\n",
      "Epoch [470/550], Loss: 0.3614\n",
      "Epoch [475/550], Loss: 0.3595\n",
      "Epoch [480/550], Loss: 0.3577\n",
      "Epoch [485/550], Loss: 0.3559\n",
      "Epoch [490/550], Loss: 0.3541\n",
      "Epoch [495/550], Loss: 0.3524\n",
      "Epoch [500/550], Loss: 0.3507\n",
      "Epoch [505/550], Loss: 0.3490\n",
      "Epoch [510/550], Loss: 0.3473\n",
      "Epoch [515/550], Loss: 0.3456\n",
      "Epoch [520/550], Loss: 0.3440\n",
      "Epoch [525/550], Loss: 0.3424\n",
      "Epoch [530/550], Loss: 0.3408\n",
      "Epoch [535/550], Loss: 0.3393\n",
      "Epoch [540/550], Loss: 0.3377\n",
      "Epoch [545/550], Loss: 0.3362\n",
      "Epoch [550/550], Loss: 0.3347\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2665\n",
      "Epoch [5/560], Loss: 1.2706\n",
      "Epoch [10/560], Loss: 1.0961\n",
      "Epoch [15/560], Loss: 1.0366\n",
      "Epoch [20/560], Loss: 1.0028\n",
      "Epoch [25/560], Loss: 0.9762\n",
      "Epoch [30/560], Loss: 0.9529\n",
      "Epoch [35/560], Loss: 0.9317\n",
      "Epoch [40/560], Loss: 0.9119\n",
      "Epoch [45/560], Loss: 0.8932\n",
      "Epoch [50/560], Loss: 0.8756\n",
      "Epoch [55/560], Loss: 0.8588\n",
      "Epoch [60/560], Loss: 0.8426\n",
      "Epoch [65/560], Loss: 0.8270\n",
      "Epoch [70/560], Loss: 0.8122\n",
      "Epoch [75/560], Loss: 0.7979\n",
      "Epoch [80/560], Loss: 0.7838\n",
      "Epoch [85/560], Loss: 0.7702\n",
      "Epoch [90/560], Loss: 0.7570\n",
      "Epoch [95/560], Loss: 0.7441\n",
      "Epoch [100/560], Loss: 0.7315\n",
      "Epoch [105/560], Loss: 0.7193\n",
      "Epoch [110/560], Loss: 0.7074\n",
      "Epoch [115/560], Loss: 0.6960\n",
      "Epoch [120/560], Loss: 0.6849\n",
      "Epoch [125/560], Loss: 0.6742\n",
      "Epoch [130/560], Loss: 0.6638\n",
      "Epoch [135/560], Loss: 0.6537\n",
      "Epoch [140/560], Loss: 0.6439\n",
      "Epoch [145/560], Loss: 0.6345\n",
      "Epoch [150/560], Loss: 0.6253\n",
      "Epoch [155/560], Loss: 0.6164\n",
      "Epoch [160/560], Loss: 0.6078\n",
      "Epoch [165/560], Loss: 0.5995\n",
      "Epoch [170/560], Loss: 0.5914\n",
      "Epoch [175/560], Loss: 0.5836\n",
      "Epoch [180/560], Loss: 0.5760\n",
      "Epoch [185/560], Loss: 0.5686\n",
      "Epoch [190/560], Loss: 0.5615\n",
      "Epoch [195/560], Loss: 0.5546\n",
      "Epoch [200/560], Loss: 0.5480\n",
      "Epoch [205/560], Loss: 0.5415\n",
      "Epoch [210/560], Loss: 0.5353\n",
      "Epoch [215/560], Loss: 0.5292\n",
      "Epoch [220/560], Loss: 0.5234\n",
      "Epoch [225/560], Loss: 0.5177\n",
      "Epoch [230/560], Loss: 0.5123\n",
      "Epoch [235/560], Loss: 0.5069\n",
      "Epoch [240/560], Loss: 0.5018\n",
      "Epoch [245/560], Loss: 0.4968\n",
      "Epoch [250/560], Loss: 0.4919\n",
      "Epoch [255/560], Loss: 0.4872\n",
      "Epoch [260/560], Loss: 0.4826\n",
      "Epoch [265/560], Loss: 0.4782\n",
      "Epoch [270/560], Loss: 0.4738\n",
      "Epoch [275/560], Loss: 0.4696\n",
      "Epoch [280/560], Loss: 0.4655\n",
      "Epoch [285/560], Loss: 0.4616\n",
      "Epoch [290/560], Loss: 0.4576\n",
      "Epoch [295/560], Loss: 0.4537\n",
      "Epoch [300/560], Loss: 0.4499\n",
      "Epoch [305/560], Loss: 0.4462\n",
      "Epoch [310/560], Loss: 0.4426\n",
      "Epoch [315/560], Loss: 0.4391\n",
      "Epoch [320/560], Loss: 0.4357\n",
      "Epoch [325/560], Loss: 0.4324\n",
      "Epoch [330/560], Loss: 0.4291\n",
      "Epoch [335/560], Loss: 0.4260\n",
      "Epoch [340/560], Loss: 0.4229\n",
      "Epoch [345/560], Loss: 0.4199\n",
      "Epoch [350/560], Loss: 0.4169\n",
      "Epoch [355/560], Loss: 0.4140\n",
      "Epoch [360/560], Loss: 0.4112\n",
      "Epoch [365/560], Loss: 0.4085\n",
      "Epoch [370/560], Loss: 0.4058\n",
      "Epoch [375/560], Loss: 0.4032\n",
      "Epoch [380/560], Loss: 0.4006\n",
      "Epoch [385/560], Loss: 0.3980\n",
      "Epoch [390/560], Loss: 0.3955\n",
      "Epoch [395/560], Loss: 0.3931\n",
      "Epoch [400/560], Loss: 0.3907\n",
      "Epoch [405/560], Loss: 0.3884\n",
      "Epoch [410/560], Loss: 0.3861\n",
      "Epoch [415/560], Loss: 0.3838\n",
      "Epoch [420/560], Loss: 0.3816\n",
      "Epoch [425/560], Loss: 0.3794\n",
      "Epoch [430/560], Loss: 0.3773\n",
      "Epoch [435/560], Loss: 0.3752\n",
      "Epoch [440/560], Loss: 0.3731\n",
      "Epoch [445/560], Loss: 0.3711\n",
      "Epoch [450/560], Loss: 0.3691\n",
      "Epoch [455/560], Loss: 0.3671\n",
      "Epoch [460/560], Loss: 0.3652\n",
      "Epoch [465/560], Loss: 0.3632\n",
      "Epoch [470/560], Loss: 0.3614\n",
      "Epoch [475/560], Loss: 0.3595\n",
      "Epoch [480/560], Loss: 0.3577\n",
      "Epoch [485/560], Loss: 0.3559\n",
      "Epoch [490/560], Loss: 0.3541\n",
      "Epoch [495/560], Loss: 0.3524\n",
      "Epoch [500/560], Loss: 0.3507\n",
      "Epoch [505/560], Loss: 0.3490\n",
      "Epoch [510/560], Loss: 0.3473\n",
      "Epoch [515/560], Loss: 0.3456\n",
      "Epoch [520/560], Loss: 0.3440\n",
      "Epoch [525/560], Loss: 0.3424\n",
      "Epoch [530/560], Loss: 0.3408\n",
      "Epoch [535/560], Loss: 0.3393\n",
      "Epoch [540/560], Loss: 0.3377\n",
      "Epoch [545/560], Loss: 0.3362\n",
      "Epoch [550/560], Loss: 0.3347\n",
      "Epoch [555/560], Loss: 0.3332\n",
      "Epoch [560/560], Loss: 0.3317\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2636\n",
      "Epoch [5/570], Loss: 1.2706\n",
      "Epoch [10/570], Loss: 1.0961\n",
      "Epoch [15/570], Loss: 1.0366\n",
      "Epoch [20/570], Loss: 1.0028\n",
      "Epoch [25/570], Loss: 0.9762\n",
      "Epoch [30/570], Loss: 0.9529\n",
      "Epoch [35/570], Loss: 0.9317\n",
      "Epoch [40/570], Loss: 0.9119\n",
      "Epoch [45/570], Loss: 0.8932\n",
      "Epoch [50/570], Loss: 0.8756\n",
      "Epoch [55/570], Loss: 0.8588\n",
      "Epoch [60/570], Loss: 0.8426\n",
      "Epoch [65/570], Loss: 0.8270\n",
      "Epoch [70/570], Loss: 0.8122\n",
      "Epoch [75/570], Loss: 0.7979\n",
      "Epoch [80/570], Loss: 0.7838\n",
      "Epoch [85/570], Loss: 0.7702\n",
      "Epoch [90/570], Loss: 0.7570\n",
      "Epoch [95/570], Loss: 0.7441\n",
      "Epoch [100/570], Loss: 0.7315\n",
      "Epoch [105/570], Loss: 0.7193\n",
      "Epoch [110/570], Loss: 0.7074\n",
      "Epoch [115/570], Loss: 0.6960\n",
      "Epoch [120/570], Loss: 0.6849\n",
      "Epoch [125/570], Loss: 0.6742\n",
      "Epoch [130/570], Loss: 0.6638\n",
      "Epoch [135/570], Loss: 0.6537\n",
      "Epoch [140/570], Loss: 0.6439\n",
      "Epoch [145/570], Loss: 0.6345\n",
      "Epoch [150/570], Loss: 0.6253\n",
      "Epoch [155/570], Loss: 0.6164\n",
      "Epoch [160/570], Loss: 0.6078\n",
      "Epoch [165/570], Loss: 0.5995\n",
      "Epoch [170/570], Loss: 0.5914\n",
      "Epoch [175/570], Loss: 0.5836\n",
      "Epoch [180/570], Loss: 0.5760\n",
      "Epoch [185/570], Loss: 0.5686\n",
      "Epoch [190/570], Loss: 0.5615\n",
      "Epoch [195/570], Loss: 0.5546\n",
      "Epoch [200/570], Loss: 0.5480\n",
      "Epoch [205/570], Loss: 0.5415\n",
      "Epoch [210/570], Loss: 0.5353\n",
      "Epoch [215/570], Loss: 0.5292\n",
      "Epoch [220/570], Loss: 0.5234\n",
      "Epoch [225/570], Loss: 0.5177\n",
      "Epoch [230/570], Loss: 0.5123\n",
      "Epoch [235/570], Loss: 0.5069\n",
      "Epoch [240/570], Loss: 0.5018\n",
      "Epoch [245/570], Loss: 0.4968\n",
      "Epoch [250/570], Loss: 0.4919\n",
      "Epoch [255/570], Loss: 0.4872\n",
      "Epoch [260/570], Loss: 0.4826\n",
      "Epoch [265/570], Loss: 0.4782\n",
      "Epoch [270/570], Loss: 0.4738\n",
      "Epoch [275/570], Loss: 0.4696\n",
      "Epoch [280/570], Loss: 0.4655\n",
      "Epoch [285/570], Loss: 0.4616\n",
      "Epoch [290/570], Loss: 0.4576\n",
      "Epoch [295/570], Loss: 0.4537\n",
      "Epoch [300/570], Loss: 0.4499\n",
      "Epoch [305/570], Loss: 0.4462\n",
      "Epoch [310/570], Loss: 0.4426\n",
      "Epoch [315/570], Loss: 0.4391\n",
      "Epoch [320/570], Loss: 0.4357\n",
      "Epoch [325/570], Loss: 0.4324\n",
      "Epoch [330/570], Loss: 0.4291\n",
      "Epoch [335/570], Loss: 0.4260\n",
      "Epoch [340/570], Loss: 0.4229\n",
      "Epoch [345/570], Loss: 0.4199\n",
      "Epoch [350/570], Loss: 0.4169\n",
      "Epoch [355/570], Loss: 0.4140\n",
      "Epoch [360/570], Loss: 0.4112\n",
      "Epoch [365/570], Loss: 0.4085\n",
      "Epoch [370/570], Loss: 0.4058\n",
      "Epoch [375/570], Loss: 0.4032\n",
      "Epoch [380/570], Loss: 0.4006\n",
      "Epoch [385/570], Loss: 0.3980\n",
      "Epoch [390/570], Loss: 0.3955\n",
      "Epoch [395/570], Loss: 0.3931\n",
      "Epoch [400/570], Loss: 0.3907\n",
      "Epoch [405/570], Loss: 0.3884\n",
      "Epoch [410/570], Loss: 0.3861\n",
      "Epoch [415/570], Loss: 0.3838\n",
      "Epoch [420/570], Loss: 0.3816\n",
      "Epoch [425/570], Loss: 0.3794\n",
      "Epoch [430/570], Loss: 0.3773\n",
      "Epoch [435/570], Loss: 0.3752\n",
      "Epoch [440/570], Loss: 0.3731\n",
      "Epoch [445/570], Loss: 0.3711\n",
      "Epoch [450/570], Loss: 0.3691\n",
      "Epoch [455/570], Loss: 0.3671\n",
      "Epoch [460/570], Loss: 0.3652\n",
      "Epoch [465/570], Loss: 0.3632\n",
      "Epoch [470/570], Loss: 0.3614\n",
      "Epoch [475/570], Loss: 0.3595\n",
      "Epoch [480/570], Loss: 0.3577\n",
      "Epoch [485/570], Loss: 0.3559\n",
      "Epoch [490/570], Loss: 0.3541\n",
      "Epoch [495/570], Loss: 0.3524\n",
      "Epoch [500/570], Loss: 0.3507\n",
      "Epoch [505/570], Loss: 0.3490\n",
      "Epoch [510/570], Loss: 0.3473\n",
      "Epoch [515/570], Loss: 0.3456\n",
      "Epoch [520/570], Loss: 0.3440\n",
      "Epoch [525/570], Loss: 0.3424\n",
      "Epoch [530/570], Loss: 0.3408\n",
      "Epoch [535/570], Loss: 0.3393\n",
      "Epoch [540/570], Loss: 0.3377\n",
      "Epoch [545/570], Loss: 0.3362\n",
      "Epoch [550/570], Loss: 0.3347\n",
      "Epoch [555/570], Loss: 0.3332\n",
      "Epoch [560/570], Loss: 0.3317\n",
      "Epoch [565/570], Loss: 0.3303\n",
      "Epoch [570/570], Loss: 0.3288\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2607\n",
      "Epoch [5/580], Loss: 1.2706\n",
      "Epoch [10/580], Loss: 1.0961\n",
      "Epoch [15/580], Loss: 1.0366\n",
      "Epoch [20/580], Loss: 1.0028\n",
      "Epoch [25/580], Loss: 0.9762\n",
      "Epoch [30/580], Loss: 0.9529\n",
      "Epoch [35/580], Loss: 0.9317\n",
      "Epoch [40/580], Loss: 0.9119\n",
      "Epoch [45/580], Loss: 0.8932\n",
      "Epoch [50/580], Loss: 0.8756\n",
      "Epoch [55/580], Loss: 0.8588\n",
      "Epoch [60/580], Loss: 0.8426\n",
      "Epoch [65/580], Loss: 0.8270\n",
      "Epoch [70/580], Loss: 0.8122\n",
      "Epoch [75/580], Loss: 0.7979\n",
      "Epoch [80/580], Loss: 0.7838\n",
      "Epoch [85/580], Loss: 0.7702\n",
      "Epoch [90/580], Loss: 0.7570\n",
      "Epoch [95/580], Loss: 0.7441\n",
      "Epoch [100/580], Loss: 0.7315\n",
      "Epoch [105/580], Loss: 0.7193\n",
      "Epoch [110/580], Loss: 0.7074\n",
      "Epoch [115/580], Loss: 0.6960\n",
      "Epoch [120/580], Loss: 0.6849\n",
      "Epoch [125/580], Loss: 0.6742\n",
      "Epoch [130/580], Loss: 0.6638\n",
      "Epoch [135/580], Loss: 0.6537\n",
      "Epoch [140/580], Loss: 0.6439\n",
      "Epoch [145/580], Loss: 0.6345\n",
      "Epoch [150/580], Loss: 0.6253\n",
      "Epoch [155/580], Loss: 0.6164\n",
      "Epoch [160/580], Loss: 0.6078\n",
      "Epoch [165/580], Loss: 0.5995\n",
      "Epoch [170/580], Loss: 0.5914\n",
      "Epoch [175/580], Loss: 0.5836\n",
      "Epoch [180/580], Loss: 0.5760\n",
      "Epoch [185/580], Loss: 0.5686\n",
      "Epoch [190/580], Loss: 0.5615\n",
      "Epoch [195/580], Loss: 0.5546\n",
      "Epoch [200/580], Loss: 0.5480\n",
      "Epoch [205/580], Loss: 0.5415\n",
      "Epoch [210/580], Loss: 0.5353\n",
      "Epoch [215/580], Loss: 0.5292\n",
      "Epoch [220/580], Loss: 0.5234\n",
      "Epoch [225/580], Loss: 0.5177\n",
      "Epoch [230/580], Loss: 0.5123\n",
      "Epoch [235/580], Loss: 0.5069\n",
      "Epoch [240/580], Loss: 0.5018\n",
      "Epoch [245/580], Loss: 0.4968\n",
      "Epoch [250/580], Loss: 0.4919\n",
      "Epoch [255/580], Loss: 0.4872\n",
      "Epoch [260/580], Loss: 0.4826\n",
      "Epoch [265/580], Loss: 0.4782\n",
      "Epoch [270/580], Loss: 0.4738\n",
      "Epoch [275/580], Loss: 0.4696\n",
      "Epoch [280/580], Loss: 0.4655\n",
      "Epoch [285/580], Loss: 0.4616\n",
      "Epoch [290/580], Loss: 0.4576\n",
      "Epoch [295/580], Loss: 0.4537\n",
      "Epoch [300/580], Loss: 0.4499\n",
      "Epoch [305/580], Loss: 0.4462\n",
      "Epoch [310/580], Loss: 0.4426\n",
      "Epoch [315/580], Loss: 0.4391\n",
      "Epoch [320/580], Loss: 0.4357\n",
      "Epoch [325/580], Loss: 0.4324\n",
      "Epoch [330/580], Loss: 0.4291\n",
      "Epoch [335/580], Loss: 0.4260\n",
      "Epoch [340/580], Loss: 0.4229\n",
      "Epoch [345/580], Loss: 0.4199\n",
      "Epoch [350/580], Loss: 0.4169\n",
      "Epoch [355/580], Loss: 0.4140\n",
      "Epoch [360/580], Loss: 0.4112\n",
      "Epoch [365/580], Loss: 0.4085\n",
      "Epoch [370/580], Loss: 0.4058\n",
      "Epoch [375/580], Loss: 0.4032\n",
      "Epoch [380/580], Loss: 0.4006\n",
      "Epoch [385/580], Loss: 0.3980\n",
      "Epoch [390/580], Loss: 0.3955\n",
      "Epoch [395/580], Loss: 0.3931\n",
      "Epoch [400/580], Loss: 0.3907\n",
      "Epoch [405/580], Loss: 0.3884\n",
      "Epoch [410/580], Loss: 0.3861\n",
      "Epoch [415/580], Loss: 0.3838\n",
      "Epoch [420/580], Loss: 0.3816\n",
      "Epoch [425/580], Loss: 0.3794\n",
      "Epoch [430/580], Loss: 0.3773\n",
      "Epoch [435/580], Loss: 0.3752\n",
      "Epoch [440/580], Loss: 0.3731\n",
      "Epoch [445/580], Loss: 0.3711\n",
      "Epoch [450/580], Loss: 0.3691\n",
      "Epoch [455/580], Loss: 0.3671\n",
      "Epoch [460/580], Loss: 0.3652\n",
      "Epoch [465/580], Loss: 0.3632\n",
      "Epoch [470/580], Loss: 0.3614\n",
      "Epoch [475/580], Loss: 0.3595\n",
      "Epoch [480/580], Loss: 0.3577\n",
      "Epoch [485/580], Loss: 0.3559\n",
      "Epoch [490/580], Loss: 0.3541\n",
      "Epoch [495/580], Loss: 0.3524\n",
      "Epoch [500/580], Loss: 0.3507\n",
      "Epoch [505/580], Loss: 0.3490\n",
      "Epoch [510/580], Loss: 0.3473\n",
      "Epoch [515/580], Loss: 0.3456\n",
      "Epoch [520/580], Loss: 0.3440\n",
      "Epoch [525/580], Loss: 0.3424\n",
      "Epoch [530/580], Loss: 0.3408\n",
      "Epoch [535/580], Loss: 0.3393\n",
      "Epoch [540/580], Loss: 0.3377\n",
      "Epoch [545/580], Loss: 0.3362\n",
      "Epoch [550/580], Loss: 0.3347\n",
      "Epoch [555/580], Loss: 0.3332\n",
      "Epoch [560/580], Loss: 0.3317\n",
      "Epoch [565/580], Loss: 0.3303\n",
      "Epoch [570/580], Loss: 0.3288\n",
      "Epoch [575/580], Loss: 0.3274\n",
      "Epoch [580/580], Loss: 0.3260\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2579\n",
      "Epoch [5/590], Loss: 1.2706\n",
      "Epoch [10/590], Loss: 1.0961\n",
      "Epoch [15/590], Loss: 1.0366\n",
      "Epoch [20/590], Loss: 1.0028\n",
      "Epoch [25/590], Loss: 0.9762\n",
      "Epoch [30/590], Loss: 0.9529\n",
      "Epoch [35/590], Loss: 0.9317\n",
      "Epoch [40/590], Loss: 0.9119\n",
      "Epoch [45/590], Loss: 0.8932\n",
      "Epoch [50/590], Loss: 0.8756\n",
      "Epoch [55/590], Loss: 0.8588\n",
      "Epoch [60/590], Loss: 0.8426\n",
      "Epoch [65/590], Loss: 0.8270\n",
      "Epoch [70/590], Loss: 0.8122\n",
      "Epoch [75/590], Loss: 0.7979\n",
      "Epoch [80/590], Loss: 0.7838\n",
      "Epoch [85/590], Loss: 0.7702\n",
      "Epoch [90/590], Loss: 0.7570\n",
      "Epoch [95/590], Loss: 0.7441\n",
      "Epoch [100/590], Loss: 0.7315\n",
      "Epoch [105/590], Loss: 0.7193\n",
      "Epoch [110/590], Loss: 0.7074\n",
      "Epoch [115/590], Loss: 0.6960\n",
      "Epoch [120/590], Loss: 0.6849\n",
      "Epoch [125/590], Loss: 0.6742\n",
      "Epoch [130/590], Loss: 0.6638\n",
      "Epoch [135/590], Loss: 0.6537\n",
      "Epoch [140/590], Loss: 0.6439\n",
      "Epoch [145/590], Loss: 0.6345\n",
      "Epoch [150/590], Loss: 0.6253\n",
      "Epoch [155/590], Loss: 0.6164\n",
      "Epoch [160/590], Loss: 0.6078\n",
      "Epoch [165/590], Loss: 0.5995\n",
      "Epoch [170/590], Loss: 0.5914\n",
      "Epoch [175/590], Loss: 0.5836\n",
      "Epoch [180/590], Loss: 0.5760\n",
      "Epoch [185/590], Loss: 0.5686\n",
      "Epoch [190/590], Loss: 0.5615\n",
      "Epoch [195/590], Loss: 0.5546\n",
      "Epoch [200/590], Loss: 0.5480\n",
      "Epoch [205/590], Loss: 0.5415\n",
      "Epoch [210/590], Loss: 0.5353\n",
      "Epoch [215/590], Loss: 0.5292\n",
      "Epoch [220/590], Loss: 0.5234\n",
      "Epoch [225/590], Loss: 0.5177\n",
      "Epoch [230/590], Loss: 0.5123\n",
      "Epoch [235/590], Loss: 0.5069\n",
      "Epoch [240/590], Loss: 0.5018\n",
      "Epoch [245/590], Loss: 0.4968\n",
      "Epoch [250/590], Loss: 0.4919\n",
      "Epoch [255/590], Loss: 0.4872\n",
      "Epoch [260/590], Loss: 0.4826\n",
      "Epoch [265/590], Loss: 0.4782\n",
      "Epoch [270/590], Loss: 0.4738\n",
      "Epoch [275/590], Loss: 0.4696\n",
      "Epoch [280/590], Loss: 0.4655\n",
      "Epoch [285/590], Loss: 0.4616\n",
      "Epoch [290/590], Loss: 0.4576\n",
      "Epoch [295/590], Loss: 0.4537\n",
      "Epoch [300/590], Loss: 0.4499\n",
      "Epoch [305/590], Loss: 0.4462\n",
      "Epoch [310/590], Loss: 0.4426\n",
      "Epoch [315/590], Loss: 0.4391\n",
      "Epoch [320/590], Loss: 0.4357\n",
      "Epoch [325/590], Loss: 0.4324\n",
      "Epoch [330/590], Loss: 0.4291\n",
      "Epoch [335/590], Loss: 0.4260\n",
      "Epoch [340/590], Loss: 0.4229\n",
      "Epoch [345/590], Loss: 0.4199\n",
      "Epoch [350/590], Loss: 0.4169\n",
      "Epoch [355/590], Loss: 0.4140\n",
      "Epoch [360/590], Loss: 0.4112\n",
      "Epoch [365/590], Loss: 0.4085\n",
      "Epoch [370/590], Loss: 0.4058\n",
      "Epoch [375/590], Loss: 0.4032\n",
      "Epoch [380/590], Loss: 0.4006\n",
      "Epoch [385/590], Loss: 0.3980\n",
      "Epoch [390/590], Loss: 0.3955\n",
      "Epoch [395/590], Loss: 0.3931\n",
      "Epoch [400/590], Loss: 0.3907\n",
      "Epoch [405/590], Loss: 0.3884\n",
      "Epoch [410/590], Loss: 0.3861\n",
      "Epoch [415/590], Loss: 0.3838\n",
      "Epoch [420/590], Loss: 0.3816\n",
      "Epoch [425/590], Loss: 0.3794\n",
      "Epoch [430/590], Loss: 0.3773\n",
      "Epoch [435/590], Loss: 0.3752\n",
      "Epoch [440/590], Loss: 0.3731\n",
      "Epoch [445/590], Loss: 0.3711\n",
      "Epoch [450/590], Loss: 0.3691\n",
      "Epoch [455/590], Loss: 0.3671\n",
      "Epoch [460/590], Loss: 0.3652\n",
      "Epoch [465/590], Loss: 0.3632\n",
      "Epoch [470/590], Loss: 0.3614\n",
      "Epoch [475/590], Loss: 0.3595\n",
      "Epoch [480/590], Loss: 0.3577\n",
      "Epoch [485/590], Loss: 0.3559\n",
      "Epoch [490/590], Loss: 0.3541\n",
      "Epoch [495/590], Loss: 0.3524\n",
      "Epoch [500/590], Loss: 0.3507\n",
      "Epoch [505/590], Loss: 0.3490\n",
      "Epoch [510/590], Loss: 0.3473\n",
      "Epoch [515/590], Loss: 0.3456\n",
      "Epoch [520/590], Loss: 0.3440\n",
      "Epoch [525/590], Loss: 0.3424\n",
      "Epoch [530/590], Loss: 0.3408\n",
      "Epoch [535/590], Loss: 0.3393\n",
      "Epoch [540/590], Loss: 0.3377\n",
      "Epoch [545/590], Loss: 0.3362\n",
      "Epoch [550/590], Loss: 0.3347\n",
      "Epoch [555/590], Loss: 0.3332\n",
      "Epoch [560/590], Loss: 0.3317\n",
      "Epoch [565/590], Loss: 0.3303\n",
      "Epoch [570/590], Loss: 0.3288\n",
      "Epoch [575/590], Loss: 0.3274\n",
      "Epoch [580/590], Loss: 0.3260\n",
      "Epoch [585/590], Loss: 0.3246\n",
      "Epoch [590/590], Loss: 0.3232\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2552\n",
      "Epoch [5/600], Loss: 1.2706\n",
      "Epoch [10/600], Loss: 1.0961\n",
      "Epoch [15/600], Loss: 1.0366\n",
      "Epoch [20/600], Loss: 1.0028\n",
      "Epoch [25/600], Loss: 0.9762\n",
      "Epoch [30/600], Loss: 0.9529\n",
      "Epoch [35/600], Loss: 0.9317\n",
      "Epoch [40/600], Loss: 0.9119\n",
      "Epoch [45/600], Loss: 0.8932\n",
      "Epoch [50/600], Loss: 0.8756\n",
      "Epoch [55/600], Loss: 0.8588\n",
      "Epoch [60/600], Loss: 0.8426\n",
      "Epoch [65/600], Loss: 0.8270\n",
      "Epoch [70/600], Loss: 0.8122\n",
      "Epoch [75/600], Loss: 0.7979\n",
      "Epoch [80/600], Loss: 0.7838\n",
      "Epoch [85/600], Loss: 0.7702\n",
      "Epoch [90/600], Loss: 0.7570\n",
      "Epoch [95/600], Loss: 0.7441\n",
      "Epoch [100/600], Loss: 0.7315\n",
      "Epoch [105/600], Loss: 0.7193\n",
      "Epoch [110/600], Loss: 0.7074\n",
      "Epoch [115/600], Loss: 0.6960\n",
      "Epoch [120/600], Loss: 0.6849\n",
      "Epoch [125/600], Loss: 0.6742\n",
      "Epoch [130/600], Loss: 0.6638\n",
      "Epoch [135/600], Loss: 0.6537\n",
      "Epoch [140/600], Loss: 0.6439\n",
      "Epoch [145/600], Loss: 0.6345\n",
      "Epoch [150/600], Loss: 0.6253\n",
      "Epoch [155/600], Loss: 0.6164\n",
      "Epoch [160/600], Loss: 0.6078\n",
      "Epoch [165/600], Loss: 0.5995\n",
      "Epoch [170/600], Loss: 0.5914\n",
      "Epoch [175/600], Loss: 0.5836\n",
      "Epoch [180/600], Loss: 0.5760\n",
      "Epoch [185/600], Loss: 0.5686\n",
      "Epoch [190/600], Loss: 0.5615\n",
      "Epoch [195/600], Loss: 0.5546\n",
      "Epoch [200/600], Loss: 0.5480\n",
      "Epoch [205/600], Loss: 0.5415\n",
      "Epoch [210/600], Loss: 0.5353\n",
      "Epoch [215/600], Loss: 0.5292\n",
      "Epoch [220/600], Loss: 0.5234\n",
      "Epoch [225/600], Loss: 0.5177\n",
      "Epoch [230/600], Loss: 0.5123\n",
      "Epoch [235/600], Loss: 0.5069\n",
      "Epoch [240/600], Loss: 0.5018\n",
      "Epoch [245/600], Loss: 0.4968\n",
      "Epoch [250/600], Loss: 0.4919\n",
      "Epoch [255/600], Loss: 0.4872\n",
      "Epoch [260/600], Loss: 0.4826\n",
      "Epoch [265/600], Loss: 0.4782\n",
      "Epoch [270/600], Loss: 0.4738\n",
      "Epoch [275/600], Loss: 0.4696\n",
      "Epoch [280/600], Loss: 0.4655\n",
      "Epoch [285/600], Loss: 0.4616\n",
      "Epoch [290/600], Loss: 0.4576\n",
      "Epoch [295/600], Loss: 0.4537\n",
      "Epoch [300/600], Loss: 0.4499\n",
      "Epoch [305/600], Loss: 0.4462\n",
      "Epoch [310/600], Loss: 0.4426\n",
      "Epoch [315/600], Loss: 0.4391\n",
      "Epoch [320/600], Loss: 0.4357\n",
      "Epoch [325/600], Loss: 0.4324\n",
      "Epoch [330/600], Loss: 0.4291\n",
      "Epoch [335/600], Loss: 0.4260\n",
      "Epoch [340/600], Loss: 0.4229\n",
      "Epoch [345/600], Loss: 0.4199\n",
      "Epoch [350/600], Loss: 0.4169\n",
      "Epoch [355/600], Loss: 0.4140\n",
      "Epoch [360/600], Loss: 0.4112\n",
      "Epoch [365/600], Loss: 0.4085\n",
      "Epoch [370/600], Loss: 0.4058\n",
      "Epoch [375/600], Loss: 0.4032\n",
      "Epoch [380/600], Loss: 0.4006\n",
      "Epoch [385/600], Loss: 0.3980\n",
      "Epoch [390/600], Loss: 0.3955\n",
      "Epoch [395/600], Loss: 0.3931\n",
      "Epoch [400/600], Loss: 0.3907\n",
      "Epoch [405/600], Loss: 0.3884\n",
      "Epoch [410/600], Loss: 0.3861\n",
      "Epoch [415/600], Loss: 0.3838\n",
      "Epoch [420/600], Loss: 0.3816\n",
      "Epoch [425/600], Loss: 0.3794\n",
      "Epoch [430/600], Loss: 0.3773\n",
      "Epoch [435/600], Loss: 0.3752\n",
      "Epoch [440/600], Loss: 0.3731\n",
      "Epoch [445/600], Loss: 0.3711\n",
      "Epoch [450/600], Loss: 0.3691\n",
      "Epoch [455/600], Loss: 0.3671\n",
      "Epoch [460/600], Loss: 0.3652\n",
      "Epoch [465/600], Loss: 0.3632\n",
      "Epoch [470/600], Loss: 0.3614\n",
      "Epoch [475/600], Loss: 0.3595\n",
      "Epoch [480/600], Loss: 0.3577\n",
      "Epoch [485/600], Loss: 0.3559\n",
      "Epoch [490/600], Loss: 0.3541\n",
      "Epoch [495/600], Loss: 0.3524\n",
      "Epoch [500/600], Loss: 0.3507\n",
      "Epoch [505/600], Loss: 0.3490\n",
      "Epoch [510/600], Loss: 0.3473\n",
      "Epoch [515/600], Loss: 0.3456\n",
      "Epoch [520/600], Loss: 0.3440\n",
      "Epoch [525/600], Loss: 0.3424\n",
      "Epoch [530/600], Loss: 0.3408\n",
      "Epoch [535/600], Loss: 0.3393\n",
      "Epoch [540/600], Loss: 0.3377\n",
      "Epoch [545/600], Loss: 0.3362\n",
      "Epoch [550/600], Loss: 0.3347\n",
      "Epoch [555/600], Loss: 0.3332\n",
      "Epoch [560/600], Loss: 0.3317\n",
      "Epoch [565/600], Loss: 0.3303\n",
      "Epoch [570/600], Loss: 0.3288\n",
      "Epoch [575/600], Loss: 0.3274\n",
      "Epoch [580/600], Loss: 0.3260\n",
      "Epoch [585/600], Loss: 0.3246\n",
      "Epoch [590/600], Loss: 0.3232\n",
      "Epoch [595/600], Loss: 0.3218\n",
      "Epoch [600/600], Loss: 0.3204\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2525\n",
      "Epoch [5/610], Loss: 1.2706\n",
      "Epoch [10/610], Loss: 1.0961\n",
      "Epoch [15/610], Loss: 1.0366\n",
      "Epoch [20/610], Loss: 1.0028\n",
      "Epoch [25/610], Loss: 0.9762\n",
      "Epoch [30/610], Loss: 0.9529\n",
      "Epoch [35/610], Loss: 0.9317\n",
      "Epoch [40/610], Loss: 0.9119\n",
      "Epoch [45/610], Loss: 0.8932\n",
      "Epoch [50/610], Loss: 0.8756\n",
      "Epoch [55/610], Loss: 0.8588\n",
      "Epoch [60/610], Loss: 0.8426\n",
      "Epoch [65/610], Loss: 0.8270\n",
      "Epoch [70/610], Loss: 0.8122\n",
      "Epoch [75/610], Loss: 0.7979\n",
      "Epoch [80/610], Loss: 0.7838\n",
      "Epoch [85/610], Loss: 0.7702\n",
      "Epoch [90/610], Loss: 0.7570\n",
      "Epoch [95/610], Loss: 0.7441\n",
      "Epoch [100/610], Loss: 0.7315\n",
      "Epoch [105/610], Loss: 0.7193\n",
      "Epoch [110/610], Loss: 0.7074\n",
      "Epoch [115/610], Loss: 0.6960\n",
      "Epoch [120/610], Loss: 0.6849\n",
      "Epoch [125/610], Loss: 0.6742\n",
      "Epoch [130/610], Loss: 0.6638\n",
      "Epoch [135/610], Loss: 0.6537\n",
      "Epoch [140/610], Loss: 0.6439\n",
      "Epoch [145/610], Loss: 0.6345\n",
      "Epoch [150/610], Loss: 0.6253\n",
      "Epoch [155/610], Loss: 0.6164\n",
      "Epoch [160/610], Loss: 0.6078\n",
      "Epoch [165/610], Loss: 0.5995\n",
      "Epoch [170/610], Loss: 0.5914\n",
      "Epoch [175/610], Loss: 0.5836\n",
      "Epoch [180/610], Loss: 0.5760\n",
      "Epoch [185/610], Loss: 0.5686\n",
      "Epoch [190/610], Loss: 0.5615\n",
      "Epoch [195/610], Loss: 0.5546\n",
      "Epoch [200/610], Loss: 0.5480\n",
      "Epoch [205/610], Loss: 0.5415\n",
      "Epoch [210/610], Loss: 0.5353\n",
      "Epoch [215/610], Loss: 0.5292\n",
      "Epoch [220/610], Loss: 0.5234\n",
      "Epoch [225/610], Loss: 0.5177\n",
      "Epoch [230/610], Loss: 0.5123\n",
      "Epoch [235/610], Loss: 0.5069\n",
      "Epoch [240/610], Loss: 0.5018\n",
      "Epoch [245/610], Loss: 0.4968\n",
      "Epoch [250/610], Loss: 0.4919\n",
      "Epoch [255/610], Loss: 0.4872\n",
      "Epoch [260/610], Loss: 0.4826\n",
      "Epoch [265/610], Loss: 0.4782\n",
      "Epoch [270/610], Loss: 0.4738\n",
      "Epoch [275/610], Loss: 0.4696\n",
      "Epoch [280/610], Loss: 0.4655\n",
      "Epoch [285/610], Loss: 0.4616\n",
      "Epoch [290/610], Loss: 0.4576\n",
      "Epoch [295/610], Loss: 0.4537\n",
      "Epoch [300/610], Loss: 0.4499\n",
      "Epoch [305/610], Loss: 0.4462\n",
      "Epoch [310/610], Loss: 0.4426\n",
      "Epoch [315/610], Loss: 0.4391\n",
      "Epoch [320/610], Loss: 0.4357\n",
      "Epoch [325/610], Loss: 0.4324\n",
      "Epoch [330/610], Loss: 0.4291\n",
      "Epoch [335/610], Loss: 0.4260\n",
      "Epoch [340/610], Loss: 0.4229\n",
      "Epoch [345/610], Loss: 0.4199\n",
      "Epoch [350/610], Loss: 0.4169\n",
      "Epoch [355/610], Loss: 0.4140\n",
      "Epoch [360/610], Loss: 0.4112\n",
      "Epoch [365/610], Loss: 0.4085\n",
      "Epoch [370/610], Loss: 0.4058\n",
      "Epoch [375/610], Loss: 0.4032\n",
      "Epoch [380/610], Loss: 0.4006\n",
      "Epoch [385/610], Loss: 0.3980\n",
      "Epoch [390/610], Loss: 0.3955\n",
      "Epoch [395/610], Loss: 0.3931\n",
      "Epoch [400/610], Loss: 0.3907\n",
      "Epoch [405/610], Loss: 0.3884\n",
      "Epoch [410/610], Loss: 0.3861\n",
      "Epoch [415/610], Loss: 0.3838\n",
      "Epoch [420/610], Loss: 0.3816\n",
      "Epoch [425/610], Loss: 0.3794\n",
      "Epoch [430/610], Loss: 0.3773\n",
      "Epoch [435/610], Loss: 0.3752\n",
      "Epoch [440/610], Loss: 0.3731\n",
      "Epoch [445/610], Loss: 0.3711\n",
      "Epoch [450/610], Loss: 0.3691\n",
      "Epoch [455/610], Loss: 0.3671\n",
      "Epoch [460/610], Loss: 0.3652\n",
      "Epoch [465/610], Loss: 0.3632\n",
      "Epoch [470/610], Loss: 0.3614\n",
      "Epoch [475/610], Loss: 0.3595\n",
      "Epoch [480/610], Loss: 0.3577\n",
      "Epoch [485/610], Loss: 0.3559\n",
      "Epoch [490/610], Loss: 0.3541\n",
      "Epoch [495/610], Loss: 0.3524\n",
      "Epoch [500/610], Loss: 0.3507\n",
      "Epoch [505/610], Loss: 0.3490\n",
      "Epoch [510/610], Loss: 0.3473\n",
      "Epoch [515/610], Loss: 0.3456\n",
      "Epoch [520/610], Loss: 0.3440\n",
      "Epoch [525/610], Loss: 0.3424\n",
      "Epoch [530/610], Loss: 0.3408\n",
      "Epoch [535/610], Loss: 0.3393\n",
      "Epoch [540/610], Loss: 0.3377\n",
      "Epoch [545/610], Loss: 0.3362\n",
      "Epoch [550/610], Loss: 0.3347\n",
      "Epoch [555/610], Loss: 0.3332\n",
      "Epoch [560/610], Loss: 0.3317\n",
      "Epoch [565/610], Loss: 0.3303\n",
      "Epoch [570/610], Loss: 0.3288\n",
      "Epoch [575/610], Loss: 0.3274\n",
      "Epoch [580/610], Loss: 0.3260\n",
      "Epoch [585/610], Loss: 0.3246\n",
      "Epoch [590/610], Loss: 0.3232\n",
      "Epoch [595/610], Loss: 0.3218\n",
      "Epoch [600/610], Loss: 0.3204\n",
      "Epoch [605/610], Loss: 0.3191\n",
      "Epoch [610/610], Loss: 0.3178\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2499\n",
      "Epoch [5/620], Loss: 1.2706\n",
      "Epoch [10/620], Loss: 1.0961\n",
      "Epoch [15/620], Loss: 1.0366\n",
      "Epoch [20/620], Loss: 1.0028\n",
      "Epoch [25/620], Loss: 0.9762\n",
      "Epoch [30/620], Loss: 0.9529\n",
      "Epoch [35/620], Loss: 0.9317\n",
      "Epoch [40/620], Loss: 0.9119\n",
      "Epoch [45/620], Loss: 0.8932\n",
      "Epoch [50/620], Loss: 0.8756\n",
      "Epoch [55/620], Loss: 0.8588\n",
      "Epoch [60/620], Loss: 0.8426\n",
      "Epoch [65/620], Loss: 0.8270\n",
      "Epoch [70/620], Loss: 0.8122\n",
      "Epoch [75/620], Loss: 0.7979\n",
      "Epoch [80/620], Loss: 0.7838\n",
      "Epoch [85/620], Loss: 0.7702\n",
      "Epoch [90/620], Loss: 0.7570\n",
      "Epoch [95/620], Loss: 0.7441\n",
      "Epoch [100/620], Loss: 0.7315\n",
      "Epoch [105/620], Loss: 0.7193\n",
      "Epoch [110/620], Loss: 0.7074\n",
      "Epoch [115/620], Loss: 0.6960\n",
      "Epoch [120/620], Loss: 0.6849\n",
      "Epoch [125/620], Loss: 0.6742\n",
      "Epoch [130/620], Loss: 0.6638\n",
      "Epoch [135/620], Loss: 0.6537\n",
      "Epoch [140/620], Loss: 0.6439\n",
      "Epoch [145/620], Loss: 0.6345\n",
      "Epoch [150/620], Loss: 0.6253\n",
      "Epoch [155/620], Loss: 0.6164\n",
      "Epoch [160/620], Loss: 0.6078\n",
      "Epoch [165/620], Loss: 0.5995\n",
      "Epoch [170/620], Loss: 0.5914\n",
      "Epoch [175/620], Loss: 0.5836\n",
      "Epoch [180/620], Loss: 0.5760\n",
      "Epoch [185/620], Loss: 0.5686\n",
      "Epoch [190/620], Loss: 0.5615\n",
      "Epoch [195/620], Loss: 0.5546\n",
      "Epoch [200/620], Loss: 0.5480\n",
      "Epoch [205/620], Loss: 0.5415\n",
      "Epoch [210/620], Loss: 0.5353\n",
      "Epoch [215/620], Loss: 0.5292\n",
      "Epoch [220/620], Loss: 0.5234\n",
      "Epoch [225/620], Loss: 0.5177\n",
      "Epoch [230/620], Loss: 0.5123\n",
      "Epoch [235/620], Loss: 0.5069\n",
      "Epoch [240/620], Loss: 0.5018\n",
      "Epoch [245/620], Loss: 0.4968\n",
      "Epoch [250/620], Loss: 0.4919\n",
      "Epoch [255/620], Loss: 0.4872\n",
      "Epoch [260/620], Loss: 0.4826\n",
      "Epoch [265/620], Loss: 0.4782\n",
      "Epoch [270/620], Loss: 0.4738\n",
      "Epoch [275/620], Loss: 0.4696\n",
      "Epoch [280/620], Loss: 0.4655\n",
      "Epoch [285/620], Loss: 0.4616\n",
      "Epoch [290/620], Loss: 0.4576\n",
      "Epoch [295/620], Loss: 0.4537\n",
      "Epoch [300/620], Loss: 0.4499\n",
      "Epoch [305/620], Loss: 0.4462\n",
      "Epoch [310/620], Loss: 0.4426\n",
      "Epoch [315/620], Loss: 0.4391\n",
      "Epoch [320/620], Loss: 0.4357\n",
      "Epoch [325/620], Loss: 0.4324\n",
      "Epoch [330/620], Loss: 0.4291\n",
      "Epoch [335/620], Loss: 0.4260\n",
      "Epoch [340/620], Loss: 0.4229\n",
      "Epoch [345/620], Loss: 0.4199\n",
      "Epoch [350/620], Loss: 0.4169\n",
      "Epoch [355/620], Loss: 0.4140\n",
      "Epoch [360/620], Loss: 0.4112\n",
      "Epoch [365/620], Loss: 0.4085\n",
      "Epoch [370/620], Loss: 0.4058\n",
      "Epoch [375/620], Loss: 0.4032\n",
      "Epoch [380/620], Loss: 0.4006\n",
      "Epoch [385/620], Loss: 0.3980\n",
      "Epoch [390/620], Loss: 0.3955\n",
      "Epoch [395/620], Loss: 0.3931\n",
      "Epoch [400/620], Loss: 0.3907\n",
      "Epoch [405/620], Loss: 0.3884\n",
      "Epoch [410/620], Loss: 0.3861\n",
      "Epoch [415/620], Loss: 0.3838\n",
      "Epoch [420/620], Loss: 0.3816\n",
      "Epoch [425/620], Loss: 0.3794\n",
      "Epoch [430/620], Loss: 0.3773\n",
      "Epoch [435/620], Loss: 0.3752\n",
      "Epoch [440/620], Loss: 0.3731\n",
      "Epoch [445/620], Loss: 0.3711\n",
      "Epoch [450/620], Loss: 0.3691\n",
      "Epoch [455/620], Loss: 0.3671\n",
      "Epoch [460/620], Loss: 0.3652\n",
      "Epoch [465/620], Loss: 0.3632\n",
      "Epoch [470/620], Loss: 0.3614\n",
      "Epoch [475/620], Loss: 0.3595\n",
      "Epoch [480/620], Loss: 0.3577\n",
      "Epoch [485/620], Loss: 0.3559\n",
      "Epoch [490/620], Loss: 0.3541\n",
      "Epoch [495/620], Loss: 0.3524\n",
      "Epoch [500/620], Loss: 0.3507\n",
      "Epoch [505/620], Loss: 0.3490\n",
      "Epoch [510/620], Loss: 0.3473\n",
      "Epoch [515/620], Loss: 0.3456\n",
      "Epoch [520/620], Loss: 0.3440\n",
      "Epoch [525/620], Loss: 0.3424\n",
      "Epoch [530/620], Loss: 0.3408\n",
      "Epoch [535/620], Loss: 0.3393\n",
      "Epoch [540/620], Loss: 0.3377\n",
      "Epoch [545/620], Loss: 0.3362\n",
      "Epoch [550/620], Loss: 0.3347\n",
      "Epoch [555/620], Loss: 0.3332\n",
      "Epoch [560/620], Loss: 0.3317\n",
      "Epoch [565/620], Loss: 0.3303\n",
      "Epoch [570/620], Loss: 0.3288\n",
      "Epoch [575/620], Loss: 0.3274\n",
      "Epoch [580/620], Loss: 0.3260\n",
      "Epoch [585/620], Loss: 0.3246\n",
      "Epoch [590/620], Loss: 0.3232\n",
      "Epoch [595/620], Loss: 0.3218\n",
      "Epoch [600/620], Loss: 0.3204\n",
      "Epoch [605/620], Loss: 0.3191\n",
      "Epoch [610/620], Loss: 0.3178\n",
      "Epoch [615/620], Loss: 0.3164\n",
      "Epoch [620/620], Loss: 0.3151\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2473\n",
      "Epoch [5/630], Loss: 1.2706\n",
      "Epoch [10/630], Loss: 1.0961\n",
      "Epoch [15/630], Loss: 1.0366\n",
      "Epoch [20/630], Loss: 1.0028\n",
      "Epoch [25/630], Loss: 0.9762\n",
      "Epoch [30/630], Loss: 0.9529\n",
      "Epoch [35/630], Loss: 0.9317\n",
      "Epoch [40/630], Loss: 0.9119\n",
      "Epoch [45/630], Loss: 0.8932\n",
      "Epoch [50/630], Loss: 0.8756\n",
      "Epoch [55/630], Loss: 0.8588\n",
      "Epoch [60/630], Loss: 0.8426\n",
      "Epoch [65/630], Loss: 0.8270\n",
      "Epoch [70/630], Loss: 0.8122\n",
      "Epoch [75/630], Loss: 0.7979\n",
      "Epoch [80/630], Loss: 0.7838\n",
      "Epoch [85/630], Loss: 0.7702\n",
      "Epoch [90/630], Loss: 0.7570\n",
      "Epoch [95/630], Loss: 0.7441\n",
      "Epoch [100/630], Loss: 0.7315\n",
      "Epoch [105/630], Loss: 0.7193\n",
      "Epoch [110/630], Loss: 0.7074\n",
      "Epoch [115/630], Loss: 0.6960\n",
      "Epoch [120/630], Loss: 0.6849\n",
      "Epoch [125/630], Loss: 0.6742\n",
      "Epoch [130/630], Loss: 0.6638\n",
      "Epoch [135/630], Loss: 0.6537\n",
      "Epoch [140/630], Loss: 0.6439\n",
      "Epoch [145/630], Loss: 0.6345\n",
      "Epoch [150/630], Loss: 0.6253\n",
      "Epoch [155/630], Loss: 0.6164\n",
      "Epoch [160/630], Loss: 0.6078\n",
      "Epoch [165/630], Loss: 0.5995\n",
      "Epoch [170/630], Loss: 0.5914\n",
      "Epoch [175/630], Loss: 0.5836\n",
      "Epoch [180/630], Loss: 0.5760\n",
      "Epoch [185/630], Loss: 0.5686\n",
      "Epoch [190/630], Loss: 0.5615\n",
      "Epoch [195/630], Loss: 0.5546\n",
      "Epoch [200/630], Loss: 0.5480\n",
      "Epoch [205/630], Loss: 0.5415\n",
      "Epoch [210/630], Loss: 0.5353\n",
      "Epoch [215/630], Loss: 0.5292\n",
      "Epoch [220/630], Loss: 0.5234\n",
      "Epoch [225/630], Loss: 0.5177\n",
      "Epoch [230/630], Loss: 0.5123\n",
      "Epoch [235/630], Loss: 0.5069\n",
      "Epoch [240/630], Loss: 0.5018\n",
      "Epoch [245/630], Loss: 0.4968\n",
      "Epoch [250/630], Loss: 0.4919\n",
      "Epoch [255/630], Loss: 0.4872\n",
      "Epoch [260/630], Loss: 0.4826\n",
      "Epoch [265/630], Loss: 0.4782\n",
      "Epoch [270/630], Loss: 0.4738\n",
      "Epoch [275/630], Loss: 0.4696\n",
      "Epoch [280/630], Loss: 0.4655\n",
      "Epoch [285/630], Loss: 0.4616\n",
      "Epoch [290/630], Loss: 0.4576\n",
      "Epoch [295/630], Loss: 0.4537\n",
      "Epoch [300/630], Loss: 0.4499\n",
      "Epoch [305/630], Loss: 0.4462\n",
      "Epoch [310/630], Loss: 0.4426\n",
      "Epoch [315/630], Loss: 0.4391\n",
      "Epoch [320/630], Loss: 0.4357\n",
      "Epoch [325/630], Loss: 0.4324\n",
      "Epoch [330/630], Loss: 0.4291\n",
      "Epoch [335/630], Loss: 0.4260\n",
      "Epoch [340/630], Loss: 0.4229\n",
      "Epoch [345/630], Loss: 0.4199\n",
      "Epoch [350/630], Loss: 0.4169\n",
      "Epoch [355/630], Loss: 0.4140\n",
      "Epoch [360/630], Loss: 0.4112\n",
      "Epoch [365/630], Loss: 0.4085\n",
      "Epoch [370/630], Loss: 0.4058\n",
      "Epoch [375/630], Loss: 0.4032\n",
      "Epoch [380/630], Loss: 0.4006\n",
      "Epoch [385/630], Loss: 0.3980\n",
      "Epoch [390/630], Loss: 0.3955\n",
      "Epoch [395/630], Loss: 0.3931\n",
      "Epoch [400/630], Loss: 0.3907\n",
      "Epoch [405/630], Loss: 0.3884\n",
      "Epoch [410/630], Loss: 0.3861\n",
      "Epoch [415/630], Loss: 0.3838\n",
      "Epoch [420/630], Loss: 0.3816\n",
      "Epoch [425/630], Loss: 0.3794\n",
      "Epoch [430/630], Loss: 0.3773\n",
      "Epoch [435/630], Loss: 0.3752\n",
      "Epoch [440/630], Loss: 0.3731\n",
      "Epoch [445/630], Loss: 0.3711\n",
      "Epoch [450/630], Loss: 0.3691\n",
      "Epoch [455/630], Loss: 0.3671\n",
      "Epoch [460/630], Loss: 0.3652\n",
      "Epoch [465/630], Loss: 0.3632\n",
      "Epoch [470/630], Loss: 0.3614\n",
      "Epoch [475/630], Loss: 0.3595\n",
      "Epoch [480/630], Loss: 0.3577\n",
      "Epoch [485/630], Loss: 0.3559\n",
      "Epoch [490/630], Loss: 0.3541\n",
      "Epoch [495/630], Loss: 0.3524\n",
      "Epoch [500/630], Loss: 0.3507\n",
      "Epoch [505/630], Loss: 0.3490\n",
      "Epoch [510/630], Loss: 0.3473\n",
      "Epoch [515/630], Loss: 0.3456\n",
      "Epoch [520/630], Loss: 0.3440\n",
      "Epoch [525/630], Loss: 0.3424\n",
      "Epoch [530/630], Loss: 0.3408\n",
      "Epoch [535/630], Loss: 0.3393\n",
      "Epoch [540/630], Loss: 0.3377\n",
      "Epoch [545/630], Loss: 0.3362\n",
      "Epoch [550/630], Loss: 0.3347\n",
      "Epoch [555/630], Loss: 0.3332\n",
      "Epoch [560/630], Loss: 0.3317\n",
      "Epoch [565/630], Loss: 0.3303\n",
      "Epoch [570/630], Loss: 0.3288\n",
      "Epoch [575/630], Loss: 0.3274\n",
      "Epoch [580/630], Loss: 0.3260\n",
      "Epoch [585/630], Loss: 0.3246\n",
      "Epoch [590/630], Loss: 0.3232\n",
      "Epoch [595/630], Loss: 0.3218\n",
      "Epoch [600/630], Loss: 0.3204\n",
      "Epoch [605/630], Loss: 0.3191\n",
      "Epoch [610/630], Loss: 0.3178\n",
      "Epoch [615/630], Loss: 0.3164\n",
      "Epoch [620/630], Loss: 0.3151\n",
      "Epoch [625/630], Loss: 0.3138\n",
      "Epoch [630/630], Loss: 0.3125\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2448\n",
      "Epoch [5/640], Loss: 1.2706\n",
      "Epoch [10/640], Loss: 1.0961\n",
      "Epoch [15/640], Loss: 1.0366\n",
      "Epoch [20/640], Loss: 1.0028\n",
      "Epoch [25/640], Loss: 0.9762\n",
      "Epoch [30/640], Loss: 0.9529\n",
      "Epoch [35/640], Loss: 0.9317\n",
      "Epoch [40/640], Loss: 0.9119\n",
      "Epoch [45/640], Loss: 0.8932\n",
      "Epoch [50/640], Loss: 0.8756\n",
      "Epoch [55/640], Loss: 0.8588\n",
      "Epoch [60/640], Loss: 0.8426\n",
      "Epoch [65/640], Loss: 0.8270\n",
      "Epoch [70/640], Loss: 0.8122\n",
      "Epoch [75/640], Loss: 0.7979\n",
      "Epoch [80/640], Loss: 0.7838\n",
      "Epoch [85/640], Loss: 0.7702\n",
      "Epoch [90/640], Loss: 0.7570\n",
      "Epoch [95/640], Loss: 0.7441\n",
      "Epoch [100/640], Loss: 0.7315\n",
      "Epoch [105/640], Loss: 0.7193\n",
      "Epoch [110/640], Loss: 0.7074\n",
      "Epoch [115/640], Loss: 0.6960\n",
      "Epoch [120/640], Loss: 0.6849\n",
      "Epoch [125/640], Loss: 0.6742\n",
      "Epoch [130/640], Loss: 0.6638\n",
      "Epoch [135/640], Loss: 0.6537\n",
      "Epoch [140/640], Loss: 0.6439\n",
      "Epoch [145/640], Loss: 0.6345\n",
      "Epoch [150/640], Loss: 0.6253\n",
      "Epoch [155/640], Loss: 0.6164\n",
      "Epoch [160/640], Loss: 0.6078\n",
      "Epoch [165/640], Loss: 0.5995\n",
      "Epoch [170/640], Loss: 0.5914\n",
      "Epoch [175/640], Loss: 0.5836\n",
      "Epoch [180/640], Loss: 0.5760\n",
      "Epoch [185/640], Loss: 0.5686\n",
      "Epoch [190/640], Loss: 0.5615\n",
      "Epoch [195/640], Loss: 0.5546\n",
      "Epoch [200/640], Loss: 0.5480\n",
      "Epoch [205/640], Loss: 0.5415\n",
      "Epoch [210/640], Loss: 0.5353\n",
      "Epoch [215/640], Loss: 0.5292\n",
      "Epoch [220/640], Loss: 0.5234\n",
      "Epoch [225/640], Loss: 0.5177\n",
      "Epoch [230/640], Loss: 0.5123\n",
      "Epoch [235/640], Loss: 0.5069\n",
      "Epoch [240/640], Loss: 0.5018\n",
      "Epoch [245/640], Loss: 0.4968\n",
      "Epoch [250/640], Loss: 0.4919\n",
      "Epoch [255/640], Loss: 0.4872\n",
      "Epoch [260/640], Loss: 0.4826\n",
      "Epoch [265/640], Loss: 0.4782\n",
      "Epoch [270/640], Loss: 0.4738\n",
      "Epoch [275/640], Loss: 0.4696\n",
      "Epoch [280/640], Loss: 0.4655\n",
      "Epoch [285/640], Loss: 0.4616\n",
      "Epoch [290/640], Loss: 0.4576\n",
      "Epoch [295/640], Loss: 0.4537\n",
      "Epoch [300/640], Loss: 0.4499\n",
      "Epoch [305/640], Loss: 0.4462\n",
      "Epoch [310/640], Loss: 0.4426\n",
      "Epoch [315/640], Loss: 0.4391\n",
      "Epoch [320/640], Loss: 0.4357\n",
      "Epoch [325/640], Loss: 0.4324\n",
      "Epoch [330/640], Loss: 0.4291\n",
      "Epoch [335/640], Loss: 0.4260\n",
      "Epoch [340/640], Loss: 0.4229\n",
      "Epoch [345/640], Loss: 0.4199\n",
      "Epoch [350/640], Loss: 0.4169\n",
      "Epoch [355/640], Loss: 0.4140\n",
      "Epoch [360/640], Loss: 0.4112\n",
      "Epoch [365/640], Loss: 0.4085\n",
      "Epoch [370/640], Loss: 0.4058\n",
      "Epoch [375/640], Loss: 0.4032\n",
      "Epoch [380/640], Loss: 0.4006\n",
      "Epoch [385/640], Loss: 0.3980\n",
      "Epoch [390/640], Loss: 0.3955\n",
      "Epoch [395/640], Loss: 0.3931\n",
      "Epoch [400/640], Loss: 0.3907\n",
      "Epoch [405/640], Loss: 0.3884\n",
      "Epoch [410/640], Loss: 0.3861\n",
      "Epoch [415/640], Loss: 0.3838\n",
      "Epoch [420/640], Loss: 0.3816\n",
      "Epoch [425/640], Loss: 0.3794\n",
      "Epoch [430/640], Loss: 0.3773\n",
      "Epoch [435/640], Loss: 0.3752\n",
      "Epoch [440/640], Loss: 0.3731\n",
      "Epoch [445/640], Loss: 0.3711\n",
      "Epoch [450/640], Loss: 0.3691\n",
      "Epoch [455/640], Loss: 0.3671\n",
      "Epoch [460/640], Loss: 0.3652\n",
      "Epoch [465/640], Loss: 0.3632\n",
      "Epoch [470/640], Loss: 0.3614\n",
      "Epoch [475/640], Loss: 0.3595\n",
      "Epoch [480/640], Loss: 0.3577\n",
      "Epoch [485/640], Loss: 0.3559\n",
      "Epoch [490/640], Loss: 0.3541\n",
      "Epoch [495/640], Loss: 0.3524\n",
      "Epoch [500/640], Loss: 0.3507\n",
      "Epoch [505/640], Loss: 0.3490\n",
      "Epoch [510/640], Loss: 0.3473\n",
      "Epoch [515/640], Loss: 0.3456\n",
      "Epoch [520/640], Loss: 0.3440\n",
      "Epoch [525/640], Loss: 0.3424\n",
      "Epoch [530/640], Loss: 0.3408\n",
      "Epoch [535/640], Loss: 0.3393\n",
      "Epoch [540/640], Loss: 0.3377\n",
      "Epoch [545/640], Loss: 0.3362\n",
      "Epoch [550/640], Loss: 0.3347\n",
      "Epoch [555/640], Loss: 0.3332\n",
      "Epoch [560/640], Loss: 0.3317\n",
      "Epoch [565/640], Loss: 0.3303\n",
      "Epoch [570/640], Loss: 0.3288\n",
      "Epoch [575/640], Loss: 0.3274\n",
      "Epoch [580/640], Loss: 0.3260\n",
      "Epoch [585/640], Loss: 0.3246\n",
      "Epoch [590/640], Loss: 0.3232\n",
      "Epoch [595/640], Loss: 0.3218\n",
      "Epoch [600/640], Loss: 0.3204\n",
      "Epoch [605/640], Loss: 0.3191\n",
      "Epoch [610/640], Loss: 0.3178\n",
      "Epoch [615/640], Loss: 0.3164\n",
      "Epoch [620/640], Loss: 0.3151\n",
      "Epoch [625/640], Loss: 0.3138\n",
      "Epoch [630/640], Loss: 0.3125\n",
      "Epoch [635/640], Loss: 0.3112\n",
      "Epoch [640/640], Loss: 0.3099\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2424\n",
      "Epoch [5/650], Loss: 1.2706\n",
      "Epoch [10/650], Loss: 1.0961\n",
      "Epoch [15/650], Loss: 1.0366\n",
      "Epoch [20/650], Loss: 1.0028\n",
      "Epoch [25/650], Loss: 0.9762\n",
      "Epoch [30/650], Loss: 0.9529\n",
      "Epoch [35/650], Loss: 0.9317\n",
      "Epoch [40/650], Loss: 0.9119\n",
      "Epoch [45/650], Loss: 0.8932\n",
      "Epoch [50/650], Loss: 0.8756\n",
      "Epoch [55/650], Loss: 0.8588\n",
      "Epoch [60/650], Loss: 0.8426\n",
      "Epoch [65/650], Loss: 0.8270\n",
      "Epoch [70/650], Loss: 0.8122\n",
      "Epoch [75/650], Loss: 0.7979\n",
      "Epoch [80/650], Loss: 0.7838\n",
      "Epoch [85/650], Loss: 0.7702\n",
      "Epoch [90/650], Loss: 0.7570\n",
      "Epoch [95/650], Loss: 0.7441\n",
      "Epoch [100/650], Loss: 0.7315\n",
      "Epoch [105/650], Loss: 0.7193\n",
      "Epoch [110/650], Loss: 0.7074\n",
      "Epoch [115/650], Loss: 0.6960\n",
      "Epoch [120/650], Loss: 0.6849\n",
      "Epoch [125/650], Loss: 0.6742\n",
      "Epoch [130/650], Loss: 0.6638\n",
      "Epoch [135/650], Loss: 0.6537\n",
      "Epoch [140/650], Loss: 0.6439\n",
      "Epoch [145/650], Loss: 0.6345\n",
      "Epoch [150/650], Loss: 0.6253\n",
      "Epoch [155/650], Loss: 0.6164\n",
      "Epoch [160/650], Loss: 0.6078\n",
      "Epoch [165/650], Loss: 0.5995\n",
      "Epoch [170/650], Loss: 0.5914\n",
      "Epoch [175/650], Loss: 0.5836\n",
      "Epoch [180/650], Loss: 0.5760\n",
      "Epoch [185/650], Loss: 0.5686\n",
      "Epoch [190/650], Loss: 0.5615\n",
      "Epoch [195/650], Loss: 0.5546\n",
      "Epoch [200/650], Loss: 0.5480\n",
      "Epoch [205/650], Loss: 0.5415\n",
      "Epoch [210/650], Loss: 0.5353\n",
      "Epoch [215/650], Loss: 0.5292\n",
      "Epoch [220/650], Loss: 0.5234\n",
      "Epoch [225/650], Loss: 0.5177\n",
      "Epoch [230/650], Loss: 0.5123\n",
      "Epoch [235/650], Loss: 0.5069\n",
      "Epoch [240/650], Loss: 0.5018\n",
      "Epoch [245/650], Loss: 0.4968\n",
      "Epoch [250/650], Loss: 0.4919\n",
      "Epoch [255/650], Loss: 0.4872\n",
      "Epoch [260/650], Loss: 0.4826\n",
      "Epoch [265/650], Loss: 0.4782\n",
      "Epoch [270/650], Loss: 0.4738\n",
      "Epoch [275/650], Loss: 0.4696\n",
      "Epoch [280/650], Loss: 0.4655\n",
      "Epoch [285/650], Loss: 0.4616\n",
      "Epoch [290/650], Loss: 0.4576\n",
      "Epoch [295/650], Loss: 0.4537\n",
      "Epoch [300/650], Loss: 0.4499\n",
      "Epoch [305/650], Loss: 0.4462\n",
      "Epoch [310/650], Loss: 0.4426\n",
      "Epoch [315/650], Loss: 0.4391\n",
      "Epoch [320/650], Loss: 0.4357\n",
      "Epoch [325/650], Loss: 0.4324\n",
      "Epoch [330/650], Loss: 0.4291\n",
      "Epoch [335/650], Loss: 0.4260\n",
      "Epoch [340/650], Loss: 0.4229\n",
      "Epoch [345/650], Loss: 0.4199\n",
      "Epoch [350/650], Loss: 0.4169\n",
      "Epoch [355/650], Loss: 0.4140\n",
      "Epoch [360/650], Loss: 0.4112\n",
      "Epoch [365/650], Loss: 0.4085\n",
      "Epoch [370/650], Loss: 0.4058\n",
      "Epoch [375/650], Loss: 0.4032\n",
      "Epoch [380/650], Loss: 0.4006\n",
      "Epoch [385/650], Loss: 0.3980\n",
      "Epoch [390/650], Loss: 0.3955\n",
      "Epoch [395/650], Loss: 0.3931\n",
      "Epoch [400/650], Loss: 0.3907\n",
      "Epoch [405/650], Loss: 0.3884\n",
      "Epoch [410/650], Loss: 0.3861\n",
      "Epoch [415/650], Loss: 0.3838\n",
      "Epoch [420/650], Loss: 0.3816\n",
      "Epoch [425/650], Loss: 0.3794\n",
      "Epoch [430/650], Loss: 0.3773\n",
      "Epoch [435/650], Loss: 0.3752\n",
      "Epoch [440/650], Loss: 0.3731\n",
      "Epoch [445/650], Loss: 0.3711\n",
      "Epoch [450/650], Loss: 0.3691\n",
      "Epoch [455/650], Loss: 0.3671\n",
      "Epoch [460/650], Loss: 0.3652\n",
      "Epoch [465/650], Loss: 0.3632\n",
      "Epoch [470/650], Loss: 0.3614\n",
      "Epoch [475/650], Loss: 0.3595\n",
      "Epoch [480/650], Loss: 0.3577\n",
      "Epoch [485/650], Loss: 0.3559\n",
      "Epoch [490/650], Loss: 0.3541\n",
      "Epoch [495/650], Loss: 0.3524\n",
      "Epoch [500/650], Loss: 0.3507\n",
      "Epoch [505/650], Loss: 0.3490\n",
      "Epoch [510/650], Loss: 0.3473\n",
      "Epoch [515/650], Loss: 0.3456\n",
      "Epoch [520/650], Loss: 0.3440\n",
      "Epoch [525/650], Loss: 0.3424\n",
      "Epoch [530/650], Loss: 0.3408\n",
      "Epoch [535/650], Loss: 0.3393\n",
      "Epoch [540/650], Loss: 0.3377\n",
      "Epoch [545/650], Loss: 0.3362\n",
      "Epoch [550/650], Loss: 0.3347\n",
      "Epoch [555/650], Loss: 0.3332\n",
      "Epoch [560/650], Loss: 0.3317\n",
      "Epoch [565/650], Loss: 0.3303\n",
      "Epoch [570/650], Loss: 0.3288\n",
      "Epoch [575/650], Loss: 0.3274\n",
      "Epoch [580/650], Loss: 0.3260\n",
      "Epoch [585/650], Loss: 0.3246\n",
      "Epoch [590/650], Loss: 0.3232\n",
      "Epoch [595/650], Loss: 0.3218\n",
      "Epoch [600/650], Loss: 0.3204\n",
      "Epoch [605/650], Loss: 0.3191\n",
      "Epoch [610/650], Loss: 0.3178\n",
      "Epoch [615/650], Loss: 0.3164\n",
      "Epoch [620/650], Loss: 0.3151\n",
      "Epoch [625/650], Loss: 0.3138\n",
      "Epoch [630/650], Loss: 0.3125\n",
      "Epoch [635/650], Loss: 0.3112\n",
      "Epoch [640/650], Loss: 0.3099\n",
      "Epoch [645/650], Loss: 0.3087\n",
      "Epoch [650/650], Loss: 0.3074\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2400\n",
      "Epoch [5/660], Loss: 1.2706\n",
      "Epoch [10/660], Loss: 1.0961\n",
      "Epoch [15/660], Loss: 1.0366\n",
      "Epoch [20/660], Loss: 1.0028\n",
      "Epoch [25/660], Loss: 0.9762\n",
      "Epoch [30/660], Loss: 0.9529\n",
      "Epoch [35/660], Loss: 0.9317\n",
      "Epoch [40/660], Loss: 0.9119\n",
      "Epoch [45/660], Loss: 0.8932\n",
      "Epoch [50/660], Loss: 0.8756\n",
      "Epoch [55/660], Loss: 0.8588\n",
      "Epoch [60/660], Loss: 0.8426\n",
      "Epoch [65/660], Loss: 0.8270\n",
      "Epoch [70/660], Loss: 0.8122\n",
      "Epoch [75/660], Loss: 0.7979\n",
      "Epoch [80/660], Loss: 0.7838\n",
      "Epoch [85/660], Loss: 0.7702\n",
      "Epoch [90/660], Loss: 0.7570\n",
      "Epoch [95/660], Loss: 0.7441\n",
      "Epoch [100/660], Loss: 0.7315\n",
      "Epoch [105/660], Loss: 0.7193\n",
      "Epoch [110/660], Loss: 0.7074\n",
      "Epoch [115/660], Loss: 0.6960\n",
      "Epoch [120/660], Loss: 0.6849\n",
      "Epoch [125/660], Loss: 0.6742\n",
      "Epoch [130/660], Loss: 0.6638\n",
      "Epoch [135/660], Loss: 0.6537\n",
      "Epoch [140/660], Loss: 0.6439\n",
      "Epoch [145/660], Loss: 0.6345\n",
      "Epoch [150/660], Loss: 0.6253\n",
      "Epoch [155/660], Loss: 0.6164\n",
      "Epoch [160/660], Loss: 0.6078\n",
      "Epoch [165/660], Loss: 0.5995\n",
      "Epoch [170/660], Loss: 0.5914\n",
      "Epoch [175/660], Loss: 0.5836\n",
      "Epoch [180/660], Loss: 0.5760\n",
      "Epoch [185/660], Loss: 0.5686\n",
      "Epoch [190/660], Loss: 0.5615\n",
      "Epoch [195/660], Loss: 0.5546\n",
      "Epoch [200/660], Loss: 0.5480\n",
      "Epoch [205/660], Loss: 0.5415\n",
      "Epoch [210/660], Loss: 0.5353\n",
      "Epoch [215/660], Loss: 0.5292\n",
      "Epoch [220/660], Loss: 0.5234\n",
      "Epoch [225/660], Loss: 0.5177\n",
      "Epoch [230/660], Loss: 0.5123\n",
      "Epoch [235/660], Loss: 0.5069\n",
      "Epoch [240/660], Loss: 0.5018\n",
      "Epoch [245/660], Loss: 0.4968\n",
      "Epoch [250/660], Loss: 0.4919\n",
      "Epoch [255/660], Loss: 0.4872\n",
      "Epoch [260/660], Loss: 0.4826\n",
      "Epoch [265/660], Loss: 0.4782\n",
      "Epoch [270/660], Loss: 0.4738\n",
      "Epoch [275/660], Loss: 0.4696\n",
      "Epoch [280/660], Loss: 0.4655\n",
      "Epoch [285/660], Loss: 0.4616\n",
      "Epoch [290/660], Loss: 0.4576\n",
      "Epoch [295/660], Loss: 0.4537\n",
      "Epoch [300/660], Loss: 0.4499\n",
      "Epoch [305/660], Loss: 0.4462\n",
      "Epoch [310/660], Loss: 0.4426\n",
      "Epoch [315/660], Loss: 0.4391\n",
      "Epoch [320/660], Loss: 0.4357\n",
      "Epoch [325/660], Loss: 0.4324\n",
      "Epoch [330/660], Loss: 0.4291\n",
      "Epoch [335/660], Loss: 0.4260\n",
      "Epoch [340/660], Loss: 0.4229\n",
      "Epoch [345/660], Loss: 0.4199\n",
      "Epoch [350/660], Loss: 0.4169\n",
      "Epoch [355/660], Loss: 0.4140\n",
      "Epoch [360/660], Loss: 0.4112\n",
      "Epoch [365/660], Loss: 0.4085\n",
      "Epoch [370/660], Loss: 0.4058\n",
      "Epoch [375/660], Loss: 0.4032\n",
      "Epoch [380/660], Loss: 0.4006\n",
      "Epoch [385/660], Loss: 0.3980\n",
      "Epoch [390/660], Loss: 0.3955\n",
      "Epoch [395/660], Loss: 0.3931\n",
      "Epoch [400/660], Loss: 0.3907\n",
      "Epoch [405/660], Loss: 0.3884\n",
      "Epoch [410/660], Loss: 0.3861\n",
      "Epoch [415/660], Loss: 0.3838\n",
      "Epoch [420/660], Loss: 0.3816\n",
      "Epoch [425/660], Loss: 0.3794\n",
      "Epoch [430/660], Loss: 0.3773\n",
      "Epoch [435/660], Loss: 0.3752\n",
      "Epoch [440/660], Loss: 0.3731\n",
      "Epoch [445/660], Loss: 0.3711\n",
      "Epoch [450/660], Loss: 0.3691\n",
      "Epoch [455/660], Loss: 0.3671\n",
      "Epoch [460/660], Loss: 0.3652\n",
      "Epoch [465/660], Loss: 0.3632\n",
      "Epoch [470/660], Loss: 0.3614\n",
      "Epoch [475/660], Loss: 0.3595\n",
      "Epoch [480/660], Loss: 0.3577\n",
      "Epoch [485/660], Loss: 0.3559\n",
      "Epoch [490/660], Loss: 0.3541\n",
      "Epoch [495/660], Loss: 0.3524\n",
      "Epoch [500/660], Loss: 0.3507\n",
      "Epoch [505/660], Loss: 0.3490\n",
      "Epoch [510/660], Loss: 0.3473\n",
      "Epoch [515/660], Loss: 0.3456\n",
      "Epoch [520/660], Loss: 0.3440\n",
      "Epoch [525/660], Loss: 0.3424\n",
      "Epoch [530/660], Loss: 0.3408\n",
      "Epoch [535/660], Loss: 0.3393\n",
      "Epoch [540/660], Loss: 0.3377\n",
      "Epoch [545/660], Loss: 0.3362\n",
      "Epoch [550/660], Loss: 0.3347\n",
      "Epoch [555/660], Loss: 0.3332\n",
      "Epoch [560/660], Loss: 0.3317\n",
      "Epoch [565/660], Loss: 0.3303\n",
      "Epoch [570/660], Loss: 0.3288\n",
      "Epoch [575/660], Loss: 0.3274\n",
      "Epoch [580/660], Loss: 0.3260\n",
      "Epoch [585/660], Loss: 0.3246\n",
      "Epoch [590/660], Loss: 0.3232\n",
      "Epoch [595/660], Loss: 0.3218\n",
      "Epoch [600/660], Loss: 0.3204\n",
      "Epoch [605/660], Loss: 0.3191\n",
      "Epoch [610/660], Loss: 0.3178\n",
      "Epoch [615/660], Loss: 0.3164\n",
      "Epoch [620/660], Loss: 0.3151\n",
      "Epoch [625/660], Loss: 0.3138\n",
      "Epoch [630/660], Loss: 0.3125\n",
      "Epoch [635/660], Loss: 0.3112\n",
      "Epoch [640/660], Loss: 0.3099\n",
      "Epoch [645/660], Loss: 0.3087\n",
      "Epoch [650/660], Loss: 0.3074\n",
      "Epoch [655/660], Loss: 0.3061\n",
      "Epoch [660/660], Loss: 0.3049\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2377\n",
      "Epoch [5/670], Loss: 1.2706\n",
      "Epoch [10/670], Loss: 1.0961\n",
      "Epoch [15/670], Loss: 1.0366\n",
      "Epoch [20/670], Loss: 1.0028\n",
      "Epoch [25/670], Loss: 0.9762\n",
      "Epoch [30/670], Loss: 0.9529\n",
      "Epoch [35/670], Loss: 0.9317\n",
      "Epoch [40/670], Loss: 0.9119\n",
      "Epoch [45/670], Loss: 0.8932\n",
      "Epoch [50/670], Loss: 0.8756\n",
      "Epoch [55/670], Loss: 0.8588\n",
      "Epoch [60/670], Loss: 0.8426\n",
      "Epoch [65/670], Loss: 0.8270\n",
      "Epoch [70/670], Loss: 0.8122\n",
      "Epoch [75/670], Loss: 0.7979\n",
      "Epoch [80/670], Loss: 0.7838\n",
      "Epoch [85/670], Loss: 0.7702\n",
      "Epoch [90/670], Loss: 0.7570\n",
      "Epoch [95/670], Loss: 0.7441\n",
      "Epoch [100/670], Loss: 0.7315\n",
      "Epoch [105/670], Loss: 0.7193\n",
      "Epoch [110/670], Loss: 0.7074\n",
      "Epoch [115/670], Loss: 0.6960\n",
      "Epoch [120/670], Loss: 0.6849\n",
      "Epoch [125/670], Loss: 0.6742\n",
      "Epoch [130/670], Loss: 0.6638\n",
      "Epoch [135/670], Loss: 0.6537\n",
      "Epoch [140/670], Loss: 0.6439\n",
      "Epoch [145/670], Loss: 0.6345\n",
      "Epoch [150/670], Loss: 0.6253\n",
      "Epoch [155/670], Loss: 0.6164\n",
      "Epoch [160/670], Loss: 0.6078\n",
      "Epoch [165/670], Loss: 0.5995\n",
      "Epoch [170/670], Loss: 0.5914\n",
      "Epoch [175/670], Loss: 0.5836\n",
      "Epoch [180/670], Loss: 0.5760\n",
      "Epoch [185/670], Loss: 0.5686\n",
      "Epoch [190/670], Loss: 0.5615\n",
      "Epoch [195/670], Loss: 0.5546\n",
      "Epoch [200/670], Loss: 0.5480\n",
      "Epoch [205/670], Loss: 0.5415\n",
      "Epoch [210/670], Loss: 0.5353\n",
      "Epoch [215/670], Loss: 0.5292\n",
      "Epoch [220/670], Loss: 0.5234\n",
      "Epoch [225/670], Loss: 0.5177\n",
      "Epoch [230/670], Loss: 0.5123\n",
      "Epoch [235/670], Loss: 0.5069\n",
      "Epoch [240/670], Loss: 0.5018\n",
      "Epoch [245/670], Loss: 0.4968\n",
      "Epoch [250/670], Loss: 0.4919\n",
      "Epoch [255/670], Loss: 0.4872\n",
      "Epoch [260/670], Loss: 0.4826\n",
      "Epoch [265/670], Loss: 0.4782\n",
      "Epoch [270/670], Loss: 0.4738\n",
      "Epoch [275/670], Loss: 0.4696\n",
      "Epoch [280/670], Loss: 0.4655\n",
      "Epoch [285/670], Loss: 0.4616\n",
      "Epoch [290/670], Loss: 0.4576\n",
      "Epoch [295/670], Loss: 0.4537\n",
      "Epoch [300/670], Loss: 0.4499\n",
      "Epoch [305/670], Loss: 0.4462\n",
      "Epoch [310/670], Loss: 0.4426\n",
      "Epoch [315/670], Loss: 0.4391\n",
      "Epoch [320/670], Loss: 0.4357\n",
      "Epoch [325/670], Loss: 0.4324\n",
      "Epoch [330/670], Loss: 0.4291\n",
      "Epoch [335/670], Loss: 0.4260\n",
      "Epoch [340/670], Loss: 0.4229\n",
      "Epoch [345/670], Loss: 0.4199\n",
      "Epoch [350/670], Loss: 0.4169\n",
      "Epoch [355/670], Loss: 0.4140\n",
      "Epoch [360/670], Loss: 0.4112\n",
      "Epoch [365/670], Loss: 0.4085\n",
      "Epoch [370/670], Loss: 0.4058\n",
      "Epoch [375/670], Loss: 0.4032\n",
      "Epoch [380/670], Loss: 0.4006\n",
      "Epoch [385/670], Loss: 0.3980\n",
      "Epoch [390/670], Loss: 0.3955\n",
      "Epoch [395/670], Loss: 0.3931\n",
      "Epoch [400/670], Loss: 0.3907\n",
      "Epoch [405/670], Loss: 0.3884\n",
      "Epoch [410/670], Loss: 0.3861\n",
      "Epoch [415/670], Loss: 0.3838\n",
      "Epoch [420/670], Loss: 0.3816\n",
      "Epoch [425/670], Loss: 0.3794\n",
      "Epoch [430/670], Loss: 0.3773\n",
      "Epoch [435/670], Loss: 0.3752\n",
      "Epoch [440/670], Loss: 0.3731\n",
      "Epoch [445/670], Loss: 0.3711\n",
      "Epoch [450/670], Loss: 0.3691\n",
      "Epoch [455/670], Loss: 0.3671\n",
      "Epoch [460/670], Loss: 0.3652\n",
      "Epoch [465/670], Loss: 0.3632\n",
      "Epoch [470/670], Loss: 0.3614\n",
      "Epoch [475/670], Loss: 0.3595\n",
      "Epoch [480/670], Loss: 0.3577\n",
      "Epoch [485/670], Loss: 0.3559\n",
      "Epoch [490/670], Loss: 0.3541\n",
      "Epoch [495/670], Loss: 0.3524\n",
      "Epoch [500/670], Loss: 0.3507\n",
      "Epoch [505/670], Loss: 0.3490\n",
      "Epoch [510/670], Loss: 0.3473\n",
      "Epoch [515/670], Loss: 0.3456\n",
      "Epoch [520/670], Loss: 0.3440\n",
      "Epoch [525/670], Loss: 0.3424\n",
      "Epoch [530/670], Loss: 0.3408\n",
      "Epoch [535/670], Loss: 0.3393\n",
      "Epoch [540/670], Loss: 0.3377\n",
      "Epoch [545/670], Loss: 0.3362\n",
      "Epoch [550/670], Loss: 0.3347\n",
      "Epoch [555/670], Loss: 0.3332\n",
      "Epoch [560/670], Loss: 0.3317\n",
      "Epoch [565/670], Loss: 0.3303\n",
      "Epoch [570/670], Loss: 0.3288\n",
      "Epoch [575/670], Loss: 0.3274\n",
      "Epoch [580/670], Loss: 0.3260\n",
      "Epoch [585/670], Loss: 0.3246\n",
      "Epoch [590/670], Loss: 0.3232\n",
      "Epoch [595/670], Loss: 0.3218\n",
      "Epoch [600/670], Loss: 0.3204\n",
      "Epoch [605/670], Loss: 0.3191\n",
      "Epoch [610/670], Loss: 0.3178\n",
      "Epoch [615/670], Loss: 0.3164\n",
      "Epoch [620/670], Loss: 0.3151\n",
      "Epoch [625/670], Loss: 0.3138\n",
      "Epoch [630/670], Loss: 0.3125\n",
      "Epoch [635/670], Loss: 0.3112\n",
      "Epoch [640/670], Loss: 0.3099\n",
      "Epoch [645/670], Loss: 0.3087\n",
      "Epoch [650/670], Loss: 0.3074\n",
      "Epoch [655/670], Loss: 0.3061\n",
      "Epoch [660/670], Loss: 0.3049\n",
      "Epoch [665/670], Loss: 0.3036\n",
      "Epoch [670/670], Loss: 0.3024\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2354\n",
      "Epoch [5/680], Loss: 1.2706\n",
      "Epoch [10/680], Loss: 1.0961\n",
      "Epoch [15/680], Loss: 1.0366\n",
      "Epoch [20/680], Loss: 1.0028\n",
      "Epoch [25/680], Loss: 0.9762\n",
      "Epoch [30/680], Loss: 0.9529\n",
      "Epoch [35/680], Loss: 0.9317\n",
      "Epoch [40/680], Loss: 0.9119\n",
      "Epoch [45/680], Loss: 0.8932\n",
      "Epoch [50/680], Loss: 0.8756\n",
      "Epoch [55/680], Loss: 0.8588\n",
      "Epoch [60/680], Loss: 0.8426\n",
      "Epoch [65/680], Loss: 0.8270\n",
      "Epoch [70/680], Loss: 0.8122\n",
      "Epoch [75/680], Loss: 0.7979\n",
      "Epoch [80/680], Loss: 0.7838\n",
      "Epoch [85/680], Loss: 0.7702\n",
      "Epoch [90/680], Loss: 0.7570\n",
      "Epoch [95/680], Loss: 0.7441\n",
      "Epoch [100/680], Loss: 0.7315\n",
      "Epoch [105/680], Loss: 0.7193\n",
      "Epoch [110/680], Loss: 0.7074\n",
      "Epoch [115/680], Loss: 0.6960\n",
      "Epoch [120/680], Loss: 0.6849\n",
      "Epoch [125/680], Loss: 0.6742\n",
      "Epoch [130/680], Loss: 0.6638\n",
      "Epoch [135/680], Loss: 0.6537\n",
      "Epoch [140/680], Loss: 0.6439\n",
      "Epoch [145/680], Loss: 0.6345\n",
      "Epoch [150/680], Loss: 0.6253\n",
      "Epoch [155/680], Loss: 0.6164\n",
      "Epoch [160/680], Loss: 0.6078\n",
      "Epoch [165/680], Loss: 0.5995\n",
      "Epoch [170/680], Loss: 0.5914\n",
      "Epoch [175/680], Loss: 0.5836\n",
      "Epoch [180/680], Loss: 0.5760\n",
      "Epoch [185/680], Loss: 0.5686\n",
      "Epoch [190/680], Loss: 0.5615\n",
      "Epoch [195/680], Loss: 0.5546\n",
      "Epoch [200/680], Loss: 0.5480\n",
      "Epoch [205/680], Loss: 0.5415\n",
      "Epoch [210/680], Loss: 0.5353\n",
      "Epoch [215/680], Loss: 0.5292\n",
      "Epoch [220/680], Loss: 0.5234\n",
      "Epoch [225/680], Loss: 0.5177\n",
      "Epoch [230/680], Loss: 0.5123\n",
      "Epoch [235/680], Loss: 0.5069\n",
      "Epoch [240/680], Loss: 0.5018\n",
      "Epoch [245/680], Loss: 0.4968\n",
      "Epoch [250/680], Loss: 0.4919\n",
      "Epoch [255/680], Loss: 0.4872\n",
      "Epoch [260/680], Loss: 0.4826\n",
      "Epoch [265/680], Loss: 0.4782\n",
      "Epoch [270/680], Loss: 0.4738\n",
      "Epoch [275/680], Loss: 0.4696\n",
      "Epoch [280/680], Loss: 0.4655\n",
      "Epoch [285/680], Loss: 0.4616\n",
      "Epoch [290/680], Loss: 0.4576\n",
      "Epoch [295/680], Loss: 0.4537\n",
      "Epoch [300/680], Loss: 0.4499\n",
      "Epoch [305/680], Loss: 0.4462\n",
      "Epoch [310/680], Loss: 0.4426\n",
      "Epoch [315/680], Loss: 0.4391\n",
      "Epoch [320/680], Loss: 0.4357\n",
      "Epoch [325/680], Loss: 0.4324\n",
      "Epoch [330/680], Loss: 0.4291\n",
      "Epoch [335/680], Loss: 0.4260\n",
      "Epoch [340/680], Loss: 0.4229\n",
      "Epoch [345/680], Loss: 0.4199\n",
      "Epoch [350/680], Loss: 0.4169\n",
      "Epoch [355/680], Loss: 0.4140\n",
      "Epoch [360/680], Loss: 0.4112\n",
      "Epoch [365/680], Loss: 0.4085\n",
      "Epoch [370/680], Loss: 0.4058\n",
      "Epoch [375/680], Loss: 0.4032\n",
      "Epoch [380/680], Loss: 0.4006\n",
      "Epoch [385/680], Loss: 0.3980\n",
      "Epoch [390/680], Loss: 0.3955\n",
      "Epoch [395/680], Loss: 0.3931\n",
      "Epoch [400/680], Loss: 0.3907\n",
      "Epoch [405/680], Loss: 0.3884\n",
      "Epoch [410/680], Loss: 0.3861\n",
      "Epoch [415/680], Loss: 0.3838\n",
      "Epoch [420/680], Loss: 0.3816\n",
      "Epoch [425/680], Loss: 0.3794\n",
      "Epoch [430/680], Loss: 0.3773\n",
      "Epoch [435/680], Loss: 0.3752\n",
      "Epoch [440/680], Loss: 0.3731\n",
      "Epoch [445/680], Loss: 0.3711\n",
      "Epoch [450/680], Loss: 0.3691\n",
      "Epoch [455/680], Loss: 0.3671\n",
      "Epoch [460/680], Loss: 0.3652\n",
      "Epoch [465/680], Loss: 0.3632\n",
      "Epoch [470/680], Loss: 0.3614\n",
      "Epoch [475/680], Loss: 0.3595\n",
      "Epoch [480/680], Loss: 0.3577\n",
      "Epoch [485/680], Loss: 0.3559\n",
      "Epoch [490/680], Loss: 0.3541\n",
      "Epoch [495/680], Loss: 0.3524\n",
      "Epoch [500/680], Loss: 0.3507\n",
      "Epoch [505/680], Loss: 0.3490\n",
      "Epoch [510/680], Loss: 0.3473\n",
      "Epoch [515/680], Loss: 0.3456\n",
      "Epoch [520/680], Loss: 0.3440\n",
      "Epoch [525/680], Loss: 0.3424\n",
      "Epoch [530/680], Loss: 0.3408\n",
      "Epoch [535/680], Loss: 0.3393\n",
      "Epoch [540/680], Loss: 0.3377\n",
      "Epoch [545/680], Loss: 0.3362\n",
      "Epoch [550/680], Loss: 0.3347\n",
      "Epoch [555/680], Loss: 0.3332\n",
      "Epoch [560/680], Loss: 0.3317\n",
      "Epoch [565/680], Loss: 0.3303\n",
      "Epoch [570/680], Loss: 0.3288\n",
      "Epoch [575/680], Loss: 0.3274\n",
      "Epoch [580/680], Loss: 0.3260\n",
      "Epoch [585/680], Loss: 0.3246\n",
      "Epoch [590/680], Loss: 0.3232\n",
      "Epoch [595/680], Loss: 0.3218\n",
      "Epoch [600/680], Loss: 0.3204\n",
      "Epoch [605/680], Loss: 0.3191\n",
      "Epoch [610/680], Loss: 0.3178\n",
      "Epoch [615/680], Loss: 0.3164\n",
      "Epoch [620/680], Loss: 0.3151\n",
      "Epoch [625/680], Loss: 0.3138\n",
      "Epoch [630/680], Loss: 0.3125\n",
      "Epoch [635/680], Loss: 0.3112\n",
      "Epoch [640/680], Loss: 0.3099\n",
      "Epoch [645/680], Loss: 0.3087\n",
      "Epoch [650/680], Loss: 0.3074\n",
      "Epoch [655/680], Loss: 0.3061\n",
      "Epoch [660/680], Loss: 0.3049\n",
      "Epoch [665/680], Loss: 0.3036\n",
      "Epoch [670/680], Loss: 0.3024\n",
      "Epoch [675/680], Loss: 0.3011\n",
      "Epoch [680/680], Loss: 0.2999\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2332\n",
      "Epoch [5/690], Loss: 1.2706\n",
      "Epoch [10/690], Loss: 1.0961\n",
      "Epoch [15/690], Loss: 1.0366\n",
      "Epoch [20/690], Loss: 1.0028\n",
      "Epoch [25/690], Loss: 0.9762\n",
      "Epoch [30/690], Loss: 0.9529\n",
      "Epoch [35/690], Loss: 0.9317\n",
      "Epoch [40/690], Loss: 0.9119\n",
      "Epoch [45/690], Loss: 0.8932\n",
      "Epoch [50/690], Loss: 0.8756\n",
      "Epoch [55/690], Loss: 0.8588\n",
      "Epoch [60/690], Loss: 0.8426\n",
      "Epoch [65/690], Loss: 0.8270\n",
      "Epoch [70/690], Loss: 0.8122\n",
      "Epoch [75/690], Loss: 0.7979\n",
      "Epoch [80/690], Loss: 0.7838\n",
      "Epoch [85/690], Loss: 0.7702\n",
      "Epoch [90/690], Loss: 0.7570\n",
      "Epoch [95/690], Loss: 0.7441\n",
      "Epoch [100/690], Loss: 0.7315\n",
      "Epoch [105/690], Loss: 0.7193\n",
      "Epoch [110/690], Loss: 0.7074\n",
      "Epoch [115/690], Loss: 0.6960\n",
      "Epoch [120/690], Loss: 0.6849\n",
      "Epoch [125/690], Loss: 0.6742\n",
      "Epoch [130/690], Loss: 0.6638\n",
      "Epoch [135/690], Loss: 0.6537\n",
      "Epoch [140/690], Loss: 0.6439\n",
      "Epoch [145/690], Loss: 0.6345\n",
      "Epoch [150/690], Loss: 0.6253\n",
      "Epoch [155/690], Loss: 0.6164\n",
      "Epoch [160/690], Loss: 0.6078\n",
      "Epoch [165/690], Loss: 0.5995\n",
      "Epoch [170/690], Loss: 0.5914\n",
      "Epoch [175/690], Loss: 0.5836\n",
      "Epoch [180/690], Loss: 0.5760\n",
      "Epoch [185/690], Loss: 0.5686\n",
      "Epoch [190/690], Loss: 0.5615\n",
      "Epoch [195/690], Loss: 0.5546\n",
      "Epoch [200/690], Loss: 0.5480\n",
      "Epoch [205/690], Loss: 0.5415\n",
      "Epoch [210/690], Loss: 0.5353\n",
      "Epoch [215/690], Loss: 0.5292\n",
      "Epoch [220/690], Loss: 0.5234\n",
      "Epoch [225/690], Loss: 0.5177\n",
      "Epoch [230/690], Loss: 0.5123\n",
      "Epoch [235/690], Loss: 0.5069\n",
      "Epoch [240/690], Loss: 0.5018\n",
      "Epoch [245/690], Loss: 0.4968\n",
      "Epoch [250/690], Loss: 0.4919\n",
      "Epoch [255/690], Loss: 0.4872\n",
      "Epoch [260/690], Loss: 0.4826\n",
      "Epoch [265/690], Loss: 0.4782\n",
      "Epoch [270/690], Loss: 0.4738\n",
      "Epoch [275/690], Loss: 0.4696\n",
      "Epoch [280/690], Loss: 0.4655\n",
      "Epoch [285/690], Loss: 0.4616\n",
      "Epoch [290/690], Loss: 0.4576\n",
      "Epoch [295/690], Loss: 0.4537\n",
      "Epoch [300/690], Loss: 0.4499\n",
      "Epoch [305/690], Loss: 0.4462\n",
      "Epoch [310/690], Loss: 0.4426\n",
      "Epoch [315/690], Loss: 0.4391\n",
      "Epoch [320/690], Loss: 0.4357\n",
      "Epoch [325/690], Loss: 0.4324\n",
      "Epoch [330/690], Loss: 0.4291\n",
      "Epoch [335/690], Loss: 0.4260\n",
      "Epoch [340/690], Loss: 0.4229\n",
      "Epoch [345/690], Loss: 0.4199\n",
      "Epoch [350/690], Loss: 0.4169\n",
      "Epoch [355/690], Loss: 0.4140\n",
      "Epoch [360/690], Loss: 0.4112\n",
      "Epoch [365/690], Loss: 0.4085\n",
      "Epoch [370/690], Loss: 0.4058\n",
      "Epoch [375/690], Loss: 0.4032\n",
      "Epoch [380/690], Loss: 0.4006\n",
      "Epoch [385/690], Loss: 0.3980\n",
      "Epoch [390/690], Loss: 0.3955\n",
      "Epoch [395/690], Loss: 0.3931\n",
      "Epoch [400/690], Loss: 0.3907\n",
      "Epoch [405/690], Loss: 0.3884\n",
      "Epoch [410/690], Loss: 0.3861\n",
      "Epoch [415/690], Loss: 0.3838\n",
      "Epoch [420/690], Loss: 0.3816\n",
      "Epoch [425/690], Loss: 0.3794\n",
      "Epoch [430/690], Loss: 0.3773\n",
      "Epoch [435/690], Loss: 0.3752\n",
      "Epoch [440/690], Loss: 0.3731\n",
      "Epoch [445/690], Loss: 0.3711\n",
      "Epoch [450/690], Loss: 0.3691\n",
      "Epoch [455/690], Loss: 0.3671\n",
      "Epoch [460/690], Loss: 0.3652\n",
      "Epoch [465/690], Loss: 0.3632\n",
      "Epoch [470/690], Loss: 0.3614\n",
      "Epoch [475/690], Loss: 0.3595\n",
      "Epoch [480/690], Loss: 0.3577\n",
      "Epoch [485/690], Loss: 0.3559\n",
      "Epoch [490/690], Loss: 0.3541\n",
      "Epoch [495/690], Loss: 0.3524\n",
      "Epoch [500/690], Loss: 0.3507\n",
      "Epoch [505/690], Loss: 0.3490\n",
      "Epoch [510/690], Loss: 0.3473\n",
      "Epoch [515/690], Loss: 0.3456\n",
      "Epoch [520/690], Loss: 0.3440\n",
      "Epoch [525/690], Loss: 0.3424\n",
      "Epoch [530/690], Loss: 0.3408\n",
      "Epoch [535/690], Loss: 0.3393\n",
      "Epoch [540/690], Loss: 0.3377\n",
      "Epoch [545/690], Loss: 0.3362\n",
      "Epoch [550/690], Loss: 0.3347\n",
      "Epoch [555/690], Loss: 0.3332\n",
      "Epoch [560/690], Loss: 0.3317\n",
      "Epoch [565/690], Loss: 0.3303\n",
      "Epoch [570/690], Loss: 0.3288\n",
      "Epoch [575/690], Loss: 0.3274\n",
      "Epoch [580/690], Loss: 0.3260\n",
      "Epoch [585/690], Loss: 0.3246\n",
      "Epoch [590/690], Loss: 0.3232\n",
      "Epoch [595/690], Loss: 0.3218\n",
      "Epoch [600/690], Loss: 0.3204\n",
      "Epoch [605/690], Loss: 0.3191\n",
      "Epoch [610/690], Loss: 0.3178\n",
      "Epoch [615/690], Loss: 0.3164\n",
      "Epoch [620/690], Loss: 0.3151\n",
      "Epoch [625/690], Loss: 0.3138\n",
      "Epoch [630/690], Loss: 0.3125\n",
      "Epoch [635/690], Loss: 0.3112\n",
      "Epoch [640/690], Loss: 0.3099\n",
      "Epoch [645/690], Loss: 0.3087\n",
      "Epoch [650/690], Loss: 0.3074\n",
      "Epoch [655/690], Loss: 0.3061\n",
      "Epoch [660/690], Loss: 0.3049\n",
      "Epoch [665/690], Loss: 0.3036\n",
      "Epoch [670/690], Loss: 0.3024\n",
      "Epoch [675/690], Loss: 0.3011\n",
      "Epoch [680/690], Loss: 0.2999\n",
      "Epoch [685/690], Loss: 0.2987\n",
      "Epoch [690/690], Loss: 0.2974\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2310\n",
      "Epoch [5/700], Loss: 1.2706\n",
      "Epoch [10/700], Loss: 1.0961\n",
      "Epoch [15/700], Loss: 1.0366\n",
      "Epoch [20/700], Loss: 1.0028\n",
      "Epoch [25/700], Loss: 0.9762\n",
      "Epoch [30/700], Loss: 0.9529\n",
      "Epoch [35/700], Loss: 0.9317\n",
      "Epoch [40/700], Loss: 0.9119\n",
      "Epoch [45/700], Loss: 0.8932\n",
      "Epoch [50/700], Loss: 0.8756\n",
      "Epoch [55/700], Loss: 0.8588\n",
      "Epoch [60/700], Loss: 0.8426\n",
      "Epoch [65/700], Loss: 0.8270\n",
      "Epoch [70/700], Loss: 0.8122\n",
      "Epoch [75/700], Loss: 0.7979\n",
      "Epoch [80/700], Loss: 0.7838\n",
      "Epoch [85/700], Loss: 0.7702\n",
      "Epoch [90/700], Loss: 0.7570\n",
      "Epoch [95/700], Loss: 0.7441\n",
      "Epoch [100/700], Loss: 0.7315\n",
      "Epoch [105/700], Loss: 0.7193\n",
      "Epoch [110/700], Loss: 0.7074\n",
      "Epoch [115/700], Loss: 0.6960\n",
      "Epoch [120/700], Loss: 0.6849\n",
      "Epoch [125/700], Loss: 0.6742\n",
      "Epoch [130/700], Loss: 0.6638\n",
      "Epoch [135/700], Loss: 0.6537\n",
      "Epoch [140/700], Loss: 0.6439\n",
      "Epoch [145/700], Loss: 0.6345\n",
      "Epoch [150/700], Loss: 0.6253\n",
      "Epoch [155/700], Loss: 0.6164\n",
      "Epoch [160/700], Loss: 0.6078\n",
      "Epoch [165/700], Loss: 0.5995\n",
      "Epoch [170/700], Loss: 0.5914\n",
      "Epoch [175/700], Loss: 0.5836\n",
      "Epoch [180/700], Loss: 0.5760\n",
      "Epoch [185/700], Loss: 0.5686\n",
      "Epoch [190/700], Loss: 0.5615\n",
      "Epoch [195/700], Loss: 0.5546\n",
      "Epoch [200/700], Loss: 0.5480\n",
      "Epoch [205/700], Loss: 0.5415\n",
      "Epoch [210/700], Loss: 0.5353\n",
      "Epoch [215/700], Loss: 0.5292\n",
      "Epoch [220/700], Loss: 0.5234\n",
      "Epoch [225/700], Loss: 0.5177\n",
      "Epoch [230/700], Loss: 0.5123\n",
      "Epoch [235/700], Loss: 0.5069\n",
      "Epoch [240/700], Loss: 0.5018\n",
      "Epoch [245/700], Loss: 0.4968\n",
      "Epoch [250/700], Loss: 0.4919\n",
      "Epoch [255/700], Loss: 0.4872\n",
      "Epoch [260/700], Loss: 0.4826\n",
      "Epoch [265/700], Loss: 0.4782\n",
      "Epoch [270/700], Loss: 0.4738\n",
      "Epoch [275/700], Loss: 0.4696\n",
      "Epoch [280/700], Loss: 0.4655\n",
      "Epoch [285/700], Loss: 0.4616\n",
      "Epoch [290/700], Loss: 0.4576\n",
      "Epoch [295/700], Loss: 0.4537\n",
      "Epoch [300/700], Loss: 0.4499\n",
      "Epoch [305/700], Loss: 0.4462\n",
      "Epoch [310/700], Loss: 0.4426\n",
      "Epoch [315/700], Loss: 0.4391\n",
      "Epoch [320/700], Loss: 0.4357\n",
      "Epoch [325/700], Loss: 0.4324\n",
      "Epoch [330/700], Loss: 0.4291\n",
      "Epoch [335/700], Loss: 0.4260\n",
      "Epoch [340/700], Loss: 0.4229\n",
      "Epoch [345/700], Loss: 0.4199\n",
      "Epoch [350/700], Loss: 0.4169\n",
      "Epoch [355/700], Loss: 0.4140\n",
      "Epoch [360/700], Loss: 0.4112\n",
      "Epoch [365/700], Loss: 0.4085\n",
      "Epoch [370/700], Loss: 0.4058\n",
      "Epoch [375/700], Loss: 0.4032\n",
      "Epoch [380/700], Loss: 0.4006\n",
      "Epoch [385/700], Loss: 0.3980\n",
      "Epoch [390/700], Loss: 0.3955\n",
      "Epoch [395/700], Loss: 0.3931\n",
      "Epoch [400/700], Loss: 0.3907\n",
      "Epoch [405/700], Loss: 0.3884\n",
      "Epoch [410/700], Loss: 0.3861\n",
      "Epoch [415/700], Loss: 0.3838\n",
      "Epoch [420/700], Loss: 0.3816\n",
      "Epoch [425/700], Loss: 0.3794\n",
      "Epoch [430/700], Loss: 0.3773\n",
      "Epoch [435/700], Loss: 0.3752\n",
      "Epoch [440/700], Loss: 0.3731\n",
      "Epoch [445/700], Loss: 0.3711\n",
      "Epoch [450/700], Loss: 0.3691\n",
      "Epoch [455/700], Loss: 0.3671\n",
      "Epoch [460/700], Loss: 0.3652\n",
      "Epoch [465/700], Loss: 0.3632\n",
      "Epoch [470/700], Loss: 0.3614\n",
      "Epoch [475/700], Loss: 0.3595\n",
      "Epoch [480/700], Loss: 0.3577\n",
      "Epoch [485/700], Loss: 0.3559\n",
      "Epoch [490/700], Loss: 0.3541\n",
      "Epoch [495/700], Loss: 0.3524\n",
      "Epoch [500/700], Loss: 0.3507\n",
      "Epoch [505/700], Loss: 0.3490\n",
      "Epoch [510/700], Loss: 0.3473\n",
      "Epoch [515/700], Loss: 0.3456\n",
      "Epoch [520/700], Loss: 0.3440\n",
      "Epoch [525/700], Loss: 0.3424\n",
      "Epoch [530/700], Loss: 0.3408\n",
      "Epoch [535/700], Loss: 0.3393\n",
      "Epoch [540/700], Loss: 0.3377\n",
      "Epoch [545/700], Loss: 0.3362\n",
      "Epoch [550/700], Loss: 0.3347\n",
      "Epoch [555/700], Loss: 0.3332\n",
      "Epoch [560/700], Loss: 0.3317\n",
      "Epoch [565/700], Loss: 0.3303\n",
      "Epoch [570/700], Loss: 0.3288\n",
      "Epoch [575/700], Loss: 0.3274\n",
      "Epoch [580/700], Loss: 0.3260\n",
      "Epoch [585/700], Loss: 0.3246\n",
      "Epoch [590/700], Loss: 0.3232\n",
      "Epoch [595/700], Loss: 0.3218\n",
      "Epoch [600/700], Loss: 0.3204\n",
      "Epoch [605/700], Loss: 0.3191\n",
      "Epoch [610/700], Loss: 0.3178\n",
      "Epoch [615/700], Loss: 0.3164\n",
      "Epoch [620/700], Loss: 0.3151\n",
      "Epoch [625/700], Loss: 0.3138\n",
      "Epoch [630/700], Loss: 0.3125\n",
      "Epoch [635/700], Loss: 0.3112\n",
      "Epoch [640/700], Loss: 0.3099\n",
      "Epoch [645/700], Loss: 0.3087\n",
      "Epoch [650/700], Loss: 0.3074\n",
      "Epoch [655/700], Loss: 0.3061\n",
      "Epoch [660/700], Loss: 0.3049\n",
      "Epoch [665/700], Loss: 0.3036\n",
      "Epoch [670/700], Loss: 0.3024\n",
      "Epoch [675/700], Loss: 0.3011\n",
      "Epoch [680/700], Loss: 0.2999\n",
      "Epoch [685/700], Loss: 0.2987\n",
      "Epoch [690/700], Loss: 0.2974\n",
      "Epoch [695/700], Loss: 0.2962\n",
      "Epoch [700/700], Loss: 0.2950\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2289\n",
      "Epoch [5/710], Loss: 1.2706\n",
      "Epoch [10/710], Loss: 1.0961\n",
      "Epoch [15/710], Loss: 1.0366\n",
      "Epoch [20/710], Loss: 1.0028\n",
      "Epoch [25/710], Loss: 0.9762\n",
      "Epoch [30/710], Loss: 0.9529\n",
      "Epoch [35/710], Loss: 0.9317\n",
      "Epoch [40/710], Loss: 0.9119\n",
      "Epoch [45/710], Loss: 0.8932\n",
      "Epoch [50/710], Loss: 0.8756\n",
      "Epoch [55/710], Loss: 0.8588\n",
      "Epoch [60/710], Loss: 0.8426\n",
      "Epoch [65/710], Loss: 0.8270\n",
      "Epoch [70/710], Loss: 0.8122\n",
      "Epoch [75/710], Loss: 0.7979\n",
      "Epoch [80/710], Loss: 0.7838\n",
      "Epoch [85/710], Loss: 0.7702\n",
      "Epoch [90/710], Loss: 0.7570\n",
      "Epoch [95/710], Loss: 0.7441\n",
      "Epoch [100/710], Loss: 0.7315\n",
      "Epoch [105/710], Loss: 0.7193\n",
      "Epoch [110/710], Loss: 0.7074\n",
      "Epoch [115/710], Loss: 0.6960\n",
      "Epoch [120/710], Loss: 0.6849\n",
      "Epoch [125/710], Loss: 0.6742\n",
      "Epoch [130/710], Loss: 0.6638\n",
      "Epoch [135/710], Loss: 0.6537\n",
      "Epoch [140/710], Loss: 0.6439\n",
      "Epoch [145/710], Loss: 0.6345\n",
      "Epoch [150/710], Loss: 0.6253\n",
      "Epoch [155/710], Loss: 0.6164\n",
      "Epoch [160/710], Loss: 0.6078\n",
      "Epoch [165/710], Loss: 0.5995\n",
      "Epoch [170/710], Loss: 0.5914\n",
      "Epoch [175/710], Loss: 0.5836\n",
      "Epoch [180/710], Loss: 0.5760\n",
      "Epoch [185/710], Loss: 0.5686\n",
      "Epoch [190/710], Loss: 0.5615\n",
      "Epoch [195/710], Loss: 0.5546\n",
      "Epoch [200/710], Loss: 0.5480\n",
      "Epoch [205/710], Loss: 0.5415\n",
      "Epoch [210/710], Loss: 0.5353\n",
      "Epoch [215/710], Loss: 0.5292\n",
      "Epoch [220/710], Loss: 0.5234\n",
      "Epoch [225/710], Loss: 0.5177\n",
      "Epoch [230/710], Loss: 0.5123\n",
      "Epoch [235/710], Loss: 0.5069\n",
      "Epoch [240/710], Loss: 0.5018\n",
      "Epoch [245/710], Loss: 0.4968\n",
      "Epoch [250/710], Loss: 0.4919\n",
      "Epoch [255/710], Loss: 0.4872\n",
      "Epoch [260/710], Loss: 0.4826\n",
      "Epoch [265/710], Loss: 0.4782\n",
      "Epoch [270/710], Loss: 0.4738\n",
      "Epoch [275/710], Loss: 0.4696\n",
      "Epoch [280/710], Loss: 0.4655\n",
      "Epoch [285/710], Loss: 0.4616\n",
      "Epoch [290/710], Loss: 0.4576\n",
      "Epoch [295/710], Loss: 0.4537\n",
      "Epoch [300/710], Loss: 0.4499\n",
      "Epoch [305/710], Loss: 0.4462\n",
      "Epoch [310/710], Loss: 0.4426\n",
      "Epoch [315/710], Loss: 0.4391\n",
      "Epoch [320/710], Loss: 0.4357\n",
      "Epoch [325/710], Loss: 0.4324\n",
      "Epoch [330/710], Loss: 0.4291\n",
      "Epoch [335/710], Loss: 0.4260\n",
      "Epoch [340/710], Loss: 0.4229\n",
      "Epoch [345/710], Loss: 0.4199\n",
      "Epoch [350/710], Loss: 0.4169\n",
      "Epoch [355/710], Loss: 0.4140\n",
      "Epoch [360/710], Loss: 0.4112\n",
      "Epoch [365/710], Loss: 0.4085\n",
      "Epoch [370/710], Loss: 0.4058\n",
      "Epoch [375/710], Loss: 0.4032\n",
      "Epoch [380/710], Loss: 0.4006\n",
      "Epoch [385/710], Loss: 0.3980\n",
      "Epoch [390/710], Loss: 0.3955\n",
      "Epoch [395/710], Loss: 0.3931\n",
      "Epoch [400/710], Loss: 0.3907\n",
      "Epoch [405/710], Loss: 0.3884\n",
      "Epoch [410/710], Loss: 0.3861\n",
      "Epoch [415/710], Loss: 0.3838\n",
      "Epoch [420/710], Loss: 0.3816\n",
      "Epoch [425/710], Loss: 0.3794\n",
      "Epoch [430/710], Loss: 0.3773\n",
      "Epoch [435/710], Loss: 0.3752\n",
      "Epoch [440/710], Loss: 0.3731\n",
      "Epoch [445/710], Loss: 0.3711\n",
      "Epoch [450/710], Loss: 0.3691\n",
      "Epoch [455/710], Loss: 0.3671\n",
      "Epoch [460/710], Loss: 0.3652\n",
      "Epoch [465/710], Loss: 0.3632\n",
      "Epoch [470/710], Loss: 0.3614\n",
      "Epoch [475/710], Loss: 0.3595\n",
      "Epoch [480/710], Loss: 0.3577\n",
      "Epoch [485/710], Loss: 0.3559\n",
      "Epoch [490/710], Loss: 0.3541\n",
      "Epoch [495/710], Loss: 0.3524\n",
      "Epoch [500/710], Loss: 0.3507\n",
      "Epoch [505/710], Loss: 0.3490\n",
      "Epoch [510/710], Loss: 0.3473\n",
      "Epoch [515/710], Loss: 0.3456\n",
      "Epoch [520/710], Loss: 0.3440\n",
      "Epoch [525/710], Loss: 0.3424\n",
      "Epoch [530/710], Loss: 0.3408\n",
      "Epoch [535/710], Loss: 0.3393\n",
      "Epoch [540/710], Loss: 0.3377\n",
      "Epoch [545/710], Loss: 0.3362\n",
      "Epoch [550/710], Loss: 0.3347\n",
      "Epoch [555/710], Loss: 0.3332\n",
      "Epoch [560/710], Loss: 0.3317\n",
      "Epoch [565/710], Loss: 0.3303\n",
      "Epoch [570/710], Loss: 0.3288\n",
      "Epoch [575/710], Loss: 0.3274\n",
      "Epoch [580/710], Loss: 0.3260\n",
      "Epoch [585/710], Loss: 0.3246\n",
      "Epoch [590/710], Loss: 0.3232\n",
      "Epoch [595/710], Loss: 0.3218\n",
      "Epoch [600/710], Loss: 0.3204\n",
      "Epoch [605/710], Loss: 0.3191\n",
      "Epoch [610/710], Loss: 0.3178\n",
      "Epoch [615/710], Loss: 0.3164\n",
      "Epoch [620/710], Loss: 0.3151\n",
      "Epoch [625/710], Loss: 0.3138\n",
      "Epoch [630/710], Loss: 0.3125\n",
      "Epoch [635/710], Loss: 0.3112\n",
      "Epoch [640/710], Loss: 0.3099\n",
      "Epoch [645/710], Loss: 0.3087\n",
      "Epoch [650/710], Loss: 0.3074\n",
      "Epoch [655/710], Loss: 0.3061\n",
      "Epoch [660/710], Loss: 0.3049\n",
      "Epoch [665/710], Loss: 0.3036\n",
      "Epoch [670/710], Loss: 0.3024\n",
      "Epoch [675/710], Loss: 0.3011\n",
      "Epoch [680/710], Loss: 0.2999\n",
      "Epoch [685/710], Loss: 0.2987\n",
      "Epoch [690/710], Loss: 0.2974\n",
      "Epoch [695/710], Loss: 0.2962\n",
      "Epoch [700/710], Loss: 0.2950\n",
      "Epoch [705/710], Loss: 0.2938\n",
      "Epoch [710/710], Loss: 0.2926\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2268\n",
      "Epoch [5/720], Loss: 1.2706\n",
      "Epoch [10/720], Loss: 1.0961\n",
      "Epoch [15/720], Loss: 1.0366\n",
      "Epoch [20/720], Loss: 1.0028\n",
      "Epoch [25/720], Loss: 0.9762\n",
      "Epoch [30/720], Loss: 0.9529\n",
      "Epoch [35/720], Loss: 0.9317\n",
      "Epoch [40/720], Loss: 0.9119\n",
      "Epoch [45/720], Loss: 0.8932\n",
      "Epoch [50/720], Loss: 0.8756\n",
      "Epoch [55/720], Loss: 0.8588\n",
      "Epoch [60/720], Loss: 0.8426\n",
      "Epoch [65/720], Loss: 0.8270\n",
      "Epoch [70/720], Loss: 0.8122\n",
      "Epoch [75/720], Loss: 0.7979\n",
      "Epoch [80/720], Loss: 0.7838\n",
      "Epoch [85/720], Loss: 0.7702\n",
      "Epoch [90/720], Loss: 0.7570\n",
      "Epoch [95/720], Loss: 0.7441\n",
      "Epoch [100/720], Loss: 0.7315\n",
      "Epoch [105/720], Loss: 0.7193\n",
      "Epoch [110/720], Loss: 0.7074\n",
      "Epoch [115/720], Loss: 0.6960\n",
      "Epoch [120/720], Loss: 0.6849\n",
      "Epoch [125/720], Loss: 0.6742\n",
      "Epoch [130/720], Loss: 0.6638\n",
      "Epoch [135/720], Loss: 0.6537\n",
      "Epoch [140/720], Loss: 0.6439\n",
      "Epoch [145/720], Loss: 0.6345\n",
      "Epoch [150/720], Loss: 0.6253\n",
      "Epoch [155/720], Loss: 0.6164\n",
      "Epoch [160/720], Loss: 0.6078\n",
      "Epoch [165/720], Loss: 0.5995\n",
      "Epoch [170/720], Loss: 0.5914\n",
      "Epoch [175/720], Loss: 0.5836\n",
      "Epoch [180/720], Loss: 0.5760\n",
      "Epoch [185/720], Loss: 0.5686\n",
      "Epoch [190/720], Loss: 0.5615\n",
      "Epoch [195/720], Loss: 0.5546\n",
      "Epoch [200/720], Loss: 0.5480\n",
      "Epoch [205/720], Loss: 0.5415\n",
      "Epoch [210/720], Loss: 0.5353\n",
      "Epoch [215/720], Loss: 0.5292\n",
      "Epoch [220/720], Loss: 0.5234\n",
      "Epoch [225/720], Loss: 0.5177\n",
      "Epoch [230/720], Loss: 0.5123\n",
      "Epoch [235/720], Loss: 0.5069\n",
      "Epoch [240/720], Loss: 0.5018\n",
      "Epoch [245/720], Loss: 0.4968\n",
      "Epoch [250/720], Loss: 0.4919\n",
      "Epoch [255/720], Loss: 0.4872\n",
      "Epoch [260/720], Loss: 0.4826\n",
      "Epoch [265/720], Loss: 0.4782\n",
      "Epoch [270/720], Loss: 0.4738\n",
      "Epoch [275/720], Loss: 0.4696\n",
      "Epoch [280/720], Loss: 0.4655\n",
      "Epoch [285/720], Loss: 0.4616\n",
      "Epoch [290/720], Loss: 0.4576\n",
      "Epoch [295/720], Loss: 0.4537\n",
      "Epoch [300/720], Loss: 0.4499\n",
      "Epoch [305/720], Loss: 0.4462\n",
      "Epoch [310/720], Loss: 0.4426\n",
      "Epoch [315/720], Loss: 0.4391\n",
      "Epoch [320/720], Loss: 0.4357\n",
      "Epoch [325/720], Loss: 0.4324\n",
      "Epoch [330/720], Loss: 0.4291\n",
      "Epoch [335/720], Loss: 0.4260\n",
      "Epoch [340/720], Loss: 0.4229\n",
      "Epoch [345/720], Loss: 0.4199\n",
      "Epoch [350/720], Loss: 0.4169\n",
      "Epoch [355/720], Loss: 0.4140\n",
      "Epoch [360/720], Loss: 0.4112\n",
      "Epoch [365/720], Loss: 0.4085\n",
      "Epoch [370/720], Loss: 0.4058\n",
      "Epoch [375/720], Loss: 0.4032\n",
      "Epoch [380/720], Loss: 0.4006\n",
      "Epoch [385/720], Loss: 0.3980\n",
      "Epoch [390/720], Loss: 0.3955\n",
      "Epoch [395/720], Loss: 0.3931\n",
      "Epoch [400/720], Loss: 0.3907\n",
      "Epoch [405/720], Loss: 0.3884\n",
      "Epoch [410/720], Loss: 0.3861\n",
      "Epoch [415/720], Loss: 0.3838\n",
      "Epoch [420/720], Loss: 0.3816\n",
      "Epoch [425/720], Loss: 0.3794\n",
      "Epoch [430/720], Loss: 0.3773\n",
      "Epoch [435/720], Loss: 0.3752\n",
      "Epoch [440/720], Loss: 0.3731\n",
      "Epoch [445/720], Loss: 0.3711\n",
      "Epoch [450/720], Loss: 0.3691\n",
      "Epoch [455/720], Loss: 0.3671\n",
      "Epoch [460/720], Loss: 0.3652\n",
      "Epoch [465/720], Loss: 0.3632\n",
      "Epoch [470/720], Loss: 0.3614\n",
      "Epoch [475/720], Loss: 0.3595\n",
      "Epoch [480/720], Loss: 0.3577\n",
      "Epoch [485/720], Loss: 0.3559\n",
      "Epoch [490/720], Loss: 0.3541\n",
      "Epoch [495/720], Loss: 0.3524\n",
      "Epoch [500/720], Loss: 0.3507\n",
      "Epoch [505/720], Loss: 0.3490\n",
      "Epoch [510/720], Loss: 0.3473\n",
      "Epoch [515/720], Loss: 0.3456\n",
      "Epoch [520/720], Loss: 0.3440\n",
      "Epoch [525/720], Loss: 0.3424\n",
      "Epoch [530/720], Loss: 0.3408\n",
      "Epoch [535/720], Loss: 0.3393\n",
      "Epoch [540/720], Loss: 0.3377\n",
      "Epoch [545/720], Loss: 0.3362\n",
      "Epoch [550/720], Loss: 0.3347\n",
      "Epoch [555/720], Loss: 0.3332\n",
      "Epoch [560/720], Loss: 0.3317\n",
      "Epoch [565/720], Loss: 0.3303\n",
      "Epoch [570/720], Loss: 0.3288\n",
      "Epoch [575/720], Loss: 0.3274\n",
      "Epoch [580/720], Loss: 0.3260\n",
      "Epoch [585/720], Loss: 0.3246\n",
      "Epoch [590/720], Loss: 0.3232\n",
      "Epoch [595/720], Loss: 0.3218\n",
      "Epoch [600/720], Loss: 0.3204\n",
      "Epoch [605/720], Loss: 0.3191\n",
      "Epoch [610/720], Loss: 0.3178\n",
      "Epoch [615/720], Loss: 0.3164\n",
      "Epoch [620/720], Loss: 0.3151\n",
      "Epoch [625/720], Loss: 0.3138\n",
      "Epoch [630/720], Loss: 0.3125\n",
      "Epoch [635/720], Loss: 0.3112\n",
      "Epoch [640/720], Loss: 0.3099\n",
      "Epoch [645/720], Loss: 0.3087\n",
      "Epoch [650/720], Loss: 0.3074\n",
      "Epoch [655/720], Loss: 0.3061\n",
      "Epoch [660/720], Loss: 0.3049\n",
      "Epoch [665/720], Loss: 0.3036\n",
      "Epoch [670/720], Loss: 0.3024\n",
      "Epoch [675/720], Loss: 0.3011\n",
      "Epoch [680/720], Loss: 0.2999\n",
      "Epoch [685/720], Loss: 0.2987\n",
      "Epoch [690/720], Loss: 0.2974\n",
      "Epoch [695/720], Loss: 0.2962\n",
      "Epoch [700/720], Loss: 0.2950\n",
      "Epoch [705/720], Loss: 0.2938\n",
      "Epoch [710/720], Loss: 0.2926\n",
      "Epoch [715/720], Loss: 0.2914\n",
      "Epoch [720/720], Loss: 0.2903\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2248\n",
      "Epoch [5/730], Loss: 1.2706\n",
      "Epoch [10/730], Loss: 1.0961\n",
      "Epoch [15/730], Loss: 1.0366\n",
      "Epoch [20/730], Loss: 1.0028\n",
      "Epoch [25/730], Loss: 0.9762\n",
      "Epoch [30/730], Loss: 0.9529\n",
      "Epoch [35/730], Loss: 0.9317\n",
      "Epoch [40/730], Loss: 0.9119\n",
      "Epoch [45/730], Loss: 0.8932\n",
      "Epoch [50/730], Loss: 0.8756\n",
      "Epoch [55/730], Loss: 0.8588\n",
      "Epoch [60/730], Loss: 0.8426\n",
      "Epoch [65/730], Loss: 0.8270\n",
      "Epoch [70/730], Loss: 0.8122\n",
      "Epoch [75/730], Loss: 0.7979\n",
      "Epoch [80/730], Loss: 0.7838\n",
      "Epoch [85/730], Loss: 0.7702\n",
      "Epoch [90/730], Loss: 0.7570\n",
      "Epoch [95/730], Loss: 0.7441\n",
      "Epoch [100/730], Loss: 0.7315\n",
      "Epoch [105/730], Loss: 0.7193\n",
      "Epoch [110/730], Loss: 0.7074\n",
      "Epoch [115/730], Loss: 0.6960\n",
      "Epoch [120/730], Loss: 0.6849\n",
      "Epoch [125/730], Loss: 0.6742\n",
      "Epoch [130/730], Loss: 0.6638\n",
      "Epoch [135/730], Loss: 0.6537\n",
      "Epoch [140/730], Loss: 0.6439\n",
      "Epoch [145/730], Loss: 0.6345\n",
      "Epoch [150/730], Loss: 0.6253\n",
      "Epoch [155/730], Loss: 0.6164\n",
      "Epoch [160/730], Loss: 0.6078\n",
      "Epoch [165/730], Loss: 0.5995\n",
      "Epoch [170/730], Loss: 0.5914\n",
      "Epoch [175/730], Loss: 0.5836\n",
      "Epoch [180/730], Loss: 0.5760\n",
      "Epoch [185/730], Loss: 0.5686\n",
      "Epoch [190/730], Loss: 0.5615\n",
      "Epoch [195/730], Loss: 0.5546\n",
      "Epoch [200/730], Loss: 0.5480\n",
      "Epoch [205/730], Loss: 0.5415\n",
      "Epoch [210/730], Loss: 0.5353\n",
      "Epoch [215/730], Loss: 0.5292\n",
      "Epoch [220/730], Loss: 0.5234\n",
      "Epoch [225/730], Loss: 0.5177\n",
      "Epoch [230/730], Loss: 0.5123\n",
      "Epoch [235/730], Loss: 0.5069\n",
      "Epoch [240/730], Loss: 0.5018\n",
      "Epoch [245/730], Loss: 0.4968\n",
      "Epoch [250/730], Loss: 0.4919\n",
      "Epoch [255/730], Loss: 0.4872\n",
      "Epoch [260/730], Loss: 0.4826\n",
      "Epoch [265/730], Loss: 0.4782\n",
      "Epoch [270/730], Loss: 0.4738\n",
      "Epoch [275/730], Loss: 0.4696\n",
      "Epoch [280/730], Loss: 0.4655\n",
      "Epoch [285/730], Loss: 0.4616\n",
      "Epoch [290/730], Loss: 0.4576\n",
      "Epoch [295/730], Loss: 0.4537\n",
      "Epoch [300/730], Loss: 0.4499\n",
      "Epoch [305/730], Loss: 0.4462\n",
      "Epoch [310/730], Loss: 0.4426\n",
      "Epoch [315/730], Loss: 0.4391\n",
      "Epoch [320/730], Loss: 0.4357\n",
      "Epoch [325/730], Loss: 0.4324\n",
      "Epoch [330/730], Loss: 0.4291\n",
      "Epoch [335/730], Loss: 0.4260\n",
      "Epoch [340/730], Loss: 0.4229\n",
      "Epoch [345/730], Loss: 0.4199\n",
      "Epoch [350/730], Loss: 0.4169\n",
      "Epoch [355/730], Loss: 0.4140\n",
      "Epoch [360/730], Loss: 0.4112\n",
      "Epoch [365/730], Loss: 0.4085\n",
      "Epoch [370/730], Loss: 0.4058\n",
      "Epoch [375/730], Loss: 0.4032\n",
      "Epoch [380/730], Loss: 0.4006\n",
      "Epoch [385/730], Loss: 0.3980\n",
      "Epoch [390/730], Loss: 0.3955\n",
      "Epoch [395/730], Loss: 0.3931\n",
      "Epoch [400/730], Loss: 0.3907\n",
      "Epoch [405/730], Loss: 0.3884\n",
      "Epoch [410/730], Loss: 0.3861\n",
      "Epoch [415/730], Loss: 0.3838\n",
      "Epoch [420/730], Loss: 0.3816\n",
      "Epoch [425/730], Loss: 0.3794\n",
      "Epoch [430/730], Loss: 0.3773\n",
      "Epoch [435/730], Loss: 0.3752\n",
      "Epoch [440/730], Loss: 0.3731\n",
      "Epoch [445/730], Loss: 0.3711\n",
      "Epoch [450/730], Loss: 0.3691\n",
      "Epoch [455/730], Loss: 0.3671\n",
      "Epoch [460/730], Loss: 0.3652\n",
      "Epoch [465/730], Loss: 0.3632\n",
      "Epoch [470/730], Loss: 0.3614\n",
      "Epoch [475/730], Loss: 0.3595\n",
      "Epoch [480/730], Loss: 0.3577\n",
      "Epoch [485/730], Loss: 0.3559\n",
      "Epoch [490/730], Loss: 0.3541\n",
      "Epoch [495/730], Loss: 0.3524\n",
      "Epoch [500/730], Loss: 0.3507\n",
      "Epoch [505/730], Loss: 0.3490\n",
      "Epoch [510/730], Loss: 0.3473\n",
      "Epoch [515/730], Loss: 0.3456\n",
      "Epoch [520/730], Loss: 0.3440\n",
      "Epoch [525/730], Loss: 0.3424\n",
      "Epoch [530/730], Loss: 0.3408\n",
      "Epoch [535/730], Loss: 0.3393\n",
      "Epoch [540/730], Loss: 0.3377\n",
      "Epoch [545/730], Loss: 0.3362\n",
      "Epoch [550/730], Loss: 0.3347\n",
      "Epoch [555/730], Loss: 0.3332\n",
      "Epoch [560/730], Loss: 0.3317\n",
      "Epoch [565/730], Loss: 0.3303\n",
      "Epoch [570/730], Loss: 0.3288\n",
      "Epoch [575/730], Loss: 0.3274\n",
      "Epoch [580/730], Loss: 0.3260\n",
      "Epoch [585/730], Loss: 0.3246\n",
      "Epoch [590/730], Loss: 0.3232\n",
      "Epoch [595/730], Loss: 0.3218\n",
      "Epoch [600/730], Loss: 0.3204\n",
      "Epoch [605/730], Loss: 0.3191\n",
      "Epoch [610/730], Loss: 0.3178\n",
      "Epoch [615/730], Loss: 0.3164\n",
      "Epoch [620/730], Loss: 0.3151\n",
      "Epoch [625/730], Loss: 0.3138\n",
      "Epoch [630/730], Loss: 0.3125\n",
      "Epoch [635/730], Loss: 0.3112\n",
      "Epoch [640/730], Loss: 0.3099\n",
      "Epoch [645/730], Loss: 0.3087\n",
      "Epoch [650/730], Loss: 0.3074\n",
      "Epoch [655/730], Loss: 0.3061\n",
      "Epoch [660/730], Loss: 0.3049\n",
      "Epoch [665/730], Loss: 0.3036\n",
      "Epoch [670/730], Loss: 0.3024\n",
      "Epoch [675/730], Loss: 0.3011\n",
      "Epoch [680/730], Loss: 0.2999\n",
      "Epoch [685/730], Loss: 0.2987\n",
      "Epoch [690/730], Loss: 0.2974\n",
      "Epoch [695/730], Loss: 0.2962\n",
      "Epoch [700/730], Loss: 0.2950\n",
      "Epoch [705/730], Loss: 0.2938\n",
      "Epoch [710/730], Loss: 0.2926\n",
      "Epoch [715/730], Loss: 0.2914\n",
      "Epoch [720/730], Loss: 0.2903\n",
      "Epoch [725/730], Loss: 0.2891\n",
      "Epoch [730/730], Loss: 0.2879\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2227\n",
      "Epoch [5/740], Loss: 1.2706\n",
      "Epoch [10/740], Loss: 1.0961\n",
      "Epoch [15/740], Loss: 1.0366\n",
      "Epoch [20/740], Loss: 1.0028\n",
      "Epoch [25/740], Loss: 0.9762\n",
      "Epoch [30/740], Loss: 0.9529\n",
      "Epoch [35/740], Loss: 0.9317\n",
      "Epoch [40/740], Loss: 0.9119\n",
      "Epoch [45/740], Loss: 0.8932\n",
      "Epoch [50/740], Loss: 0.8756\n",
      "Epoch [55/740], Loss: 0.8588\n",
      "Epoch [60/740], Loss: 0.8426\n",
      "Epoch [65/740], Loss: 0.8270\n",
      "Epoch [70/740], Loss: 0.8122\n",
      "Epoch [75/740], Loss: 0.7979\n",
      "Epoch [80/740], Loss: 0.7838\n",
      "Epoch [85/740], Loss: 0.7702\n",
      "Epoch [90/740], Loss: 0.7570\n",
      "Epoch [95/740], Loss: 0.7441\n",
      "Epoch [100/740], Loss: 0.7315\n",
      "Epoch [105/740], Loss: 0.7193\n",
      "Epoch [110/740], Loss: 0.7074\n",
      "Epoch [115/740], Loss: 0.6960\n",
      "Epoch [120/740], Loss: 0.6849\n",
      "Epoch [125/740], Loss: 0.6742\n",
      "Epoch [130/740], Loss: 0.6638\n",
      "Epoch [135/740], Loss: 0.6537\n",
      "Epoch [140/740], Loss: 0.6439\n",
      "Epoch [145/740], Loss: 0.6345\n",
      "Epoch [150/740], Loss: 0.6253\n",
      "Epoch [155/740], Loss: 0.6164\n",
      "Epoch [160/740], Loss: 0.6078\n",
      "Epoch [165/740], Loss: 0.5995\n",
      "Epoch [170/740], Loss: 0.5914\n",
      "Epoch [175/740], Loss: 0.5836\n",
      "Epoch [180/740], Loss: 0.5760\n",
      "Epoch [185/740], Loss: 0.5686\n",
      "Epoch [190/740], Loss: 0.5615\n",
      "Epoch [195/740], Loss: 0.5546\n",
      "Epoch [200/740], Loss: 0.5480\n",
      "Epoch [205/740], Loss: 0.5415\n",
      "Epoch [210/740], Loss: 0.5353\n",
      "Epoch [215/740], Loss: 0.5292\n",
      "Epoch [220/740], Loss: 0.5234\n",
      "Epoch [225/740], Loss: 0.5177\n",
      "Epoch [230/740], Loss: 0.5123\n",
      "Epoch [235/740], Loss: 0.5069\n",
      "Epoch [240/740], Loss: 0.5018\n",
      "Epoch [245/740], Loss: 0.4968\n",
      "Epoch [250/740], Loss: 0.4919\n",
      "Epoch [255/740], Loss: 0.4872\n",
      "Epoch [260/740], Loss: 0.4826\n",
      "Epoch [265/740], Loss: 0.4782\n",
      "Epoch [270/740], Loss: 0.4738\n",
      "Epoch [275/740], Loss: 0.4696\n",
      "Epoch [280/740], Loss: 0.4655\n",
      "Epoch [285/740], Loss: 0.4616\n",
      "Epoch [290/740], Loss: 0.4576\n",
      "Epoch [295/740], Loss: 0.4537\n",
      "Epoch [300/740], Loss: 0.4499\n",
      "Epoch [305/740], Loss: 0.4462\n",
      "Epoch [310/740], Loss: 0.4426\n",
      "Epoch [315/740], Loss: 0.4391\n",
      "Epoch [320/740], Loss: 0.4357\n",
      "Epoch [325/740], Loss: 0.4324\n",
      "Epoch [330/740], Loss: 0.4291\n",
      "Epoch [335/740], Loss: 0.4260\n",
      "Epoch [340/740], Loss: 0.4229\n",
      "Epoch [345/740], Loss: 0.4199\n",
      "Epoch [350/740], Loss: 0.4169\n",
      "Epoch [355/740], Loss: 0.4140\n",
      "Epoch [360/740], Loss: 0.4112\n",
      "Epoch [365/740], Loss: 0.4085\n",
      "Epoch [370/740], Loss: 0.4058\n",
      "Epoch [375/740], Loss: 0.4032\n",
      "Epoch [380/740], Loss: 0.4006\n",
      "Epoch [385/740], Loss: 0.3980\n",
      "Epoch [390/740], Loss: 0.3955\n",
      "Epoch [395/740], Loss: 0.3931\n",
      "Epoch [400/740], Loss: 0.3907\n",
      "Epoch [405/740], Loss: 0.3884\n",
      "Epoch [410/740], Loss: 0.3861\n",
      "Epoch [415/740], Loss: 0.3838\n",
      "Epoch [420/740], Loss: 0.3816\n",
      "Epoch [425/740], Loss: 0.3794\n",
      "Epoch [430/740], Loss: 0.3773\n",
      "Epoch [435/740], Loss: 0.3752\n",
      "Epoch [440/740], Loss: 0.3731\n",
      "Epoch [445/740], Loss: 0.3711\n",
      "Epoch [450/740], Loss: 0.3691\n",
      "Epoch [455/740], Loss: 0.3671\n",
      "Epoch [460/740], Loss: 0.3652\n",
      "Epoch [465/740], Loss: 0.3632\n",
      "Epoch [470/740], Loss: 0.3614\n",
      "Epoch [475/740], Loss: 0.3595\n",
      "Epoch [480/740], Loss: 0.3577\n",
      "Epoch [485/740], Loss: 0.3559\n",
      "Epoch [490/740], Loss: 0.3541\n",
      "Epoch [495/740], Loss: 0.3524\n",
      "Epoch [500/740], Loss: 0.3507\n",
      "Epoch [505/740], Loss: 0.3490\n",
      "Epoch [510/740], Loss: 0.3473\n",
      "Epoch [515/740], Loss: 0.3456\n",
      "Epoch [520/740], Loss: 0.3440\n",
      "Epoch [525/740], Loss: 0.3424\n",
      "Epoch [530/740], Loss: 0.3408\n",
      "Epoch [535/740], Loss: 0.3393\n",
      "Epoch [540/740], Loss: 0.3377\n",
      "Epoch [545/740], Loss: 0.3362\n",
      "Epoch [550/740], Loss: 0.3347\n",
      "Epoch [555/740], Loss: 0.3332\n",
      "Epoch [560/740], Loss: 0.3317\n",
      "Epoch [565/740], Loss: 0.3303\n",
      "Epoch [570/740], Loss: 0.3288\n",
      "Epoch [575/740], Loss: 0.3274\n",
      "Epoch [580/740], Loss: 0.3260\n",
      "Epoch [585/740], Loss: 0.3246\n",
      "Epoch [590/740], Loss: 0.3232\n",
      "Epoch [595/740], Loss: 0.3218\n",
      "Epoch [600/740], Loss: 0.3204\n",
      "Epoch [605/740], Loss: 0.3191\n",
      "Epoch [610/740], Loss: 0.3178\n",
      "Epoch [615/740], Loss: 0.3164\n",
      "Epoch [620/740], Loss: 0.3151\n",
      "Epoch [625/740], Loss: 0.3138\n",
      "Epoch [630/740], Loss: 0.3125\n",
      "Epoch [635/740], Loss: 0.3112\n",
      "Epoch [640/740], Loss: 0.3099\n",
      "Epoch [645/740], Loss: 0.3087\n",
      "Epoch [650/740], Loss: 0.3074\n",
      "Epoch [655/740], Loss: 0.3061\n",
      "Epoch [660/740], Loss: 0.3049\n",
      "Epoch [665/740], Loss: 0.3036\n",
      "Epoch [670/740], Loss: 0.3024\n",
      "Epoch [675/740], Loss: 0.3011\n",
      "Epoch [680/740], Loss: 0.2999\n",
      "Epoch [685/740], Loss: 0.2987\n",
      "Epoch [690/740], Loss: 0.2974\n",
      "Epoch [695/740], Loss: 0.2962\n",
      "Epoch [700/740], Loss: 0.2950\n",
      "Epoch [705/740], Loss: 0.2938\n",
      "Epoch [710/740], Loss: 0.2926\n",
      "Epoch [715/740], Loss: 0.2914\n",
      "Epoch [720/740], Loss: 0.2903\n",
      "Epoch [725/740], Loss: 0.2891\n",
      "Epoch [730/740], Loss: 0.2879\n",
      "Epoch [735/740], Loss: 0.2868\n",
      "Epoch [740/740], Loss: 0.2856\n",
      "Training complete.\n",
      "Test Accuracy: 96.67%\n",
      "Test Loss (Cross-Entropy): 0.2208\n"
     ]
    }
   ],
   "source": [
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Optionally, for CUDA (if you're using GPU)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "torch_results = []\n",
    "torch_preds = []\n",
    "\n",
    "my_results = []\n",
    "my_preds = []\n",
    "\n",
    "similarity = []\n",
    "epochs = range(10, 750, 10)\n",
    "\n",
    "for eps in epochs:\n",
    "    model_torch = NeuralNetwork()\n",
    "    train(X_train_tensor, y_train_tensor, model_torch, 0.01, eps, nn.CrossEntropyLoss(), X_test_tensor, y_test_tensor, False)\n",
    "    preds = test(X_test_tensor, y_test_tensor, model_torch, nn.CrossEntropyLoss()).detach().numpy()\n",
    "    torch_preds.append(preds)\n",
    "    torch_results.append(np.mean(preds == y_test))\n",
    "    my_model = ANNClassification([15], [\"relu\"])\n",
    "    my_model.fit(X_train, y_train, 0.01, eps)\n",
    "    preds2 = np.argmax(my_model.predict(X_test), axis=1)\n",
    "    my_preds.append(preds2)\n",
    "    my_results.append(np.mean(preds2 == y_test))\n",
    "\n",
    "    similarity.append(np.mean(preds2 == preds))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19d28e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, torch_results, label=\"Torch Results\")\n",
    "plt.plot(epochs, my_results, linestyle=\"--\", alpha=1, label=\"My Results\")\n",
    "plt.plot(epochs, similarity, label=\"Similarity\")\n",
    "\n",
    "# Adding the legend\n",
    "plt.legend()\n",
    "\n",
    "# Optionally, you can also add titles and labels\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Display the plot\n",
    "plt.savefig(\"Classification comparison.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aadc2b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 4])\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.fetch_california_housing()\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_reg = scaler.fit_transform(X_train_reg)\n",
    "X_test_reg = scaler.transform(X_test_reg)\n",
    "\n",
    "X_train_reg_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_reg_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_reg_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_reg_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "print(X_train_reg_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77a0ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        random.seed(42)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 1)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        # Use NumPy to generate random numbers for weight initialization\n",
    "        for layer in self.linear_relu_stack:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Use NumPy to generate random values for weight initialization\n",
    "                weight_init = np.random.uniform(0, 1, size=layer.weight.shape)\n",
    "                bias_init = np.ones_like(layer.bias.detach())  # Detach bias from computation graph\n",
    "\n",
    "                # Assign the NumPy-generated weights to PyTorch layers\n",
    "                layer.weight.data = torch.tensor(weight_init, dtype=torch.float32)\n",
    "                layer.bias.data = torch.tensor(bias_init, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02832c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.7011\n",
      "Epoch [10/10], Loss: 0.6905\n",
      "Training complete.\n",
      "Test Accuracy: 33.33%\n",
      "Test Loss (Cross-Entropy): 0.7245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matej\\miniconda3\\envs\\MLDS_HW1\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([120])) that is different to the input size (torch.Size([120, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\matej\\miniconda3\\envs\\MLDS_HW1\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([30])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m preds = test(X_test_reg_tensor, y_test_reg_tensor, model_torch, nn.MSELoss()).detach().numpy()\n\u001b[32m     21\u001b[39m torch_preds.append(preds)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m torch_results.append(np.mean((\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_reg_tensor\u001b[49m) ** \u001b[32m2\u001b[39m))\n\u001b[32m     23\u001b[39m my_model = ANNRegression([\u001b[32m15\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     24\u001b[39m my_model.fit(X_train_reg, y_train_reg, \u001b[32m0.01\u001b[39m, eps)\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "\n",
    "# Optionally, for CUDA (if you're using GPU)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "torch_results = []\n",
    "torch_preds = []\n",
    "\n",
    "my_results = []\n",
    "my_preds = []\n",
    "\n",
    "similarity = []\n",
    "epochs = range(10, 750, 10)\n",
    "\n",
    "for eps in epochs:\n",
    "    model_torch = RegressionNeuralNetwork()\n",
    "    train(X_train_reg_tensor, y_train_reg_tensor, model_torch, 0.01, eps, nn.MSELoss(), X_test_reg_tensor, y_test_reg_tensor, False)\n",
    "    preds = test(X_test_reg_tensor, y_test_reg_tensor, model_torch, nn.MSELoss()).detach().numpy()\n",
    "    torch_preds.append(preds)\n",
    "    torch_results.append(np.mean((preds - y_test_reg) ** 2))\n",
    "    my_model = ANNRegression([15], [\"relu\"])\n",
    "    my_model.fit(X_train_reg, y_train_reg, 0.01, eps)\n",
    "    preds2 = my_model.predict(X_test_reg)\n",
    "    my_preds.append(preds2)\n",
    "    my_results.append(np.mean((preds2 - y_test_reg) ** 2))\n",
    "\n",
    "    similarity.append(np.mean((preds2 - preds) ** 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDS_HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
