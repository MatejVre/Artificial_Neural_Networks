{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cd8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eab8769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"image1-competition.hdf5\"\n",
    "\n",
    "with h5py.File(fn, \"r\") as f:\n",
    "    data = np.array(f[\"data\"])\n",
    "    wns = np.array(f[\"wns\"])\n",
    "    tissue_mask = np.array(f[\"tissue_mask\"])\n",
    "    classes = np.array(f[\"classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f24270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data[475:700, 750:1000, :]\n",
    "y_test = classes[475:700, 750:1000]\n",
    "\n",
    "train_data = np.delete(data, slice(475, 700), axis=0)\n",
    "X_train = np.delete(train_data, slice(750, 1000), axis=1)\n",
    "\n",
    "train_data_y = np.delete(classes, slice(475, 700), axis=0)\n",
    "y_train = np.delete(train_data_y, slice(750, 1000), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fc30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_sel_test = y_test != -1\n",
    "\n",
    "X_test = X_test[annotated_sel_test, :]\n",
    "y_test = y_test[annotated_sel_test]\n",
    "\n",
    "\n",
    "\n",
    "annotated_sel_train = y_train != -1\n",
    "X_train = X_train[annotated_sel_train]\n",
    "y_train = y_train[annotated_sel_train]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2255528f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25769, 187])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c538712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(187, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "def train(X, y, model, learning_rate, num_epochs, loss_fn, X_test, y_test, validation=False):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    model.train()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Forward pass\n",
    "        preds = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Print loss after each epoch\n",
    "        if (epoch + 1) % 10 == 0:  # Print every 10 epochs (adjust as needed)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero gradients for the next step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if validation:      \n",
    "            # Validation step\n",
    "            model.eval()  # Switch to evaluation mode (disables dropout, batch norm, etc.)\n",
    "            with torch.no_grad():  # Disable gradient calculation for validation to save memory and computation\n",
    "                val_preds = model(X_test)\n",
    "                val_loss = loss_fn(val_preds, y_test)\n",
    "            \n",
    "            # Print validation loss after each epoch\n",
    "            if (epoch + 1) % 10 == 0:  # Print every 10 epochs (adjust as needed)\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss.item():.4f}\")\n",
    "            \n",
    "        model.train() \n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "def test(X_test, y_test, model, loss_fn):\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():  # No gradient tracking needed for inference\n",
    "        preds = model(X_test)\n",
    "    # Convert predictions to class labels (for classification)\n",
    "    _, predicted = torch.max(preds, 1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (predicted == y_test).sum().item()\n",
    "    total = y_test.size(0)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Calculate cross-entropy loss\n",
    "    print(y_test.shape)\n",
    "    loss = loss_fn(preds, y_test)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Test Loss (Cross-Entropy): {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1200], Loss: 1.9000\n",
      "Epoch [10/1200], Validation Loss: 1.8362\n",
      "Epoch [20/1200], Loss: 1.8043\n",
      "Epoch [20/1200], Validation Loss: 1.8121\n",
      "Epoch [30/1200], Loss: 1.7092\n",
      "Epoch [30/1200], Validation Loss: 1.7827\n",
      "Epoch [40/1200], Loss: 1.6270\n",
      "Epoch [40/1200], Validation Loss: 1.7478\n",
      "Epoch [50/1200], Loss: 1.5571\n",
      "Epoch [50/1200], Validation Loss: 1.7092\n",
      "Epoch [60/1200], Loss: 1.4807\n",
      "Epoch [60/1200], Validation Loss: 1.6630\n",
      "Epoch [70/1200], Loss: 1.3872\n",
      "Epoch [70/1200], Validation Loss: 1.6146\n",
      "Epoch [80/1200], Loss: 1.2790\n",
      "Epoch [80/1200], Validation Loss: 1.5570\n",
      "Epoch [90/1200], Loss: 1.1661\n",
      "Epoch [90/1200], Validation Loss: 1.5052\n",
      "Epoch [100/1200], Loss: 1.0640\n",
      "Epoch [100/1200], Validation Loss: 1.4795\n",
      "Epoch [110/1200], Loss: 0.9788\n",
      "Epoch [110/1200], Validation Loss: 1.5093\n",
      "Epoch [120/1200], Loss: 0.9131\n",
      "Epoch [120/1200], Validation Loss: 1.5764\n",
      "Epoch [130/1200], Loss: 0.8596\n",
      "Epoch [130/1200], Validation Loss: 1.6313\n",
      "Epoch [140/1200], Loss: 0.8126\n",
      "Epoch [140/1200], Validation Loss: 1.6322\n",
      "Epoch [150/1200], Loss: 0.7690\n",
      "Epoch [150/1200], Validation Loss: 1.5966\n",
      "Epoch [160/1200], Loss: 0.7275\n",
      "Epoch [160/1200], Validation Loss: 1.5574\n",
      "Epoch [170/1200], Loss: 0.6902\n",
      "Epoch [170/1200], Validation Loss: 1.5278\n",
      "Epoch [180/1200], Loss: 0.6563\n",
      "Epoch [180/1200], Validation Loss: 1.4850\n",
      "Epoch [190/1200], Loss: 0.6249\n",
      "Epoch [190/1200], Validation Loss: 1.4341\n",
      "Epoch [200/1200], Loss: 0.5964\n",
      "Epoch [200/1200], Validation Loss: 1.3932\n",
      "Epoch [210/1200], Loss: 0.5715\n",
      "Epoch [210/1200], Validation Loss: 1.3612\n",
      "Epoch [220/1200], Loss: 0.5490\n",
      "Epoch [220/1200], Validation Loss: 1.3280\n",
      "Epoch [230/1200], Loss: 0.5280\n",
      "Epoch [230/1200], Validation Loss: 1.2931\n",
      "Epoch [240/1200], Loss: 0.5079\n",
      "Epoch [240/1200], Validation Loss: 1.2562\n",
      "Epoch [250/1200], Loss: 0.4894\n",
      "Epoch [250/1200], Validation Loss: 1.2186\n",
      "Epoch [260/1200], Loss: 0.4724\n",
      "Epoch [260/1200], Validation Loss: 1.1772\n",
      "Epoch [270/1200], Loss: 0.4567\n",
      "Epoch [270/1200], Validation Loss: 1.1325\n",
      "Epoch [280/1200], Loss: 0.4418\n",
      "Epoch [280/1200], Validation Loss: 1.0881\n",
      "Epoch [290/1200], Loss: 0.4281\n",
      "Epoch [290/1200], Validation Loss: 1.0442\n",
      "Epoch [300/1200], Loss: 0.4156\n",
      "Epoch [300/1200], Validation Loss: 0.9989\n",
      "Epoch [310/1200], Loss: 0.4044\n",
      "Epoch [310/1200], Validation Loss: 0.9595\n",
      "Epoch [320/1200], Loss: 0.3942\n",
      "Epoch [320/1200], Validation Loss: 0.9301\n",
      "Epoch [330/1200], Loss: 0.3849\n",
      "Epoch [330/1200], Validation Loss: 0.9109\n",
      "Epoch [340/1200], Loss: 0.3765\n",
      "Epoch [340/1200], Validation Loss: 0.9056\n",
      "Epoch [350/1200], Loss: 0.3688\n",
      "Epoch [350/1200], Validation Loss: 0.9086\n",
      "Epoch [360/1200], Loss: 0.3617\n",
      "Epoch [360/1200], Validation Loss: 0.9193\n",
      "Epoch [370/1200], Loss: 0.3549\n",
      "Epoch [370/1200], Validation Loss: 0.9414\n",
      "Epoch [380/1200], Loss: 0.3486\n",
      "Epoch [380/1200], Validation Loss: 0.9690\n",
      "Epoch [390/1200], Loss: 0.3424\n",
      "Epoch [390/1200], Validation Loss: 0.9976\n",
      "Epoch [400/1200], Loss: 0.3367\n",
      "Epoch [400/1200], Validation Loss: 1.0168\n",
      "Epoch [410/1200], Loss: 0.3313\n",
      "Epoch [410/1200], Validation Loss: 1.0283\n",
      "Epoch [420/1200], Loss: 0.3263\n",
      "Epoch [420/1200], Validation Loss: 1.0341\n",
      "Epoch [430/1200], Loss: 0.3215\n",
      "Epoch [430/1200], Validation Loss: 1.0401\n",
      "Epoch [440/1200], Loss: 0.3169\n",
      "Epoch [440/1200], Validation Loss: 1.0525\n",
      "Epoch [450/1200], Loss: 0.3126\n",
      "Epoch [450/1200], Validation Loss: 1.0651\n",
      "Epoch [460/1200], Loss: 0.3085\n",
      "Epoch [460/1200], Validation Loss: 1.0654\n",
      "Epoch [470/1200], Loss: 0.3046\n",
      "Epoch [470/1200], Validation Loss: 1.0702\n",
      "Epoch [480/1200], Loss: 0.3008\n",
      "Epoch [480/1200], Validation Loss: 1.0757\n",
      "Epoch [490/1200], Loss: 0.2971\n",
      "Epoch [490/1200], Validation Loss: 1.0712\n",
      "Epoch [500/1200], Loss: 0.2936\n",
      "Epoch [500/1200], Validation Loss: 1.0639\n",
      "Epoch [510/1200], Loss: 0.2902\n",
      "Epoch [510/1200], Validation Loss: 1.0536\n",
      "Epoch [520/1200], Loss: 0.2870\n",
      "Epoch [520/1200], Validation Loss: 1.0509\n",
      "Epoch [530/1200], Loss: 0.2839\n",
      "Epoch [530/1200], Validation Loss: 1.0416\n",
      "Epoch [540/1200], Loss: 0.2809\n",
      "Epoch [540/1200], Validation Loss: 1.0275\n",
      "Epoch [550/1200], Loss: 0.2780\n",
      "Epoch [550/1200], Validation Loss: 1.0164\n",
      "Epoch [560/1200], Loss: 0.2753\n",
      "Epoch [560/1200], Validation Loss: 1.0025\n",
      "Epoch [570/1200], Loss: 0.2726\n",
      "Epoch [570/1200], Validation Loss: 0.9948\n",
      "Epoch [580/1200], Loss: 0.2699\n",
      "Epoch [580/1200], Validation Loss: 0.9871\n",
      "Epoch [590/1200], Loss: 0.2673\n",
      "Epoch [590/1200], Validation Loss: 0.9715\n",
      "Epoch [600/1200], Loss: 0.2648\n",
      "Epoch [600/1200], Validation Loss: 0.9574\n",
      "Epoch [610/1200], Loss: 0.2622\n",
      "Epoch [610/1200], Validation Loss: 0.9472\n",
      "Epoch [620/1200], Loss: 0.2596\n",
      "Epoch [620/1200], Validation Loss: 0.9267\n",
      "Epoch [630/1200], Loss: 0.2571\n",
      "Epoch [630/1200], Validation Loss: 0.8997\n",
      "Epoch [640/1200], Loss: 0.2546\n",
      "Epoch [640/1200], Validation Loss: 0.8835\n",
      "Epoch [650/1200], Loss: 0.2523\n",
      "Epoch [650/1200], Validation Loss: 0.8726\n",
      "Epoch [660/1200], Loss: 0.2500\n",
      "Epoch [660/1200], Validation Loss: 0.8623\n",
      "Epoch [670/1200], Loss: 0.2477\n",
      "Epoch [670/1200], Validation Loss: 0.8515\n",
      "Epoch [680/1200], Loss: 0.2454\n",
      "Epoch [680/1200], Validation Loss: 0.8458\n",
      "Epoch [690/1200], Loss: 0.2432\n",
      "Epoch [690/1200], Validation Loss: 0.8399\n",
      "Epoch [700/1200], Loss: 0.2410\n",
      "Epoch [700/1200], Validation Loss: 0.8346\n",
      "Epoch [710/1200], Loss: 0.2390\n",
      "Epoch [710/1200], Validation Loss: 0.8274\n",
      "Epoch [720/1200], Loss: 0.2369\n",
      "Epoch [720/1200], Validation Loss: 0.8148\n",
      "Epoch [730/1200], Loss: 0.2349\n",
      "Epoch [730/1200], Validation Loss: 0.8088\n",
      "Epoch [740/1200], Loss: 0.2330\n",
      "Epoch [740/1200], Validation Loss: 0.8025\n",
      "Epoch [750/1200], Loss: 0.2311\n",
      "Epoch [750/1200], Validation Loss: 0.7947\n",
      "Epoch [760/1200], Loss: 0.2294\n",
      "Epoch [760/1200], Validation Loss: 0.7869\n",
      "Epoch [770/1200], Loss: 0.2277\n",
      "Epoch [770/1200], Validation Loss: 0.7800\n",
      "Epoch [780/1200], Loss: 0.2260\n",
      "Epoch [780/1200], Validation Loss: 0.7698\n",
      "Epoch [790/1200], Loss: 0.2244\n",
      "Epoch [790/1200], Validation Loss: 0.7682\n",
      "Epoch [800/1200], Loss: 0.2228\n",
      "Epoch [800/1200], Validation Loss: 0.7646\n",
      "Epoch [810/1200], Loss: 0.2213\n",
      "Epoch [810/1200], Validation Loss: 0.7610\n",
      "Epoch [820/1200], Loss: 0.2199\n",
      "Epoch [820/1200], Validation Loss: 0.7563\n",
      "Epoch [830/1200], Loss: 0.2184\n",
      "Epoch [830/1200], Validation Loss: 0.7500\n",
      "Epoch [840/1200], Loss: 0.2171\n",
      "Epoch [840/1200], Validation Loss: 0.7451\n",
      "Epoch [850/1200], Loss: 0.2157\n",
      "Epoch [850/1200], Validation Loss: 0.7382\n",
      "Epoch [860/1200], Loss: 0.2144\n",
      "Epoch [860/1200], Validation Loss: 0.7315\n",
      "Epoch [870/1200], Loss: 0.2132\n",
      "Epoch [870/1200], Validation Loss: 0.7276\n",
      "Epoch [880/1200], Loss: 0.2119\n",
      "Epoch [880/1200], Validation Loss: 0.7254\n",
      "Epoch [890/1200], Loss: 0.2107\n",
      "Epoch [890/1200], Validation Loss: 0.7218\n",
      "Epoch [900/1200], Loss: 0.2095\n",
      "Epoch [900/1200], Validation Loss: 0.7185\n",
      "Epoch [910/1200], Loss: 0.2084\n",
      "Epoch [910/1200], Validation Loss: 0.7165\n",
      "Epoch [920/1200], Loss: 0.2073\n",
      "Epoch [920/1200], Validation Loss: 0.7111\n",
      "Epoch [930/1200], Loss: 0.2061\n",
      "Epoch [930/1200], Validation Loss: 0.7104\n",
      "Epoch [940/1200], Loss: 0.2050\n",
      "Epoch [940/1200], Validation Loss: 0.7053\n",
      "Epoch [950/1200], Loss: 0.2039\n",
      "Epoch [950/1200], Validation Loss: 0.7014\n",
      "Epoch [960/1200], Loss: 0.2029\n",
      "Epoch [960/1200], Validation Loss: 0.6937\n",
      "Epoch [970/1200], Loss: 0.2018\n",
      "Epoch [970/1200], Validation Loss: 0.6902\n",
      "Epoch [980/1200], Loss: 0.2007\n",
      "Epoch [980/1200], Validation Loss: 0.6876\n",
      "Epoch [990/1200], Loss: 0.1996\n",
      "Epoch [990/1200], Validation Loss: 0.6893\n",
      "Epoch [1000/1200], Loss: 0.1985\n",
      "Epoch [1000/1200], Validation Loss: 0.6929\n",
      "Epoch [1010/1200], Loss: 0.1975\n",
      "Epoch [1010/1200], Validation Loss: 0.6938\n",
      "Epoch [1020/1200], Loss: 0.1965\n",
      "Epoch [1020/1200], Validation Loss: 0.7022\n",
      "Epoch [1030/1200], Loss: 0.1955\n",
      "Epoch [1030/1200], Validation Loss: 0.6960\n",
      "Epoch [1040/1200], Loss: 0.1945\n",
      "Epoch [1040/1200], Validation Loss: 0.6969\n",
      "Epoch [1050/1200], Loss: 0.1935\n",
      "Epoch [1050/1200], Validation Loss: 0.6974\n",
      "Epoch [1060/1200], Loss: 0.1926\n",
      "Epoch [1060/1200], Validation Loss: 0.7021\n",
      "Epoch [1070/1200], Loss: 0.1917\n",
      "Epoch [1070/1200], Validation Loss: 0.6986\n",
      "Epoch [1080/1200], Loss: 0.1908\n",
      "Epoch [1080/1200], Validation Loss: 0.7084\n",
      "Epoch [1090/1200], Loss: 0.1899\n",
      "Epoch [1090/1200], Validation Loss: 0.7045\n",
      "Epoch [1100/1200], Loss: 0.1891\n",
      "Epoch [1100/1200], Validation Loss: 0.7137\n",
      "Epoch [1110/1200], Loss: 0.1883\n",
      "Epoch [1110/1200], Validation Loss: 0.7084\n",
      "Epoch [1120/1200], Loss: 0.1875\n",
      "Epoch [1120/1200], Validation Loss: 0.7202\n",
      "Epoch [1130/1200], Loss: 0.1867\n",
      "Epoch [1130/1200], Validation Loss: 0.7158\n",
      "Epoch [1140/1200], Loss: 0.1859\n",
      "Epoch [1140/1200], Validation Loss: 0.7297\n",
      "Epoch [1150/1200], Loss: 0.1852\n",
      "Epoch [1150/1200], Validation Loss: 0.7256\n",
      "Epoch [1160/1200], Loss: 0.1845\n",
      "Epoch [1160/1200], Validation Loss: 0.7389\n",
      "Epoch [1170/1200], Loss: 0.1838\n",
      "Epoch [1170/1200], Validation Loss: 0.7386\n",
      "Epoch [1180/1200], Loss: 0.1831\n",
      "Epoch [1180/1200], Validation Loss: 0.7441\n",
      "Epoch [1190/1200], Loss: 0.1825\n",
      "Epoch [1190/1200], Validation Loss: 0.7453\n",
      "Epoch [1200/1200], Loss: 0.1818\n",
      "Epoch [1200/1200], Validation Loss: 0.7524\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "train(X_train, y_train, model, 0.001, 1200, nn.CrossEntropyLoss(), X_test, y_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d50a0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5192])\n",
      "Test Accuracy: 72.23%\n",
      "Test Loss (Cross-Entropy): 0.8767\n"
     ]
    }
   ],
   "source": [
    "test(X_test, y_test, model, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3279\n",
      "Epoch [20/100], Loss: 0.3019\n",
      "Epoch [30/100], Loss: 0.2860\n",
      "Epoch [40/100], Loss: 0.2709\n",
      "Epoch [50/100], Loss: 0.2588\n",
      "Epoch [60/100], Loss: 0.2477\n",
      "Epoch [70/100], Loss: 0.2384\n",
      "Epoch [80/100], Loss: 0.2307\n",
      "Epoch [90/100], Loss: 0.2243\n",
      "Epoch [100/100], Loss: 0.2215\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "annotated_sel = classes != -1\n",
    "X_real = data[annotated_sel]\n",
    "y_real = classes[annotated_sel]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_real = scaler.fit_transform(X_real)\n",
    "\n",
    "data_predict = data[265:465,360:660]\n",
    "\n",
    "X_real = torch.tensor(X_real, dtype=torch.float32)\n",
    "y_real = torch.tensor(y_real, dtype=torch.long)\n",
    "train(X_real, y_real, model, 0.001, 1200, nn.CrossEntropyLoss(), [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc0ee47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "lin_data_predict = data_predict.reshape(-1, data_predict.shape[-1])\n",
    "lin_data_predict = scaler.transform(lin_data_predict)\n",
    "\n",
    "lin_data_predict = torch.tensor(lin_data_predict, dtype=torch.float32)\n",
    "pred = model(lin_data_predict)\n",
    "pred = torch.softmax(pred, dim=1)\n",
    "pred = pred.reshape((200, 300, -1)).detach().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "with open(\"first.npy\", \"wb\") as f:\n",
    "    np.save(f, pred.astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLDS_HW1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
